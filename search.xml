<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HTTPS原理</title>
    <url>/2016/11/15/basic/HTTPS%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h2 id="HTTPS原理"><a href="#HTTPS原理" class="headerlink" title="HTTPS原理"></a>HTTPS原理</h2><h3 id="一、HTTPS及其优点"><a href="#一、HTTPS及其优点" class="headerlink" title="一、HTTPS及其优点"></a>一、HTTPS及其优点</h3><p>HTTPS并不是一种新的协议，而是HTTP Over SSL(TLS)。它工作在客户端和服务端之间，协议栈如下图所示。</p>
<p><img src="https://cattail.me/assets/how-https-works/tcp-ip-model.png" alt="tcp ip model"></p>
<p>这里的<strong>TLS</strong>协议是一组应用层协议的统称，其前身是SSL协议，<strong>TLS1.2</strong>又被称为<strong>SSL3.3</strong>，其实起着相同的作用。</p>
<p>HTTPS能够对网络会话进行以下保护。</p>
<p>1，  内容加密。浏览器到服务器的内容都是以加密形式传输，中间者无法直接查看原始内容。这是依靠加密算法做到的。</p>
<p>2，  身份认证。保证用户访问的是希望访问的服务器，即使被 DNS 劫持到了第三方站点，也会提醒用户有可能被劫持。这是依靠CA和证书做到的。</p>
<p>3，  数据完整性。防止内容被第三方冒充或者篡改。这是依靠Hash算法和加密算法一起做到的。</p>
<h3 id="二、HTTPS的工作过程"><a href="#二、HTTPS的工作过程" class="headerlink" title="二、HTTPS的工作过程"></a>二、HTTPS的工作过程</h3><p><img src="http://pic002.cnblogs.com/images/2012/38542/2012072310244445.png" alt="img"></p>
<ul>
<li>1.客户端（浏览器）把自己支持的加密算法打包告诉服务器。</li>
<li>2与3.服务器选出一组加密算法和Hash算法，并将自己的身份信息以<strong>证书</strong>的形式发回给客户端。证书中主要包含以下内容：<ul>
<li>服务端Url，即网站地址。</li>
<li>加密的公钥。</li>
<li>证书的颁发机构。</li>
</ul>
</li>
<li>4.客户端收到证书后，进行下面的操作：<ul>
<li>检验证书的合法性，看证书是不是CA颁发的，如果不是浏览器会发出警告。</li>
<li>第一步完成后（证书有效或用户信任证书），客户端产生一串随机值P，使用证书的公钥进行加密。</li>
</ul>
</li>
<li>5.客户端把加密的随机值P发送给服务端。</li>
<li>6.服务端使用存储的私钥进行解密，得到随机值P。</li>
<li>7.服务端使用P作为密钥加密HTTP协议的内容，并发送给客户端。</li>
</ul>
<h3 id="三、几个问题"><a href="#三、几个问题" class="headerlink" title="三、几个问题"></a>三、几个问题</h3><ul>
<li>HTTPS连接的过程中使用的到底是什么加密算法？</li>
</ul>
<p>如下图所示，可以看到密钥的交换机制是RSA，而加密机制是AES。</p>
<p><img src="http://op.baidu.com/wp-content/uploads/2015/04/pic4.png" alt="pic4"></p>
<p>HTTPS涉及到对称加密、非对称加密以及Hash算法，由于非对称加密（公私钥）开销较大，因此只用于HTTP握手阶段交换对称加密的密钥使用，而在具体传输阶段，使用的是对称加密。</p>
<p>而且，HTTPS每条消息都会附加一个Hash值，Hash算法保证了发送和接收的信息不被其他人篡改。而且证书也会通过Hash算法进行签名，防止证书被修改。</p>
<ul>
<li>为什么不在整个过程中使用非对称加密算法？</li>
</ul>
<p>非对称加密算法对于CPU 计算资源消耗非常大。一次完全 TLS 握手，密钥交换时的非对称解密计算量占整个握手过程的 90% 以上。而对称加密的计算量只相当于非对称加密的 0.1%，如果应用层数据也使用非对称加解密，性能开销太大，无法承受。</p>
<p>而且，非对称加密算法对加密内容的长度有限制，不能超过公钥长度。比如现在常用的公钥长度是 2048 位，意味着待加密内容不能超过 256 个字节。</p>
<ul>
<li>证书的内容？</li>
</ul>
<p>如图所示。</p>
<p><img src="http://op.baidu.com/wp-content/uploads/2015/04/pic5.png" alt="pic5"></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://www.xuanfengge.com/https-principle.html" target="_blank" rel="noopener">http://www.xuanfengge.com/https-principle.html</a></p>
<p><a href="http://op.baidu.com/2015/04/https-s01a01/" target="_blank" rel="noopener">http://op.baidu.com/2015/04/https-s01a01/</a></p>
]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>B站-《线性代数的本质》-笔记</title>
    <url>/2017/05/01/basic/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9A%84%E6%9C%AC%E8%B4%A8-%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>闲来无事，假期把B站上《线性代数的本质》系列看了一遍，在此记录一下要点。（系列里的所有动画的程序可以在<a href="https://github.com/3b1b/manim" target="_blank" rel="noopener">https://github.com/3b1b/manim</a>获得。这个系列能够让你从变换的角度来解读和理解线性代数的一些概念。</p>
<h4 id="一、向量是什么"><a href="#一、向量是什么" class="headerlink" title="一、向量是什么"></a>一、向量是什么</h4><p>向量(Vector)：</p>
<ul>
<li>物理领域，是由方向和长度确定的一个量。</li>
<li>计算机领域，是一个有序的数字列表，比如<script type="math/tex">[1,2,3]^T</script>。</li>
<li>数学领域，更加抽象，可以进行相加和数乘操作的任何量。</li>
</ul>
<p>加法：沿两个向量运动的方向运动到的的最终方向，乘法：向量在其方向上scale的大小。</p>
<a id="more"></a>
<h4 id="二、线性组合、张成的空间、基"><a href="#二、线性组合、张成的空间、基" class="headerlink" title="二、线性组合、张成的空间、基"></a>二、线性组合、张成的空间、基</h4><p>向量坐标(coordinate)：对一个空间中每个向量，其坐标都是一组<strong>基</strong>向量(basis vectors)的缩放相加(scale and add)形式即<strong>线性组合</strong>。不同的基向量就是不同的坐标系。</p>
<p><strong>张成空间</strong>(span)：即一组基向量通过线性组合能够表达的向量集合。</p>
<p>如果考虑单个向量，把它当做是一条线；而如果考虑一个向量集合，把每一个向量表示为一个点。（在几何可视化的时候更加清晰）</p>
<p><strong>线性相关</strong>(linearly dependent)：如果你有一组n个向量，而其中存在一些向量，即使把这些向量去掉，也不会减少张成空间的维度，那么这组向量线性相关。</p>
<p>如果一组n个向量线性无关，那么可以张成n维空间。</p>
<h4 id="三、矩阵与线性变换（-）"><a href="#三、矩阵与线性变换（-）" class="headerlink" title="三、矩阵与线性变换（*）"></a>三、矩阵与线性变换（*）</h4><p><strong>线性变换</strong>(linear transformation)：一个函数（映射），接受输入向量，输出对应向量。为什么叫变换呢，就是让你以一种运动的眼光来审视它。至于线性变换，就是限制变换为以下条件：</p>
<ul>
<li>变换前在一条直线上的点（向量），变换后仍然在一条直线上；（包括对角线的向量）</li>
<li>变换过程中，原点的位置不能变。（能变的是仿射变换affine transformation）</li>
</ul>
<p>那么，如何用数值来描述这些各种各样的线性变换呢？也就是给你一个n维空间中的向量坐标<script type="math/tex">\vec K=[x_1,...,x_n]^T</script>，如何计算变换后的向量坐标<script type="math/tex">\vec {K'}= [\hat x_1,...,\hat x_n]^T</script>。</p>
<p>由于任意给定一个向量，它的坐标都是一组基向量<script type="math/tex">\{\vec a_1,...,\vec a_n\}</script>的线性组合，所以有<script type="math/tex">\vec K=\alpha_1\vec a_1+...+\alpha_n\vec a_n</script>，而对任意的线性变换，设变换后上述基向量集合变化为<script type="math/tex">B=\{\vec b_1,...,\vec b_n\}</script>，那么<script type="math/tex">\vec K'=\alpha_1\vec b_1+...+\alpha_n\vec b_n</script>。</p>
<p>那么，如果一个n维空间中，原始的坐标是<script type="math/tex">[\alpha_1,...,\alpha_n]</script>，那么任意的线性变换，只需要由下面这个<script type="math/tex">n\times n</script>的矩阵定义，其中每一列表示变换后<strong>每个基向量的坐标</strong>：</p>
<script type="math/tex; mode=display">
B=\begin {bmatrix}
a^1_{1} & a^2_{1} & \cdots & a^n_1\\
\vdots & \vdots & \ddots & \vdots\\
a^1_n & a^2_n & \cdots & a^n_n\\
\end {bmatrix}</script><p>而这个变换矩阵和向量变换前坐标进行矩阵向量间乘法<script type="math/tex">B \vec K</script>的结果就是变换后的坐标<script type="math/tex">\vec {K'}</script>，所以，矩阵乘法就是计算线性变换作用于给定向量的一种方式。</p>
<h4 id="四、矩阵乘法与线性变换复合"><a href="#四、矩阵乘法与线性变换复合" class="headerlink" title="四、矩阵乘法与线性变换复合"></a>四、矩阵乘法与线性变换复合</h4><p>矩阵：代表线性变换；</p>
<p>矩阵和向量的乘法：代表线性变换作用于向量。</p>
<p>复合变换：几个线性变换的复合(composition)。</p>
<p>矩阵和矩阵的乘法：两个矩阵A、B相乘：<script type="math/tex">AB</script>，等价于先进行线性变换B，再进行线性变换A，可以这样认为：<script type="math/tex">C=AB == A(B(\cdot))</script>，C就是代表这个复合变换的矩阵。</p>
<h4 id="五、行列式"><a href="#五、行列式" class="headerlink" title="五、行列式"></a>五、行列式</h4><p>行列式(determinant)：一个线性变换的行列式，代表变换后单位区域面积与变换前单位区域空间之比。比如，在二维空间中<script type="math/tex">det(A)=2</script>表示线性变换A后每个区域面积被拉大到了原先的2倍，而在n维空间中<script type="math/tex">det(B)=0</script>表示线性变换B后整个n维空间被挤压到不足n维了。（若行列式的值为负数，可以认为空间的取向被翻转了）</p>
<p>所以<script type="math/tex">det(M_1M_2)=det(M_1)\cdot det(M_2)</script>在这种理解下也是显而易见的。</p>
<h4 id="六、逆矩阵、列空间与零空间"><a href="#六、逆矩阵、列空间与零空间" class="headerlink" title="六、逆矩阵、列空间与零空间"></a>六、逆矩阵、列空间与零空间</h4><p>线性方程组(linear system of equations)：未知量之间只进行加法和与常数的乘积这两种运算的方程组，能够用矩阵和向量表示为<script type="math/tex">A\vec x=\vec v</script>的形式，也就是寻找一个向量<script type="math/tex">\vec x</script>，这个向量在进行变换<script type="math/tex">A</script>后与<script type="math/tex">\vec v</script>向量是重合的。这节用线性方程组引入了其他概念的理解。</p>
<p>逆矩阵(inverse matrice)：就是逆向变换，<script type="math/tex">A^{-1}</script>就是把变换<script type="math/tex">A</script>造成的影响消除的变换，所以<script type="math/tex">A^{-1}A=I</script>，这里的I就是恒等变换(identity transformation)。但是如果一个矩阵的行列式为0，就意味着空间被挤压了，那么就无法找到一个变换把空间恢复原状。</p>
<p>列空间(column space)：对任意向量<script type="math/tex">\vec v</script>，所有可能的输出向量<script type="math/tex">A\vec v</script>的集合。满秩就代表着列空间的维数和输入空间的维数相同。</p>
<p>秩(rank)：形容一个变换之后得到的空间的维度。</p>
<p>零空间(null space)或者核(kernel)：变换后落在原点的向量的集合。</p>
<h4 id="附：关于非方阵"><a href="#附：关于非方阵" class="headerlink" title="附：关于非方阵"></a>附：关于非方阵</h4><p>之前的理解老是拿方阵作为例子，这里讲一下对非方阵的理解。如果一个<script type="math/tex">2\times 2</script>的方阵表示2维空间到2维空间的变换，其中每一列表示变换后的基向量。那么一个<script type="math/tex">3\times 2</script>的矩阵呢，可以认为它代表着一个2维空间到3维空间的变换，其中每一列的意义不变，所以变换后的每个基向量的坐标是3维的，对应矩阵有3行；基向量只有2个，对应原空间是2维的。</p>
<h4 id="七、点积与对偶性"><a href="#七、点积与对偶性" class="headerlink" title="七、点积与对偶性"></a>七、点积与对偶性</h4><p>点积(dot product)：两个向量<script type="math/tex">\vec a,\vec b</script>，其点积<script type="math/tex">\vec a\cdot \vec b</script>的结果如下：</p>
<ul>
<li>a和b同方向：a在b方向上投影的长度乘上b的长度。</li>
<li>a和b反方向：上面的结果加上一个负号。</li>
</ul>
<p>假定向量<script type="math/tex">\vec a = [a_1,a_2,\cdots,a_n]^T</script>，<script type="math/tex">\vec b = [b_1,b_2,\cdots,b_n]^T</script>，设<script type="math/tex">T=a^T=[a_1,a_2,\cdots,a_n]</script>，那么<script type="math/tex">\vec a\cdot \vec b = T\vec b=n</script>，其中n是一个标量。从点积的角度理解，可以认为是计算了两个向量的相似度，而从变换的角度理解，就可以认为是变换T针对向量<script type="math/tex">\vec b</script>向1维空间的坐标变换。因此可以认为两个n维向量的点积也就是前一个n维向量是一个n维到1维的变换，把后一个投影到了1维空间得到一个坐标。嗯，不过有什么用吗？</p>
<h4 id="八、从线性变换的角度理解叉积"><a href="#八、从线性变换的角度理解叉积" class="headerlink" title="八、从线性变换的角度理解叉积"></a>八、从线性变换的角度理解叉积</h4><p>叉积(cross product)：记为<script type="math/tex">\vec a\times \vec b</script>，若<script type="math/tex">\vec a = [a_1,a_2]^T</script>，<script type="math/tex">\vec b = [b_1,b_2]^T</script>，那么我们<strong>把a和b的坐标分别作为矩阵的列</strong>：</p>
<script type="math/tex; mode=display">
\vec a\times \vec b = det(
\begin{bmatrix}
a_1 & b_1\\
a_2 & b_2\\
\end{bmatrix})</script><p>可以从行列式的定义去理解这个计算过程。也就是计算出<script type="math/tex">\vec a</script>和<script type="math/tex">\vec b</script>张成的图形的面积。</p>
<p>在三维空间中，该计算过程变成：</p>
<script type="math/tex; mode=display">
\vec a \times \vec b = det(\begin{bmatrix}
\hat i & a_1 & b_1 \\
\hat j & a_2 & b_2 \\
\hat k & a_3 & b_3 \\
\end{bmatrix})</script><p>为了证明上面的计算结果满足叉积的定义，用线性变换的角度理解三维叉积。</p>
<p>对偶向量(dual vector)：如果一个n维到1维的变换存在，那么一定存在一个n维向量<script type="math/tex">\vec a</script>和这个变换等价，这个向量<script type="math/tex">\vec a</script>就是这个变换的对偶向量。</p>
<p>那么我们根据<script type="math/tex">\vec a</script>和<script type="math/tex">\vec b</script>计算一个三维到一维的线性变换，并找到其对偶向量，证明这个对偶向量就是<script type="math/tex">\vec a\times \vec b</script>。</p>
<p>如果我们把二维的叉积自然的扩展到三维，可以得到：</p>
<script type="math/tex; mode=display">
\vec a\times \vec b\times \vec c = det(
\begin{bmatrix}
a_1 & b_1 & c_1\\
a_2 & b_2 & c_2\\
a_3 & b_3 & c_3\\
\end{bmatrix})</script><p>而实际上，三维的叉积也是两个向量之间的运算，所以我们把<script type="math/tex">\vec a</script>看做变量，可以得到如下的形式：</p>
<script type="math/tex; mode=display">
f(\vec a) =f(\begin{bmatrix}x\\y\\z\end{bmatrix}) =\begin{bmatrix}p_1&p_2&p_3\end{bmatrix}\begin{bmatrix}x\\y\\z\end{bmatrix}=
\begin{bmatrix}p_1\\p_2\\p_3\end{bmatrix}\cdot\begin{bmatrix}x\\y\\z\end{bmatrix}
=
det(

\begin{bmatrix}

x & b1 & c_1\\

y & b2 & c_2\\

z & b3 & c_3\\

\end{bmatrix})</script><p>也就是得到了一个三维到一维空间（数轴）的线性变换。其中从第三项到第四项应用了点积得出的推论。所以，我们可以看出，三维叉积的计算结果就是找到了这个三维到一维变换的对偶向量<script type="math/tex">\vec p=\begin{bmatrix}p_1\\p_2\\p_3\end{bmatrix}</script>。</p>
<h4 id="九、基变换"><a href="#九、基变换" class="headerlink" title="九、基变换"></a>九、基变换</h4><p>这节讲了如何将向量坐标在两个不同的坐标系下（基向量下）互相转化。</p>
<ul>
<li>假如小明有一组基向量，以你的坐标系来描述其坐标是<script type="math/tex">\vec a_1,\cdots,\vec a_n</script>，那么将其组成一个矩阵<script type="math/tex">A=\begin{bmatrix}a_1&\cdots&a_n\end{bmatrix}</script>，这个矩阵叫做基变换矩阵。</li>
<li>这时有一个向量<script type="math/tex">\vec n = \begin{bmatrix}x_j\\y_j\end{bmatrix}</script>，其坐标是以小明的坐标系来描述的。</li>
<li>那么你要计算出这个向量在你的坐标系下的坐标<script type="math/tex">\begin{bmatrix}x_0\\y_0\end{bmatrix}</script>，使用<script type="math/tex">A\begin{bmatrix}x_j\\y_j\end{bmatrix}=\begin{bmatrix}x_0\\y_0\end{bmatrix}</script>。</li>
<li>这是因为A等价于一个我们的坐标系到小明的坐标系的变换（小明的语言到我们的语言的变换），因此上面等式左边结果就把一个小明语言中的向量转化为我们语言中的向量。</li>
</ul>
<p>同样的，对于小明坐标系下的任意向量<script type="math/tex">\vec v</script>，若小明的坐标系中发生了线性变换B（用我们的坐标系描述），那么<script type="math/tex">A^{-1}BA\vec v</script>就是变换后小明坐标系下向量的新坐标。这个过程首先应用基变换，而后应用线性变换，最后应用基变换的逆，那么<script type="math/tex">A^{-1}BA</script>就是<strong>用小明的坐标系描述的相同线性变换矩阵</strong>。</p>
<p>所以，当你看到一个<script type="math/tex">A^{-1}MA</script>形式的矩阵，你要明白M是你的角度看到的一个变换，而矩阵的结果转换了视角，是从另一个角度看到的同一个变换。</p>
<h4 id="十、特征向量与特征值"><a href="#十、特征向量与特征值" class="headerlink" title="十、特征向量与特征值"></a>十、特征向量与特征值</h4><p>特征向量(Eigen Vectors)：在一个线性变换中，没有离开其所张成空间的向量。</p>
<p>特征值(Eigen Values)：特征向量在变换中缩放(scale)的比例。因此，同一个特征值可以有多个不同特征向量这一事实也就显而易见了。</p>
<script type="math/tex; mode=display">
A\vec v = \lambda \vec v=(\lambda I) \vec v\\
(A-\lambda I)\vec v = \vec 0\\
det(A-\lambda I)=0</script><p>对角矩阵(diagonal matrix)：对应一种线性变换，其中所有的基向量都是特征向量。对角矩阵的计算很方便。</p>
<p>特征基(eigen basis)：对一个变换M，如果其特征向量能够张成原有空间维度，那么将其作为基变换矩阵A，<script type="math/tex">A^{-1}MA</script>就是原来的变换M在新的基向量下的变换，重要的是这个结果是一个对角矩阵容易计算。</p>
<h4 id="十一、抽象向量空间"><a href="#十一、抽象向量空间" class="headerlink" title="十一、抽象向量空间"></a>十一、抽象向量空间</h4><p>将函数或算子(operators)看做是更抽象的向量。将线性代数中的概念与代数中的概念类比。</p>
<p>满足下面两个性质的变换是线性的：</p>
<ul>
<li>可加性：<script type="math/tex">L(\vec v + \vec w)=L(\vec v)+L(\vec w)</script></li>
<li>成比例（一阶齐次）：<script type="math/tex">L(c\vec v)=cL(\vec v)</script></li>
</ul>
<p>那么，求导数是一种线性运算，因为其符合上面两个性质，所以可以使用一个矩阵A代表求导算子<script type="math/tex">\frac{d}{dx}</script>。</p>
]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>线性代数</tag>
        <tag>Bilibili</tag>
      </tags>
  </entry>
  <entry>
    <title>DRL-Dueling-《Dueling Network Architectures for Deep Reinforcement Learning》</title>
    <url>/2017/08/31/deeplearning/DRL-Dueling-%E3%80%8ADueling%20Network%20Architectures%20for%20Deep%20Reinforcement%20Learning%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>Deepmind在2016年4月发表在arXiv上的文章。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>在DQN提出后，很多这方面的工作，但是这些工作要不就把已有的CNN、RNN、AutoEncoder等各种结构应用于各种RL方法中，要不就是针对RL方法而不是神经网络的部分进行改进。本文提出了一种Dueling的网络结构来进行Policy Evaluation，这种网络结构更适合使用在RL模型上，在Atari 2600上取得了SOTA的成绩。</p>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>在看这篇论文之前，有几个概念需要了解：</p>
<blockquote>
<p>Bellman residual，是TD error的期望形式，即<script type="math/tex">E[\delta_t]</script>。可以利用Bellman residual的均方值e进行各种不同形式的梯度下降来学习Q函数。</p>
<p>价值和优势，1993年Baird提出将动作价值函数Q分解为状态价值V和优势（Advantage）A，其中V只与状态相关，而A与状态和动作相关。</p>
<p>策略梯度Policy Gradient，Policy-Based方法中，将策略π参数化为<script type="math/tex">\pi_\theta</script>，并找到一个关于<script type="math/tex">\theta</script>的目标函数<script type="math/tex">J(\theta)</script>，而后利用梯度下降法来优化Policy的参数，如下图。</p>
</blockquote>
<p><img src="/images/Ft5aPA-LyTN4JFPu7y7U4PCcHZLs.jpg" alt=""></p>
<p>dueling结构就是将DQN中的Q(s,a)分解为两个部分V(s)和A(s,a)，用来区分出同等状态下两个动作的好坏，但是这种区分只体现在Q网络的中间层：</p>
<p>原始DQN的Q网络，两层卷积层+两层全连接，最后输出每个动作的Q值。</p>
<p>而dueling网络，在两层卷积层之后，分别计算V和计算A分别用两层参数不同的全连接层，其中V的输出可以是一个scalar（因为是状态s的价值函数），而A的输出是维度|A|（动作数量）的向量。最后，两个张量通过下面的公式相加计算出动作价值函数Q：</p>
<script type="math/tex; mode=display">
Q(s,a)=V(s)+(A(s,a)-\frac{1}{|A|}\sum_{a'}A(s,a'))</script><p>需要注意的是，这里的A值减去了A的均值，这是论文最后实验中采用的计算方式，因为这样在实际实验中效果比较好。</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>dueling</tag>
      </tags>
  </entry>
  <entry>
    <title>paperWeekly知识图谱阅读小组-《Modeling Relational Data with Graph Convolutional Networks》</title>
    <url>/2017/04/27/deeplearning/KG-GCN-arXiv2017-%E3%80%8AModeling%20Relational%20Data%20with%20Graph%20Convolutional%20Networks%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>来源于arXiv，发表于2017年3月。是paperWeekly知识图谱阅读小组的本周阅读论文。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>虽然知识图谱（知识库，Knowledge Base）得到了广泛应用，但是即使最大的知识库也是不完整的，下游的应用（QA、IR）如果使用，需要进行统计关系学习（statistical relational learning，SRL）。</p>
<p>本文认为知识图谱中应当包含(实体，关系，实体)的三元组，以及实体对应的标签（属性），本文的研究着力于抽取缺失的关系，以及对实体进行分类（补全属性）。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>用GCN（Graph Convolutional Network）的变种R-GCN对实体进行分类、关系进行抽取。下图是R-GCN的结构图。</p>
<p><img src="/images/FgqqJ0Utjf2RmxvHI3wT2dFEpt8h.jpg" alt=""></p>
<p>对每一层，计算每个实体i的隐层状态<script type="math/tex">h_i^{(l)}</script>，也就是利用上一层与i有关系（边）的实体j的隐状态的加权和与上一层自己的隐状态加权，相加的结果就是本层的隐状态。</p>
<p>而这个加权的权重矩阵W，使用B个基本变化加权相加得到的。至于为什么不直接设计为权重矩阵，我没看明白，也许GCN就是这么设计的。（论文里面说这样可以看作不同关系之间的权值共享，减少了权值数量，对关系数很多的知识库比较好）</p>
<p>最终的R-GCN是叠加了L个上述层，其中第一层每个实体的输入是一个one-hot向量（或者其他的向量，比如和实体相关的文档的BOW向量）</p>
<p>关系预测：</p>
<p>知识库可以被表示为一张图G=(V,E,R)，其中E表示边（也就是实体间关系），当然E并不完整，我们要做的就是补充缺失的E。如R-GCN的模型图c，引入R-GCN作为编码器，把每个实体编码为一个d维向量。而后，通过DistMult因子分解（2014年知识图谱的一篇论文中的方法）作为译码器，对一个三元组(s,r,o)，其输出如下，其中<script type="math/tex">R_r\in R^{d\times d}</script>是一个对角阵：</p>
<script type="math/tex; mode=display">
f(s,r,o)=e_s^TR_re_o</script><p>最后就要对每个预测的输出计算总的loss了，用负采样法（Negative Sample）计算总的样本集合的交叉熵损失之和。</p>
<p>实体分类：</p>
<p>对实体分类任务，只是使用了堆叠的R-GCN层，并在最后一层的输出上加了一个softmax，做K分类。损失函数如下所示：</p>
<script type="math/tex; mode=display">
L=-\sum_{i\in y}\sum_{k=1}^Kt_{ik}lnh_{ik}^{(L)}</script><p>模型是通过<em>Keras</em>和<em>Theano</em>框架实现的。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><ul>
<li>关系抽取实验：</li>
</ul>
<p>数据集：WordNet(WN18)，Freebase(FB15K)</p>
<p>baseline：DistMult，CP，TransE，HolE，ComplEx</p>
<p>评价准则：MRR（Raw，Filtered），Hits @（1，3，10）</p>
<p>结果如下图：</p>
<p><img src="/images/FjF3at0ScHM1fIRX1AhkqA1nBeGl.jpg" alt="2"></p>
<ul>
<li>实体分类实验：</li>
</ul>
<p>数据集：AIFB、MUTAG、BGS</p>
<p>baseline：Feat、WL、RDF2Vec</p>
<p>评价准则：准确率</p>
<p>结果如下图：</p>
<p><img src="/images/Fv6TXnYXQFRIO98lpAaFUYxQxI2_.jpg" alt=""></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>知识图谱</tag>
        <tag>实体分类</tag>
        <tag>关系抽取</tag>
        <tag>GCN</tag>
        <tag>paperWeekly</tag>
      </tags>
  </entry>
  <entry>
    <title>《AAAI2017-Joint Copying and Restricted Generation for Paraphrase 》</title>
    <url>/2017/03/09/deeplearning/NLP-AAAI2017-%E3%80%8AJoint-Copying-and-Restricted-Generation-for-Paraphrase%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>AAAI-2017年论文。研究关于NLG（Natural Language Generation，自然语言生成）的问题。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>面向转述（释义）的NLG任务中，有Copy和Rewrite两种生成方式，但是之前的Seq2Seq模型只用了一个decoder。</p>
<p>Seq2Seq模型，也就是encoder-decoder模型，从源文本中编码为上下文向量，而后对其进行解码生成目标文本。但是典型的Seq2Seq模型无法反映出Copy-Mechanism，这样的话，出现频率较低的命名实体可能被识别为UNK。</p>
<p>另外，之前工作中的decoder通常根据当前上下文，从词库中选择可能性最高的词汇。这样做，时间开销和词库数量线性相关（$10^4-10^5$），而且可能生成不相关的<strong>命名实体</strong>或者<strong>数字</strong>。</p>
<p>已有方法：</p>
<ul>
<li>COPYNET（2016）：增加了一个额外的attention-like层，用来预测copy权重分布，而后同一个生成译码器的输出竞争。<ul>
<li>缺点1：无法解释Copy和Gen的贡献；</li>
<li>缺点2：没有使用两种writing模式（Copy和Gen）进行监督式训练；</li>
<li>缺点3：引入了更多的参数；</li>
<li>缺点4：Gen译码器只产生频繁词，丢弃了很多rewriting-pattern。</li>
</ul>
</li>
</ul>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>提出一种Seq2Seq模型，CoRe。包含两个译码器。coping decoder使用<strong>注意力</strong>模型判断复制的位置；generative decoder产生【源文本-目标文本词汇对齐表+一个很小的频繁词表】中的词汇。增加一个predictor来判断使用哪个decoder，这个预测器可以通过训练集进行训练。</p>
<ul>
<li>一、编码器，使用GRU，双向RNN，注意力机制。</li>
<li>二、复制译码器C：</li>
</ul>
<p>复制输入的每个词汇$x<em>\tau $的概率，等于注意力参数$\alpha </em>{t\tau}$。</p>
<p><img src="/images/FuiQlYnNKCSuJMRHbvkydhnoc3Mb.jpg" alt="1"></p>
<ul>
<li>三、受限制的生成译码器G：</li>
</ul>
<p>利用IBM2013年提出的模型，预先训练对齐表$A$，A包含了很多有代表性的rewrite模式，在实验中对每个词保留可能性最高的10个模式。而后增加一个额外的常用词表$U$。最后该译码器G利用的词表为：$ V_G=A(X)\cup U $。</p>
<p>该译码器与典型的Seq2Seq模型中译码器的区别就是，减少了备选词的数量，提高了效率，同时避免了模型生成一些不相关的词汇。</p>
<ul>
<li>四、预测器：</li>
</ul>
<p>预测器的学习，判断在当前上下文中，使用生成还是复制的方式产生下一个词。</p>
<script type="math/tex; mode=display">\lambda _t = \sigma(W_CS_t)</script><script type="math/tex; mode=display">P(y_t|y<t,X) = \lambda _tP_C + (1-\lambda _t)P_G</script><p>而正确的标签$\lambda _t^*$如下：</p>
<p><img src="/images/Fm2phK62FqDIMZM-4D0v_ocA667U.jpg" alt="2"></p>
<p>也就是，源语句中出现了这个词，那么就是Copy模式才是对的，否则就应该用Rewrite模式。</p>
<ul>
<li>五、学习过程：</li>
</ul>
<p>学习下面两个交叉熵损失函数，分别对应<em>结果序列的误差</em>和<em>预测器的预测任务误差</em>，使用RmsProp优化算法进行学习。</p>
<p><img src="/images/FiL5q7OQcx1O_MhMKfo-JqEpmmz8.jpg" alt="3"></p>
<p><img src="/images/Fk9NnUXyhCYwsRJRgD1HJatsB6HJ.jpg" alt="4"></p>
<p>贡献：</p>
<ul>
<li>1.模拟了转述过程中人的思考方式：Copy和Generate。</li>
<li>2.预测writing模式，利用了额外的信息。</li>
<li>3.给生成译码器增加限制，生成更加相关的信息。</li>
<li>4.相比于CopyNet等之前利用Copy-Mechenism的模型，更加容易解释清楚Copy和Rewrite起的作用，比如下面这个。</li>
</ul>
<p><img src="/images/FtXBoyM82zEJRKyqZ9nalTS2MnIz.jpg" alt="3"></p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><ul>
<li>一、数据集</li>
</ul>
<p>单句抽象式摘要：使用了Google-Deepmind团队在2015年论文《Teaching Machines to Read and Comprehend 》中生成的语料库，得到<strong>&lt;新闻，摘要&gt;</strong>对。</p>
<p>文本缩写：使用了2013年某个论文中维基百科的<strong>&lt;维基文本，简化维基文本&gt;</strong></p>
<ul>
<li>二、工具</li>
</ul>
<p>Theano，dl4mt，Fast Align</p>
<ul>
<li>三、Baseline</li>
</ul>
<p>LEAD：直接选择输入中当前概率最高的词作为输出。</p>
<p>Moses（2007）：利用词和短语的共现度进行翻译的模型。</p>
<p>ABS（2015）：一个使用了注意力机制的Seq2Seq模型。</p>
<ul>
<li>四、评价及结果</li>
</ul>
<p><img src="https://ooo.0o0.ooo/2017/03/09/58c10ed75e93e.jpg" alt=""></p>
<h4 id="5、方法的优缺点"><a href="#5、方法的优缺点" class="headerlink" title="5、方法的优缺点"></a>5、方法的优缺点</h4><p>文章自己已经把优点讲的差不多了，个人观点，不足有两个：</p>
<ul>
<li>1.将词汇是否出现在源语句中作为使用Copy-Mechenism的依据，会不会不准确。比如对于一些in、at之类的介词，可能只是偶然性的重复而不是Copy过来的。</li>
<li>2.生成译码器限制了词典数量，固然提高了效率，但是限制了生成模型的多样度，有利有弊。</li>
</ul>
<h4 id="【参考文献】"><a href="#【参考文献】" class="headerlink" title="【参考文献】"></a>【参考文献】</h4><ul>
<li>Rewrite译码器参考的文献：</li>
</ul>
<p>《A convolutional attention network for extreme summarization of source code. 》：带Attention的卷积神经网络，与本文看起来关联性不是很高，只是DNN应用于NLP的另外一个例子。</p>
<p>《Neural machine translation by jointly learning to align and translate 》：将Attention用于End2End机器翻译的文章。</p>
<p>《On using very large target vocabulary for neural machine translation. 》：在机器翻译任务中如何扩展当前词表到一个很大的词表，同时不增加训练的时间复杂度。</p>
<p>《A simple, fast, and effective reparameterization of ibm model 2. 》：2013年NAACL，一个快速的词对齐工具；在训练后，能够根据长度为n的源语句，确定目标语句的长度m，并生成对齐表$a={a_1,a_2,…,a_m}$，其中$a_i\in {0,n}\cup NULL$。</p>
<ul>
<li>Copy译码器参考的文献：</li>
</ul>
<p>《Incorporating copying mechanism in sequence-to-sequence learning. 》：CopyNet，提出了Copy-Mechenism，被本文借鉴。使用两种模式g（生成）和c（复制）的混合概率决定t时刻的输出。</p>
<p><img src="/images/FvI4jFh8roLACqeQjSzw7r1MNeEa.jpg" alt="0"></p>
<p><img src="/images/FiXTCVLVIc_EEVZxqdTvKBOVEV71.jpg" alt="1"></p>
<p>《Pointer Networks 》：2015年NIPS，提出了一种根据attention寻找copy的位置的方法，被本文借鉴。将attention作为指针，来决定将输入序列的哪几位作为输出。</p>
<ul>
<li>其他文献：</li>
</ul>
<p>《On the Properties of Neural Machine Translation: Encoder–Decoder Approaches》：Bengio发表在ACL2014上的论文，评估了Seq2Seq模型，并提出了两个新的Seq2Seq模型。</p>
<p>《Teaching machines to read and comprehend. 》：2015年NIPS，提出一种生成大规模语料库的方法，并在此基础上训练并对比不同模型的机器阅读理解能力。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention上的Attention-《Attention-over-Attention Neural Networks for Reading Comprehension》</title>
    <url>/2017/04/26/deeplearning/NLP-Attention-Cloze-arXiv2016-%E3%80%8AAttention-over-Attention%20Neural%20Networks%20for%20Reading%20Comprehension%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>2016年7月发表在arXiv上，哈工大讯飞联合实验室的论文。和另一篇文章《Consensus Attention-based Neural Networks for Chinese Reading Comprehension》同时阅读。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>完形填空形式<script type="math/tex">(q,d,a)</script>的阅读理解任务的解决模型。</p>
<p>中文的类似形式的任务所需要的数据集。</p>
<p>以往的模型多利用文档级别的注意力机制配合RNN来做，缺少中文相关的数据集，而且模型中有很多无法训练的超参数需要调。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>AoA：Attention over Attention Reader，主要受AS-Reader的启发。（2016年本文作者稍早一些的论文提出的模型CAS-Reader可以看作此模型的简化版本）</p>
<ul>
<li>1.把q和d通过共享词向量矩阵转化为向量表示，并通过双向GRU将d和q中每个单词表示为其上下文向量。其表示分别是<script type="math/tex">h_{doc}\in R^{|D|\times d},h_{query}\in R^{|q|\times d}</script>，其中d是GRU隐层单元的二倍。</li>
<li>2.计算相似度矩阵<script type="math/tex">M</script>，其中每个元素表示q和d中每个单词表示的相似度，<script type="math/tex">M_{ij} = h_{doc}(i)\odot h_{query}(j)</script>。</li>
<li>3.计算对于q中每个单词，文档级别的注意力权重<script type="math/tex">\alpha(t)</script>，计算的方式就是对<script type="math/tex">M</script>矩阵的每一列应用softmax函数。最终计算出<script type="math/tex">\alpha = [\alpha(1),...,\alpha(|q|)]</script>，注意这还是一个<script type="math/tex">R^{|D|\times |q|}</script>的矩阵，但是每一列的意义变成了对于q中每个单词，文档d中每个单词的重要性。</li>
<li>4.之前的论文中的模型CAS-Reader就直接把不同time-step的<script type="math/tex">\alpha</script>进行相加、平均或者最大操作得到最终的注意力权重。而本文首先计算对于d中每个单词，问题级别的注意力权重<script type="math/tex">\beta(t)</script>，计算的方式与3中所述类似，也是应用softmax函数。而后对<script type="math/tex">\beta</script>进行平均，平均后的意义可以认为是对于文档d来说，q中每个单词的重要程度。</li>
<li>5.而后，对<script type="math/tex">\alpha</script>和<script type="math/tex">\beta</script>矩阵进行点乘，这个点乘可以认为是一个互信息，既利用了d中单词对q的重要性也利用了q中单词对d的重要性，点乘结果就是最终的注意力权重<script type="math/tex">s\in R^{|D|}</script>了。</li>
<li>6.最后一步，和AS-Reader类似，就是把相同词的权重加起来，进行预测。</li>
</ul>
<p><img src="/images/Fux0KRqCk4lQB_2RJNqOqssc38Lp.jpg" alt="1"></p>
<p>中文数据集：</p>
<p>另一篇稍早的论文中，提出了人民日报和儿童童话故事基础上生成的两个中文完形填空数据集。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>实验主要在三个公开数据集上进行：CNN/DailyMail和CBT，结果就是单模型上就几乎比之前的模型集成起来效果好，state-of-the-art。</p>
<p><img src="/images/FsiTWU4mw6tWns7vtmuJ_d0qNVYl.jpg" alt=""></p>
<h4 id="5、方法的优缺点"><a href="#5、方法的优缺点" class="headerlink" title="5、方法的优缺点"></a>5、方法的优缺点</h4><p>不足之处：</p>
<ul>
<li>模型较为复杂，计算量较大。</li>
<li>论文开始说已有的方法的不足之一就是有很多超参数需要调，但是看AoA也要调这些超参数啊。</li>
</ul>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4>]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>机器阅读理解</tag>
        <tag>完形填空</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention-ICLR2015-《Neural machine translation by jointly learning to align and translate》</title>
    <url>/2017/03/09/deeplearning/NLP-Attention-ICLR2015-%E3%80%8ANeural-machine-translation-by-jointly-learning-to-align-and-translate%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>发表在ICLR2015，Bengio大神的文章。注意力机制在机器翻译（自然语言处理）方面的应用。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>已有的用于机器翻译的神经网络模型，通过将输入的句子编码为固定长度的向量再译码为输出句子进行翻译，（已有研究证明）这种方法存在的问题就是句子长度增加时翻译效果会下降。</p>
<p>已有的方法比如2014年提出的两个Seq2Seq模型：《Sequence to sequence learning with neural networks》和《Learning phrase representations using RNN encoder-decoder for statistical machine translation》。</p>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>编码器：典型的双向RNN，接受输入<script type="math/tex">(x_1,x_2,....,x_{T_x})</script>，产生输出<script type="math/tex">(h_1,h_2,...,h_{T_x})</script>，注意这里的<script type="math/tex">h_i</script>是由正向RNN和反向RNN的隐藏输出拼接的向量。</p>
<p>译码器：</p>
<p>i时刻的条件概率<script type="math/tex">p(y_i|y_1,...,y_{i-1},x)=g(y_{i-1},s_i,c_i)</script>，而<script type="math/tex">s_i</script>是i时刻RNN的隐藏层状态,<script type="math/tex">S_i=f(s_{i-1},y_{i-1},c_i)</script>，<script type="math/tex">y_i</script>是i时刻的正确标签。</p>
<p><script type="math/tex">C_i</script>是i时刻的上下文（随i变化，体现了注意力机制）。计算如下公式：</p>
<p>公式中的<script type="math/tex">h_j</script>表示输入向量被编码器映射后的输出。</p>
<p><img src="/images/FrekHF_D9R9cRHUT-kCCB-1nBnOY.jpg" alt="0"></p>
<p>而<script type="math/tex">e_{ij}</script>表示一个对齐模型（alignment model），计算出i位置的输出和j位置输入之间的相关度。这个模型a和整个模型同时训练。</p>
<p><img src="/images/FuvQBnF6bTyIwg2f_F9511b7tst9.jpg" alt="2"></p>
<p><img src="/images/FkEMFKzFA91BvhB3a2MV3D_42z8-.jpg" alt="1"></p>
<p>通过每次确定输入中的一些位置，加强这些位置的权重并作为上下文，而不是使用原本固定的上下文。做法是编码时不将输入编码为固定长度向量，而是编码为向量集合。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>实验：英语到法语的翻译实验。</p>
<ul>
<li>一、语料库</li>
</ul>
<p>来自WMT‘14的英法双语语料库。</p>
<ul>
<li>二、模型</li>
</ul>
<p>本文提出的方法和2014年提出的RNNencdec。句子长度分别为30、50的情况下，使用SGD+Adadelta方法进行训练。以及传统phrase-based的模型Moses。</p>
<ul>
<li>三、结果</li>
</ul>
<p>使用RELU进行评价，发现结果比RNNencdec好。而且在训练数据少于Moses的情况下，基本达到了Moses的效果（不包括UNK的时候）。</p>
<h4 id="5、方法的优缺点"><a href="#5、方法的优缺点" class="headerlink" title="5、方法的优缺点"></a>5、方法的优缺点</h4><p>如作者所说，如何更好的处理低频词。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://zhuanlan.zhihu.com/p/22081325" target="_blank" rel="noopener">知乎专栏</a></p>
<p><a href="http://www.cnblogs.com/robert-dlut/p/5952032.html" target="_blank" rel="noopener">cnblogs</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-DCNN-ACL2014-《A convolutional neural network for modeling sentences》</title>
    <url>/2017/03/23/deeplearning/NLP-DCNN-ACL2014-%E3%80%8AA-convolutional-neural-network-for-modeling-sentences%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>N.Kal等人发表在ACL2014，提出了一种DCNN模型，解决句子的语义建模问题。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>句子建模，目的是分析和表示句子的语义内容，以进一步进行分类或者生成，是很多NLP任务的基础。其核心是一个特征函数，定义了如何根据单词和n-gram的特征抽取句子的特征。</p>
<p>已有的方法：</p>
<ul>
<li>基于组合的模型，通过对单词向量之间的组合操作得到句子的表示；组合函数可以是代数运算，也可以是通过学习得出的与句法结构和词性相关的操作；</li>
<li>自动抽取逻辑形式来表示句子（Zettle  et.2005）；</li>
<li>基于神经网络的模型。</li>
</ul>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><ul>
<li>一、网络的定义</li>
</ul>
<p>提出一个基于神经网络的模型解决句子建模的问题，该模型反复叠加一维卷积层和动态k-max池化层。我们以下图为例来解释：</p>
<p>设词向量长度为d，下图中d=4；</p>
<p>设filter长度为m，下图中m=2、3；</p>
<p>设输入的句子单词数为s，下图中s=7；</p>
<p>设池化层的动态参数为k，下图中k=3、5；</p>
<p>输入序列是一个矩阵I，维度为$R^{d<em>s}$，下图中维度为$R^{4</em>7}$.</p>
<ul>
<li>一维卷积层：对I的每一行（词向量的每个维度），对第i个单词，计算filter和$I[n,i-m+1:i]$向量点乘的结果。一维卷积分为wide和narrow两种，wide就是从第1个单词开始卷积，最后结果矩阵是$R^{d<em>s+m-1 }$，narrow就是从第m个单词开始卷积，最后结果矩阵是$R^{d</em>s}$。</li>
<li>动态k-max池化层：是最大池化层的一种泛化，对一个序列中的值，不是返回最大的一个而是最大的k个，具体见下图，结果矩阵是$R^{d*k}$。</li>
<li>Folding层，这个层是为了避免FC层之前词向量的每个维度之间互相没有联系，因此把矩阵每两行按元素相加形成一个$R^{d/2*h}$层，该层在卷积层后面池化层前面。</li>
<li>多个Feature Map，会提取出多个特征图，保证特征提取的多样性。</li>
</ul>
<p><img src="/images/Fjpva8OBIVDVFTGCgiU0TFZMbxYO.jpg" alt="1"></p>
<ul>
<li>特征图的推导：特征图可以认为是网络的权重，其中卷积层涉及的节点相互连接，pooling层选择的节点相互连接。对于没有folding层的网络，每一行矩阵形成的子图可以认为是捕捉到的一种关系</li>
</ul>
<ul>
<li>二、网络的解释</li>
</ul>
<p>长度为m的filter，能够捕捉n-gram的pattern，其中n&lt;m，因此m通常设为一个较大的值比如10；</p>
<p>而pooling层，能够推导出词汇的绝对位置而且不会影响其相对位置。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>电影情感分析 &gt; 已有的state-of-the-art</p>
<p>TREC数据集的QA任务 = 已有的state-of-the-art</p>
<p>分析推特推文情感 &gt;&gt;2009年的baseline</p>
<h4 id="5、方法的优缺点"><a href="#5、方法的优缺点" class="headerlink" title="5、方法的优缺点"></a>5、方法的优缺点</h4><p>该方法的特点有：</p>
<ul>
<li>一维卷积是对词向量的每个维度分别进行卷积的，这点与之后的模型不太一样；</li>
<li>动态Pooling和k-max-pooling，能够捕捉到句子不同位置之间词的关联；</li>
<li>能够应用于难以解析的句子（不需要外部的解析树）。</li>
</ul>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li><a href="http://blog.csdn.net/malefactor/article/details/51078135" target="_blank" rel="noopener">自然语言处理中CNN模型几种常见的Max Pooling操作</a></li>
<li><a href="http://glacier.iego.net/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88cnn%EF%BC%89%E5%9C%A8nlp%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/" target="_blank" rel="noopener">卷积神经网络（CNN）在NLP中的应用</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-CNN-EMNLP2014-《Convolutional neural networks for sentence classification》</title>
    <url>/2017/03/27/deeplearning/NLP-CNN-EMNLP2014-%E3%80%8AConvolutional-neural-networks-for-sentence-classification%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>纽约大学的Kim Y发表在EMNLP2014，使用CNN进行文本分类的经典论文。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>CNN已经在计算机视觉领域得到广泛应用，但是自然语言处理方面之前的研究热点在词向量。</p>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>未登录词的词向量可以用0或者随机小的正数表示。</p>
<p>使用Google预训练的长度为k的word2vec词向量，句子长度为n，那么输入就是一个n*k的矩阵。对于每个卷积核，其卷积窗口为h，那么对第i个单词进行卷积的结果$c_i$如下公式：</p>
<script type="math/tex; mode=display">c_i=f(w\cdot x_{i:i+h-1}+b)</script><p>该卷积核卷积的Feature Map如下：</p>
<script type="math/tex; mode=display">c = [c_1,c_2,...,c_{n-h+1}]</script><p>而后通过一个max-pooling层，$\hat c = max{c}$，这可以认为是一个卷积核产生的对句子影响最大的特征。</p>
<p>如果我们有m个卷积核，那么这m个卷积核得到的结果拼接起来送到一个全连接层并进行softmax分类。</p>
<p>两个channel模型在实验中效果不错，一个channel的词向量固定，另一个随着BP在训练中进行微调。</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=53827ed376804e73d2d1273ee5a94796" alt=""></p>
<p>正则化：在倒数第二层进行随机的dropout。</p>
<p>梯度限制：对梯度的二范数进行阈值k限制。</p>
<ul>
<li>模型的变种</li>
</ul>
<p>对模型的以下变种进行了测试：</p>
<p>CNN-rand：词向量随机初始化；</p>
<p>CNN-static：词向量预训练，且不变；</p>
<p>CNN-NonStatic：词向量预训练，且微调；</p>
<p>CNN-MultiChannel：多个channel，如上所述。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>MR</p>
<p>SST-1</p>
<p>SST-2</p>
<p>Subj</p>
<p>TREC</p>
<p>CR</p>
<p>MPQA</p>
<p>实验的结论和分析如下：</p>
<ul>
<li>1.在上述7个数据集中，有4个取得了state-of-the-art的结果；</li>
<li>2.相比于随机初始化词向量，使用预训练的效果好得多；</li>
<li>3.训练中微调词向量会让结果更好；</li>
<li>4.多channel在小规模数据集上表现更好；</li>
<li>5.如下表格，词向量微调之后和预训练时的对比，可以发现针对特定任务的确应该微调词向量使之更加符合这项任务的本质。</li>
</ul>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=6d4b6982ec3792ff9f58f46bb70175c0" alt=""></p>
<h4 id="5、方法的优缺点"><a href="#5、方法的优缺点" class="headerlink" title="5、方法的优缺点"></a>5、方法的优缺点</h4><p>优点：只需一层CNN，没有网络层数的堆叠。实验详细。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="http://www.jeyzhang.com/cnn-apply-on-modelling-sentence.html" target="_blank" rel="noopener">一篇博客-这篇论文阅读笔记</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>情感分析</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-Attention-EMNLP2015-《Effective Approaches to Attention-based Neural Machine Translation》</title>
    <url>/2017/03/10/deeplearning/NLP-Attention-EMNLP2015-%E3%80%8AEffective-Approaches-to-Attention-based-Neural-Machine-Translation%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>文章来源于EMNLP2015，是<a href="https://github.com/terryum/awesome-deep-learning-papers" target="_blank" rel="noopener">awesome-deep-learning-papers</a>中NLP方面的论文，对注意力机制进行了进一步的探究。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>问题：Attention-Based-NMT（基于注意力机制的神经机器翻译）的基础上进行进一步加强。</p>
<p>已有方法：</p>
<ul>
<li>Attention-EMNLP2015-《Effective Approaches to Attention-based Neural Machine Translation》：将注意力机制用于NMT；</li>
<li>CVPR2015-《Show,Attend and Tell-Neural Image Caption Generation with Visual Attention》：提出hard和soft attention机制，用于图像摘要生成。</li>
</ul>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>Encoder-Decoder，Encoder的输入是(mini_batch, embedding_dim, scentence_length)，就像下面这个三层LSTM，其中横线表示上下文向量，竖线表示LSTM的输入输出。Encoder的输出是上下文向量，也就是最后一层的hidden state（每个时刻一个向量输出）。</p>
<p><img src="/images/Frv6wdBbGJM1YRTOpTOJIA0Qd7Wl.jpg" alt="1"></p>
<p>Decoder中第一层的hidden state被初始化为encoder输出的上下文向量。在最上层放置一个softmax层，t时刻输出当前时刻词汇的条件概率。t时刻的输出会在t+1时刻作为最下层LSTM的输入。</p>
<p><img src="/images/FspCPdLALGkFnugBNviZMQT9F_93.jpg" alt="2"></p>
<p>提出了两种类型的注意力机制：</p>
<ul>
<li>1.全局Global Attention，同时考虑所有输入位置。</li>
</ul>
<p><img src="https://ooo.0o0.ooo/2017/03/11/58c361e80c31b.jpg" alt="6"></p>
<p><img src="/images/Fs7xg9IAPRpsed8tqcSKSX5pKgG2.jpg" alt="1"></p>
<ul>
<li>2.局部Local Attention，同一时间只考虑部分输入位置。</li>
</ul>
<p><img src="/images/FlrwqFtlyrZdzWBxxGH1OteqmrWx.jpg" alt="7"></p>
<p><img src="/images/FoFSPbQQ0bBWkO6PRJ81czpjiADD.jpg" alt="5"></p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>模型训练的细节略过，有需要可以回来看。</p>
<p>参加了WMT’15的英语到德语双向翻译，并在BLEU得分上获得第一名（state-of-the-art）。</p>
<p>分析：</p>
<ul>
<li>带有attention的模型在长句子上表现更好。</li>
<li>local比global学习到的alignment的AER更高。</li>
<li>local-p是所有模型中表现最好的。</li>
</ul>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-notes6.pdf" target="_blank" rel="noopener">CS224n</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-EncoDecoRepr-EMNLP2014-《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》</title>
    <url>/2017/03/28/deeplearning/NLP-EncoDecoRepr-EMNLP2014-%E3%80%8ALearning-Phrase-Representations-using-RNN-Encoder%E2%80%93Decoder-for-Statistical-Machine-Translation%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p><a href="http://www.kyunghyuncho.me/" target="_blank" rel="noopener">Kyunghyun Cho</a>博士（蒙特利尔大学）发表在EMNLP2014的一篇论文，提出了RNN Encoder-Decoder模型用于机器翻译。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>机器翻译问题，之前的方法都是SMT（统计机器翻译）的方法，少有NMT（神经网络机器翻译）的。</p>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>模型由两个RNN组成。</p>
<p>第一个RNN（编码器）将输入序列编码为固定长度的向量（表示），即下图的C，是t时刻RNN的隐藏状态$h_t$；</p>
<p>第二个RNN（译码器）将中间表示译码生成输出序列，和RNN不同的是$h_t,y_t$都和C相关。</p>
<script type="math/tex; mode=display">h_t=f(h_{t-1},y_{t-1},C)</script><script type="math/tex; mode=display">y_t=g(h_t,y_{t-1},C)</script><p>两个RNN被联合训练，最大化对数条件概率。</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=4f7c0adce3ffef4b8c5f8e3a58d0e858" alt="1"></p>
<p>此外，本文还提出了GRU这个LSTM的简化模型，GRU的隐层单元结构更简单、同时很有效。</p>
<p><img src="/images/FrHQHvzQzelxQhGwORkMnrjLPn7M.jpg" alt="1"></p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=c7ecbb27742c44ab07ae5417500eaf52" alt="2"></p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>模型在WMT’14的英语到法语的数据集上进行了测试。</p>
<p>Baseline：基于Moses的p’hrased-based-SMT模型。</p>
<p>模型:</p>
<p>RNN</p>
<p>CSLM+RNN</p>
<p>CSLM+RNN+WP</p>
<p>其中，WP是在神经网络训练中，对UNK的出现进行惩罚（避免UNK）。CSLM（连续空间语言模型）是一种传统的方法（2003），被加进来和RNN一起使用，看一看有没有什么优化。</p>
<p>结果：</p>
<p><img src="/images/Ftikl5pTuQ58JnksVXnaqz4SJClX.jpg" alt=""></p>
<p>最后，类似word2vec，本模型在学习语言模型的同时能够学习到连续空间的词向量和短语向量（就像标题中说的），而且从可视化结果来看学习的效果不错。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>Seq2Seq</tag>
        <tag>RNN</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
  <entry>
    <title>seq2seq的NMT模型怎样训练-论文《Massive Exploration of Neural Machine Translation Architectures》</title>
    <url>/2017/04/11/deeplearning/NLP-Hyperparams-train-arXiv2017-%E3%80%8AMassive%20Exploration%20of%20Neural%20Machine%20Translation%20Architectures%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>来源于arXiv2017，Google Brain利用超过25万个GPU小时，对循环神经网络的训练做出了一些工作，得到了一些经验性的做法。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>NMT（神经机器翻译）网络虽然效果不错，但是经常需要几天甚至几周才能训练完成，如果加上调超参数那训练时间更加无法忍受。</p>
<p>已有的方法就是如同炼丹一样，依靠调参的人的经验，尽量减少开销。Google Brain通过自家GPU，给出了encoder-decoder模型下NMT任务超参数选择的一些规则，并分析了超参数的选择会对性能产生怎样的影响。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法以及进行的实验"><a href="#3、提出的解决方法以及进行的实验" class="headerlink" title="3、提出的解决方法以及进行的实验"></a>3、提出的解决方法以及进行的实验</h4><p>提出的解决方法，就是用Google的GPU，先做一次这个事，给后来者趟出一条路。</p>
<p>首先说一下用于此次试验的模型，是一个encoder-decoder模型，包含两个双向RNN。并使用了注意力机制。损失函数是负对数损失，优化算法是SGD。</p>
<p>试验使用了WMT‘15英语到法语的语料库，并用了Moses的脚本进行了分词，使用了BPE进行了词库优化，最后的词库大小为37K。（好小。。。）</p>
<p>试验的beam search使用了<strong>length normalization</strong>和<strong>coverage penalty</strong>。前者是beam search生成一个序列时为了避免短序列倾向问题。后者是为了鼓励将翻译进行到原序列的最后（而不是在中间就停止）。具体可以参加Google-NMT系统的论文。</p>
<p>实验在8个K40/K80上进行，每个模型训练2500万步，并使用不同初始值训练4次。</p>
<p>实验中，一次改变一个超参数。</p>
<p><img src="/images/Ft1D_swbgiZAfvPCDgmzyhw7fnR0.jpg" alt="1"></p>
<h4 id="4、实验结果和分析"><a href="#4、实验结果和分析" class="headerlink" title="4、实验结果和分析"></a>4、实验结果和分析</h4><p>评估的指标：BLEU、困惑度、模型大小、收敛时间。</p>
<p>词向量维度：从128每次乘以2改变到2048，发现对各个指标影响不大，128维就够好了。这里作者分析了可能是优化技术的缺陷导致无法发挥维度优势。</p>
<p>RNN-cell：试验中LSTM略好于GRU，但是它们都比vanillaRNN好得多。</p>
<p>模型的层数（和残差连接）：编码器的层数2层就够了，而译码器4层比2层稍微好一点点，残差连接在深度网络中有作用，但容易发散无法收敛。</p>
<p>单向和双向：双向比单向要好，但好的不多；与此同时，如果是单向RNN的encoder，那么把输入序列reverse一下可以提高性能。</p>
<p>注意力机制：用了下面两种注意力机制（add和mul），并根据Wh相乘后的维度不同分类，同时加上了不使用注意力机制时的对比，发现add比mul总是要好一点，而它们都远好于不用注意力机制的模型。</p>
<p><img src="/images/FuZWrlHT_7CykNLHBgT5DVbAUMT5.jpg" alt="2"></p>
<p>beam search：选择beam的数量和length penalty很重要，非常影响最后的结果。</p>
<p>最后，这个NMT模型的超参数被这个团队给tune下来了，如下所示。</p>
<p><img src="/images/Fnlo7muGrs44zVNaZNt7DIL-iS1Y.jpg" alt="3"><em>**</em></p>
<h4 id="5、方法对比"><a href="#5、方法对比" class="headerlink" title="5、方法对比"></a>5、方法对比</h4><p>如下图所示，他们的方法在2016年只比google自己用的翻译差一点点，但是GNMT太复杂又不开源。</p>
<p>本文所有的代码都被Google Brain放到了<a href="https://github.com/google/seq2seq/" target="_blank" rel="noopener">https://github.com/google/seq2seq/ </a>上面，可以用来复现。</p>
<p><img src="/images/Fn7CLSeVnS0r5j_l9dyyJ_OTWZmu.jpg" alt="4"></p>
<p>本文作者好人，调好了自己的模型还不忘写出来给大家参考借鉴。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>模型训练</tag>
      </tags>
  </entry>
  <entry>
    <title>一次解决两个任务-《Question Answering and Question Generation as Dual Tasks》</title>
    <url>/2017/06/19/deeplearning/NLP-QA&amp;QG_Dual-%E3%80%8AQuestion%20Answering%20and%20Question%20Generation%20as%20Dual%20Tasks%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>paperWeekly的第十八周QA论文，来自微软研究院。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>Question Generation（QG）接受一个句子a作为输入，并生成一个问题q，q的答案就是a，QG是一个序列生成的问题，计算P(q|a)。</p>
<p>Question Answering（QA）接受一个序列作为输入，给定一组候选答案，输出可能性最高的一个或者一组答案，可以认为是一个排序问题，计算P(a|q)。</p>
<p>本文提出的方法尝试训练一个同时应用于这两种任务的模型。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>用seq2seq解决QG，用RNN解决QA。模型每次输入m个QA对正例和m个负例，并通过QG和QA的模型计算各自的loss，再加上一个正则化项<script type="math/tex">l_{dual}</script>一起计算参数梯度并更新参数。</p>
<p><img src="/images/FrzsLo6uA9ceyCMAZyk2zhe5lqZa.jpg" alt=""></p>
<p>正则化dual项如下第二个公式所示，利用了QA和QG输入输出的对偶关系。</p>
<p><img src="/images/FrKMKyqcTdsGVQkpDrtfAIOXfHtv.jpg" alt=""></p>
<p><img src="/images/FkoL3QNZzc2V8iI-ujRpAyBPAR1n.jpg" alt=""></p>
<p>这个方法之前的一些方法应用在multi task上的时候，要不就是存在pre-train要不就是有一个main task，但是这个方法两个方法是对等的而且是jointly-learned。同样的，该方法与GAN思想有一点类似但是也存在着不同。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>在MARCO和SQuAD上的实验表明了方法的有效性。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>多任务</tag>
        <tag>QA</tag>
        <tag>QG</tag>
        <tag>SQuAD</tag>
        <tag>MARCO</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-ICML2016-《Exploring the Limits of Language Modeling》</title>
    <url>/2017/03/29/deeplearning/NLP-ICML2016-%E3%80%8AExploring-the-Limits-of-Language-Modeling%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>Google Brain团队的研究人员发表在ICML2016。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>大规模数据集（语料库）对于训练复杂模型是必要的，而复杂模型对解决自然语言理解这种复杂任务是必要的。本文旨在在大规模的语料库（one billion word benchmark）上训练语言模型。</p>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><ul>
<li>一、背景</li>
</ul>
<p>词表较大时的softmax改进：重要性采样importance sampling，话说softmax的改进分为两种：各种softmax变种（分层softmax、分片softmax、等）以及基于采样的方法。</p>
<p>基于采样的方法：</p>
<p>蒙特卡洛采样，估计一个函数$f(x)$的积分，如果函数无法解析，那么就随机取一些点$x_1,x_2,…,x_n$，用这些点的函数值求区间积分，就是用小矩形面积之和估计函数面积的思想。</p>
<p>重要性采样，就是在蒙特卡洛采样的基础上，根据一个重要性函数$p(x)$，根据这个值确定当前的x代表的矩阵的宽度，$p(x)$越大说明这个点附近越重要，要多采样几个点，宽度就要降低。具体到这里，就是选择1个正确单词和k-1个错误单词，进行k类分类并优化。</p>
<p>噪声对比估计（NCE）：另一种基于采样的方法。其思想是训练一个模型来区分目标词语与噪声。于是，待解决的问题就由预测正确的词语简化为一个二值分类器任务，分类器试图将正确的词语与其它噪声样本中区分开来。NCE方法的理论证明很完善，随着噪声样本数量增加，其导数接近softmax的梯度。</p>
<p>负采样（NS），可以被认为是NCE的近似版本。</p>
<p><img src="/images/FhF4IUiHJe8yaZkR_hRKzyM4NdFz.jpg" alt="1"></p>
<ul>
<li><p>二、模型</p>
<p>1.分析了NCE和IS之间的相似和不同点。</p>
<p>2.CNNSoftmax，本文的词向量不是来自一个$R^{V*h}$的词向量矩阵，而是$e<em>w=CNN(char</em>{s_w})$。由于是charCNN，所以对于发音相近的词无法区分得很好，所以加上一个校正因子。</p>
<script type="math/tex; mode=display">
z_w=h^TCNN(chars_w) + h^TMcorr_w</script><p>这里的M是个参数矩阵，而$corr_w$是一个低维的词向量。</p>
<p>3.charLSTM（2011、2013），一次预测一个字母，所以很高效，但是即使在很小的数据集比如PTB中效果也不好。这篇文章推测，这主要是一次输入一个字母导致time-step数量太多。</p>
<p>这篇文章把一个wordLSTM的隐状态输入到一个小的LSTM中一次预测一个字母。</p>
<p>首先训练wordLSTM，而后固定其参数训练顶层的LSTM。</p>
</li>
<li><p>三、贡献</p>
<p>本文的工作贡献体现在以下几点：</p>
</li>
</ul>
<ul>
<li>1.总结合并了大规模语言模型的研究。</li>
<li>2.在CharCNN上提出了一种高效的softmax损失函数。</li>
<li>3.单模型和集成模型都获得了state-of-the-art的结果。</li>
<li>4.开源了所有代码。</li>
</ul>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>在One Billion Word Benchmark（2013）上进行了评估。</p>
<p><img src="/images/Fv9bsHk2XlXoTvfNRCbYkbN23duS.jpg" alt="3"></p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="http://www.wxhaowen.com/article_dc5a954b35fc40dcb0b2523f3b4c4ae9.shtml" target="_blank" rel="noopener">漫谈词向量之基于Softmax与Sampling的方法</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>从维基百科中找答案-《Reading Wikipedia to Answer Open-Domain Questions》</title>
    <url>/2017/07/09/deeplearning/NLP-QA-wiki-%E3%80%8AReading%20Wikipedia%20to%20Answer%20Open-Domain%20Questions%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>paperWeekly的第二十一周QA论文，斯坦福大学和FaceBook-AI于2017年4月发表。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>针对开放域QA中出现的事实型问题（针对某个事实提问，比如：<em>中国有多少个省份？</em>），认为问题根据维基百科的内容提供相应的回答。这样一个问题相对于利用知识图谱（freeBase等）作为知识库，能提供更多更新的内容，但是如何找到与问题相关的内容并产生回答要更复杂一些。</p>
<p>要解决这个问题，首先要根据问题，从维基百科中找到一些相关的文章；而后从这些文章中抽取出答案。</p>
<p>之前的工作，如2010年IBM的Ferrucci等人的DeepQA，从维基百科、KB、字典、新闻等不同知识源中抽取信息回答问题，该方法正确回答问题依赖知识源中知识的冗余。（所以后来阅读理解这个子领域得到了研究关注，专注于提高机器的阅读和理解能力）</p>
<p>而单纯的阅读理解模型，将与问题相关的上下文给定了（SQuAD、CBT、CNN等）简化了问题，但是对构建一个开放域的QA系统来说这是不够的。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>DrQA系统，包含几个部分：</p>
<ul>
<li>1.Document Retriever，给定一个问题，返回wikipedia中相关的若干篇文章；</li>
</ul>
<p>在这里用了TF-IDF词袋向量空间模型下的倒排索引，而且引入了n-gram的词序特征，每个问题返回5篇文章。</p>
<ul>
<li>2.Document Reader，从这若干篇文章中找到答案的answer spans。</li>
</ul>
<p>给定一个具有l个字符的问题q，和一个具有n个段落每个段落有m个字符的文档d：</p>
<p>1.文档编码：</p>
<p>首先将d中每个段落的字符输入RNN，输出每个字符的表示<script type="math/tex">p_i</script>，可以认为是编码了上下文的语义信息的向量。RNN的输入信息包含：词向量、Exact-match（该字符是否出现在问题中）、字符特征（词性、命名实体、词频）、对齐问题向量（也就是对每个字符与问题中每个字符通过非线性映射的词向量进行对比）</p>
<p>2.问题编码：</p>
<p>将q中每个字符的词向量输入RNN，输出q的表示向量，如下图。</p>
<p><img src="/images/FsC1poFJH7Sdjli8IpbvMFkVxQuk.jpg" alt=""></p>
<p>3.预测：</p>
<p>需要预测答案在文档中的开始和结束位置，将上面两个编码输出作为输入，分别训练两个分类器。</p>
<p><img src="/images/FvpVgc8Z7hqKaSXjk2RnpW5rrzMx.jpg" alt=""></p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>对document retriever，性能超过了维基百科的elasticSearch搜索引擎；</p>
<p><img src="/images/FonFWevkl58cAkAWcyejpjxbfY8S.jpg" alt=""></p>
<p>对document reader，SQuAD数据集上获得了不错的成绩（2017年2月时的第一名）；</p>
<p><img src="/images/FnJJhR-yknYUJogs0b8hjIT8mAS5.jpg" alt=""></p>
<p>对整个系统，用CuratedTREC等三个数据集测试了系统开放域QA任务上的表现（利用远程监督的方法抽取训练集），将YodaQA作为对比的目标。</p>
<p><img src="/images/FvzeVBZtZqmyCS5l8-xNmXDqDRLH.jpg" alt=""></p>
<h4 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h4><p> 本文提出的方法没有使用维基百科中文章之间的图关系，仅仅当做若干个互不相干的文章，因此该方法能够推广到其他的知识源中。</p>
<p>本文同时也提出了最近提出的很多阅读理解模型应用于开放域问答系统的一种途径。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>机器阅读理解</tag>
        <tag>多任务</tag>
        <tag>QA</tag>
      </tags>
  </entry>
  <entry>
    <title>几篇KBQA论文阅读</title>
    <url>/2018/06/15/deeplearning/NLP-KBQA-Paper-Reading-2018-06/</url>
    <content><![CDATA[<p>七篇和KBQA多多少少有些相关的论文，有些精读有些只是略读。</p>
<h3 id="1-《Improving-Natural-Language-Inference-Using-External-Knowledge-in-the-Science-Questions-Domain》"><a href="#1-《Improving-Natural-Language-Inference-Using-External-Knowledge-in-the-Science-Questions-Domain》" class="headerlink" title="1.《Improving Natural Language Inference Using External Knowledge in the Science Questions Domain》"></a>1.《Improving Natural Language Inference Using External Knowledge in the Science Questions Domain》</h3><p>在NLI（自然语言推理）任务中，提出一种结合知识库的方法。</p>
<p>已有的引入外部知识的方法，无非是：</p>
<p>1.引入事实（facts），包含实体以及他们之间的关系；</p>
<p>2.引入词法信息，比如一个实体的所有同义词；</p>
<p>3.引入常识，比如所有涉及到的concept组成的subgraph。</p>
<p>提出的使用外部知识的方法：</p>
<p>text-only，graph-only，text-and-graph</p>
<p>Text-Based Model：把Hypothesis和Premise的文本作为输入的一个多层分类模型。</p>
<p>Graph-Based Model：对Premise和Hypothesis分别构建一个sub-graph，包含里面出现过的entity，以及相应的relation。</p>
<p>用来验证的数据集：SciTail</p>
<h3 id="2-《Exploiting-Rich-Syntactic-Information-for-Semantic-Parsing-with-Graph-to-Sequence-Model》"><a href="#2-《Exploiting-Rich-Syntactic-Information-for-Semantic-Parsing-with-Graph-to-Sequence-Model》" class="headerlink" title="2.《Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model》"></a>2.《Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model》</h3><p>发表在EMNLP’2018的短文，腾讯AI实验室与IBM Search的工作。</p>
<p>之前的语义解析（Semantic Parsing）方法通常都是在decoder端的改进，encoder就是简单的seqLSTM来编码word-order的特征。但是句法特征(syntactic features)对于编码非常重要。</p>
<p>在Semantic Parsing的几个Benchmark数据集Jobs640，ATIS和Geo880上达到了与SOTA差不多的性能，而且对抗样本上的鲁棒性更强。</p>
<p>方法：</p>
<ol>
<li>利用syntactic graph来表示word-order，dependency和constituency特征。</li>
</ol>
<ul>
<li>word-order特征，句子里每个单词是一个节点互相连接。（双向）</li>
<li>dependency特征，代表了单词直接的语法关系，依赖信息非常重要（Reddy等人证明依赖解析树能够直接转化为Logical Form），将单词之间以有向边连接并指向依赖标签。</li>
<li>constituency特征，constituency解析的结果代表了phrase结构信息，把constituent tree中非终止节点以及它们之间的边加入到syntactic graph中。</li>
</ul>
<ol>
<li>使用graph2seq模型（Xu et al. 2018），首先使用graph encoder对syntactic graph进行编码，而后使用RNN+Attention进行解码得到Logical Form。</li>
</ol>
<ul>
<li>把图中节点的text信息转化为embedding表示该节点；</li>
<li>找到每个节点的前向邻居和后向邻居；并以一种规则更新节点的表示；</li>
<li>把所有节点表示feed到MLP+Max-Pooling网络中得到graph-embedding。</li>
<li>解码阶段使用graph embedding初始化隐状态，Attention的M是节点的表示。</li>
</ul>
<h3 id="3-《Learning-to-Map-Sentences-to-Logical-Form-Structured-Classification-with-Probabilistic-Categorial-Grammars-》"><a href="#3-《Learning-to-Map-Sentences-to-Logical-Form-Structured-Classification-with-Probabilistic-Categorial-Grammars-》" class="headerlink" title="3.《Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars 》"></a>3.《Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars 》</h3><p>MIT在2005年semantic parsing方面的工作，把自然语言句子映射到Logic Form上。</p>
<p>1.介绍了一下λ表达式；</p>
<p>2.介绍了本文中使用的logic form，combinatory categorial grammars组合范畴语法。</p>
<p>CCG包含一个词典lexicon，其中有一些词类型是基本类型NP，另一些是复杂类型。</p>
<p>CCG还包含一个combinatory rules，所以相邻的字符串能够递归的组合起来。比如函数组合规则A/B B -&gt; A。</p>
<p>CCG的优势就是语法类型和语义类型一致性保持的较好。比如lexicon中每个词在规定语法类型的同时，可以规定语义类型（λ表达式）</p>
<p>3.介绍如何把CCG扩展到Probabilistic CCG，也就是求P（L，T|S）其中L是最后的逻辑表达式，T是每一步推导的结果而S是原文。PCCG主要解决CCG中的歧义问题，该问题可能产生于语法解析过程，也可能产生于语义解析过程。（前者对应同一个词不同的语法类型或语法组合类型，后者对应不同语法对应到了同一语义）</p>
<p>对于一个S，推导除了多个（L，T）的时候，这篇文章首先学习出一个f，把(S,L,T)三元组映射到d个特征向量（对应S和T中不同子结构的数量），而后使用一个函数（如下）作为不同(L,T)的最终概率。</p>
<p><img src="https://i.loli.net/2018/06/26/5b31f7634f03a.jpg" alt=""></p>
<p>最终使用的feature是每个lexical entry在T中使用的次数。</p>
<h3 id="4-《SQLNet》-amp-《Seq2SQL》"><a href="#4-《SQLNet》-amp-《Seq2SQL》" class="headerlink" title="4.《SQLNet》&amp;《Seq2SQL》"></a>4.《SQLNet》&amp;《Seq2SQL》</h3><p>与semantic parsing相关的论文。17年11月。ICLR2018。以及17年6月。</p>
<p>Seq2SQL利用RL解决生成SQL的where子句中order导致的交叉熵错误问题，在wikiSQL上显著的提高了效果。</p>
<p>SQLNet利用column attention来预测每个column name是否出现在column slot中。</p>
<h3 id="5-《The-Web-as-a-Knowledge-base-for-Answering-Complex-Questions》"><a href="#5-《The-Web-as-a-Knowledge-base-for-Answering-Complex-Questions》" class="headerlink" title="5.《The Web as a Knowledge-base for Answering Complex Questions》"></a>5.《The Web as a Knowledge-base for Answering Complex Questions》</h3><p>特拉维夫Tel-Aviv大学的Talmor和Berant在2018年的工作。</p>
<p>关于Semantic Parsing和QA的问题，针对复杂问题，提出了ComplexWebQuestion数据集，以及一个PointerNetwork为主的框架解决QueryDecomposition。</p>
<p>相关工作，有一个研究Query-Paraphrase的工作，还有一个SQA研究decomp的工作（那个工作使用了人工标注的split-query，需要更多的监督信息，而且针对table这样结构化数据而不是web数据）</p>
<p>首先，利用PointerNetwork把复杂问题分解，然后把分解后的简单问题送入搜索引擎，最后利用RC模型抽取答案。</p>
<p>几个关键步骤：</p>
<ol>
<li>数据集的产生，由simpleQuestion的logicForm进行扩展（四种方式），并生成MG-Complex-Query，并采用众包的方式产生HG-Complex-Query。</li>
<li>数据集问题分类：Comp（two hop），conj（two constraint），comparable，superlative。</li>
<li>监督信号的来源，noisy alignment，通过MG-query-split-point，通过对齐的方式找到最可能的HG-split-point。</li>
<li>noisy supervision产生的具体过程：<ol>
<li>给原数据集每个sample增加annotation；</li>
<li>计算MG和NL的输出的word相似度矩阵（利用glove-300d-vector）。</li>
<li>计算MG的query的split-point，容易求得。</li>
<li>根据2&amp;3的结果，得到带有噪音的NL的split-points作为监督信号。</li>
</ol>
</li>
<li>训练中computation tree（也就是输入和输出）的表示：<ol>
<li>comp（two hop），i&amp;j，i和j之间的是第一跳的query，得到的答案填充到<i和>j的部分作为第二跳的query。</li>
<li>conj（two constrain），i决定在哪里断开，j决定是否把某个词复制到断开的后面一段的问题中（不复制即为-1）。</li>
</ol>
</li>
<li>评价准则，Precision@1（27.5），距离人类的标准还有差距（63）。</li>
<li>模型训练时无法处理的情况（comparable，superlative，negative），这些不太好通过搜索引擎获得（如何获得所有entity的结果？），因此在数据集中这类问题（10%）直接作为simpleQ处理，以后如果基于KB或者Table的QA进行研究的话，会增加这类算子。</li>
<li>AugmentedPointerNetwork模型，利用query+comp+conj作为decode的词库。</li>
<li>决定是否进行decomp，利用RC模型给出的答案的score，查看simp还是decomp的score高，进而决定是否进行decomp。</li>
<li>使用了两个RC模型，web-based QA+RASOR以及DocQA，来验证decomp的有效性。</li>
<li>但是RC和Web-based QA这两个模型的代码并没有放进来。</li>
</ol>
<h3 id="6-《Evaluating-Semantic-Parsing-against-a-Simple-Web-based-Question-Answering-Model》"><a href="#6-《Evaluating-Semantic-Parsing-against-a-Simple-Web-based-Question-Answering-Model》" class="headerlink" title="6.《Evaluating Semantic Parsing against a Simple Web-based Question Answering Model》"></a>6.《Evaluating Semantic Parsing against a Simple Web-based Question Answering Model》</h3><p>特拉维夫Tel-Aviv大学的Talmor在2017年的工作。上一篇论文中web-based qa framework就是这个工作。</p>
<p>首先，利用query在www上获取前100篇snippets。而后利用tf-idf，获取1-4gram的前若干个candidate，之后利用log-linear模型和一些特征从这些candidate中给出top-k，k由一个公式获得。在ComplexQuestions数据集（junwei）上获得了不错的效果。</p>
<h3 id="7-《Entity-Duet-Neural-Ranking-Understanding-the-Role-of-Knowledge-Graph-Semantics-in-Neural-Information-Retrieval》-ACL2018"><a href="#7-《Entity-Duet-Neural-Ranking-Understanding-the-Role-of-Knowledge-Graph-Semantics-in-Neural-Information-Retrieval》-ACL2018" class="headerlink" title="7.《Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval》-ACL2018"></a>7.《Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval》-ACL2018</h3><p>1.问题</p>
<p>搜索引擎排序。Ranking task in search systems。</p>
<p>在Neural IR系统中，entity和semantic起到的作用。</p>
<p>2.已有方法（相关工作）</p>
<p>两个方向：</p>
<p>1.entity-oriented search，从知识图谱中抽取实体和语义信息，增强基于特征匹配的搜索引擎排序。能够利用实体和知识图谱中的语义信息。</p>
<pre><code>a.利用KB中的attributes，作为ranking feature，Dietz等人2014.

b.通过相关entity，构建query和documents之间的额外关联。
</code></pre><p>2.Neural Ranking Models：能够利用神经网络，在大量数据情况下学到更加有效和鲁棒的ranking model。</p>
<pre><code>a.学习分布式表示，然后找到相关的关系（基于表示的）

b.直接通过word-level的信息交互，对q和d的相关度进行建模（基于交互的），这种方法提高了准确率（尤其是大规模数据的）
</code></pre><p>详细的相关工作：</p>
<p>Huang等人的DSSM：把letter-tri-grams给哈希到低维向量。输入是三个字母为窗口的vector（50w的词库-&gt;3w维的向量），通过hash压缩向量空间并增加泛化能力。而后通过多层MLP得到语义向量，然后把q和d的语义向量cos得到后验概率，最大化被点击的d的后验概率。使用词袋模型，丢失了语义和词序信息。直接学习到文档的语义表示。</p>
<p><img src="https://i.loli.net/2018/06/07/5b18e3db4d9a0.jpg" alt=""></p>
<p>Xiong等人的K-NRM：模型的translation layer计算q和d的双向cos-similarity（这里叫做translation matrix）。然后，在kernel pooling层，利用RBF核把词与词之间的相似度转化为q-d的相似度特征向量。K个核，每个核计算得到的是query中每个词和d中每个词的相似度。用RBF的话，是一种动态的pooling，计算相似度离<script type="math/tex">\mu</script>最接近的词汇数量。最后通过一个tanh的仿射变换，得到ranking score。</p>
<p><img src="https://i.loli.net/2018/06/07/5b18eb092c42a.jpg" alt=""></p>
<p>Dai&amp;Xiong等人的Conv-KNRM：conv层利用CNN抽取N-gram的embedding feature，而后在cross-match层对n进行match得到若干个match matrix，每个矩阵得到一个pooling feature，而后这些feature矩阵通过concat得到最终的feature，最后用pair-wise-LeToR损失进行优化。</p>
<p>此外，还有一些工作（3-4个）研究KB辅助Neural-based-model进行IR任务，和本文工作相似。</p>
<p>3.论文提出的方法</p>
<p><img src="https://i.loli.net/2018/06/07/5b18f05817aa8.jpg" alt=""></p>
<p>EDRM</p>
<p>利用知识图谱的语义信息（实体的分布式表达），增强neural based rank model的性能。</p>
<p>首先学习entity的分布式语义表示：</p>
<pre><code>entity embedding：就是直接通过embedding得到。

description embedding，通过描述字段的embedding矩阵进行卷积和max-pooling得到的。

type embedding，每个entity有n个type的话，首先获得每个type的embedding，而后通过和bag-of-words进行注意力计算并加权求和，得到type embedding。

最后，把三种信息concat起来得到enriched entity embedding。
</code></pre><p>而后通过Xiong等人提出的一个entity-oriented-search framework（但是xiong等人提出的是依靠人工设计的feature），通过bag-of-words和bag-of-entities来计算d和q的匹配度，一共计算出四种方向的矩阵。融入n-gram信息（n=1,2,3），把每个n-gram汇聚在一起。</p>
<p>其中的pooling也是和之前的工作一样的。之所以叫soft-TF(term frequency)，是因为当<script type="math/tex">\mu=1,\sigma\rightarrow0</script>的时候，就等于统计EM的词数量。</p>
<p>把entity的信息融入，同时使用基于神经网络而不是特征匹配的排序系统。增加了系统的泛化能力。</p>
<p>4.实验及结果</p>
<p>数据集：一个商业的搜索log。Sogou和CN-DBPedia的query-log。</p>
<p>进行了ablation analysis，case study，以及性能分析。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-RC-ACL2016-《Text Understanding with the Attention Sum Reader Network》</title>
    <url>/2017/03/15/deeplearning/NLP-RC-ACL2016-%E3%80%8AText-Understanding-with-the-Attention-Sum-Reader-Network%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>IBM-Watson实验室的工作，文章发表于ACL2016，针对机器阅读理解任务提出了一个模型。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>使用类似完形填空（Cloze）这种形式的问答任务来评估模型的阅读理解能力。如下图所示，是完形填空任务的一个问答对。</p>
<p>完形填空任务作为评估目标具有以下优点：</p>
<ul>
<li>容易评估对文本的理解程度，直接利用准确率即可评价。</li>
<li>容易改变任务的难度和侧重点，比如：改变空白词词性或者改变问题的挑选方式。</li>
<li>大规模语料库容易搜集。</li>
</ul>
<p><img src="/images/FgWJk2zt_WP7x4r8ttBElnLvcOVs.jpg" alt="1"></p>
<p>一个问答对可以被形式化的表示为$(q,d,a,A)$的形式，其中A是备选答案、a是正确答案、q是问题、d是文档，其中$ a\in d,d\subseteq V$，也就是正确答案在d中。</p>
<p>已有的方法：</p>
<ol>
<li>LSTM：在动词和介词的预测上能够达到人类水平，但在名词和命名实体填充方面效果不佳。</li>
</ol>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=3e388f18106f704527bde4797502b04a" alt=""></p>
<ol>
<li>Attentive Reader [利用注意力机制，利用当前词和q编码向量u的相似性得到注意力权重，并编码文档为r]。</li>
</ol>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=517d615c239b1a5b7c1240e5b71da0bd" alt=""></p>
<ol>
<li>Impatient Reader [在AR的基础上进行了细化] state-of-the-art in DailyNews。</li>
</ol>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=a2c7e98e507316775b8e96fed39a8dbd" alt=""></p>
<p>  上面两个模型来源于(2015-NIPS) teaching machines to read andcomprehend.其不足是对于多个正确答案的情况，可能会导致选择错误答案。</p>
<ol>
<li><p>MemNN [Memory] state-of-the-art in CBT。</p>
<p>来源于  (2016-ICLR)The goldilocks principle: Reading children’s books with explicit memoryrepresentations. </p>
</li>
<li><p>Dynamic Entity Repres [利用注意力机制的复杂模型，针对命名实体优化]state-of-the-art in CNN。</p>
</li>
</ol>
<p>来源于  (2016-NAACL-HLT)  Dynamic Entity Representation withMax-pooling Improves Machine Reading. </p>
<ol>
<li>Chen et al. [简化版本的Attentive Reader] 同期工作。</li>
</ol>
<p>来源于  (2016-ACL) A Thorough Examination of the CNN /Daily Mail Reading Comprehension Task. </p>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>Attention Sum Reader(AS-Reader)，能够在$a\in d$即【正确答案是文档中的词】这个前提下达到state-of-the-art。</p>
<p>（1） 计算问题q的向量；</p>
<p>（2） 计算上下文向量，即一个单词在d的上下文环境下的向量；</p>
<p>（3） 将q的向量和每一个候选词在d上下文环境下的向量点乘，找到最可能的候选词。</p>
<p><img src="/images/FkVx76vox_8d7N4PIyl0zFNG56-b.jpg" alt="2"></p>
<p>使用e(w)表示单词w的embedding。</p>
<p>使用f和g两个encoder，分别将文档里第i位的单词的embedding即$f_i(d)$和问题的embedding即$g(q)$编码到固定长度的向量。</p>
<p>所以，将$exp(f_i(d)\cdot g(q))$看做是文章中第i个单词就是q的答案的概率，注意这个概率要归一化，归一化之后的概率称为$s_i$。这个概率也可以被看做是注意力。</p>
<p>什么是Pointer sum attention呢，就是说对每一个单词w，将其在d中出现的每一次算出的概率$s_i$加起来得到$P(w|q,d)$。这种使用方式受2015年的Pointer Networks启发。</p>
<p>实际的模型实现中，编码器f由一个双向的GRU实现，双向的第i个隐层状态拼接起来就是上下文向量。</p>
<p>g由另一个双向GRU实现。这次就是正向的最后一个隐层状态和反向的第一个隐层状态拼接起来是q-embedding。</p>
<p>而e就是一个lookup-table，里面每一行存一个单词的embedding。</p>
<p>训练时，同时优化f,g,e。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>在CNN、Daily Mail和CBT语料库上进行了测试。</p>
<p>虽然该模型简单，但是三个数据集上都是state-of-the-art。</p>
<p>用Adam+SGD优化，负对数似然作为损失函数。</p>
<p>e中的参数用[-0.1,0.1]的均匀分布随机初始化。f和g的参数用随机正交矩阵初始化。</p>
<p>batch_size=32，梯度裁剪阈值=10。</p>
<p>每个epoch对样本打乱，而且每个mini_batch的样本长度差不多。</p>
<p>没有使用pre-train的词向量，命名实体被随机变换。</p>
<p>没有任何文本预处理，没有正则化项。</p>
<p>使用准确率进行评估。</p>
<p>试验中采用了三种ensemble的方式：</p>
<p>（1）average ensemble by top 20%</p>
<p>更改初始化参数，训练多个模型，然后取在验证集上效果最好的前20%个模型做bagging.</p>
<p>（2）average ensemble</p>
<p>取前效果排名前70%的model做bagging</p>
<p>（3）greedy ensemble</p>
<p>根据效果排序从效果最好的模型开始bagging，如果bagging后的模型在验证集上效果更好就加入，一直持续到最后。</p>
<p><strong>实验分析</strong>：</p>
<p>在CNN/Daily Mail数据集上，随着document的长度增加，测试的准确率会下降，而在CBT数据集上得到了相反的结论。从中可以看得出，两个数据集有着不同的特征。在论文中也对产生这种现象的原因进行了分析。</p>
<p>chen2016分析了CNN、DailyNews语料库，并对其抽样进行了人工测试，认为模型能够达到的最高精确度应该就是本文模型所达到的水平了（因为语料库的噪音、错误答案等）。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="">PaperWeekly的第二期GitChat</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/23462480#!" target="_blank" rel="noopener">知乎专栏，相关工作看这里介绍</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>问答系统</tag>
      </tags>
  </entry>
  <entry>
    <title>SQuAD阅读理解第一名-《R-net Machine Reading Comprehension With self-matching Networks》</title>
    <url>/2017/05/18/deeplearning/NLP-RC-ACL2017-%E3%80%8AR-net%20Machine%20Reading%20Comprehension%20With%20self-matching%20Networks%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>文章由MSRA发表于ACL2017，并在SQuAD和MS-MARCO阅读理解任务上都取得了SOTA效果。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>问题：如何使阅读理解的准确率更高，更好的理解上下文。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>基本思路是：</p>
<ul>
<li>1.注意力机制，根据与问题的相关度，集中权重在文章的某些部分。这是通过gating mechanism实现的。</li>
<li>2.self-matching mechanism，可以看做一个二次推理的过程，后文详细讲。</li>
</ul>
<p>模型的structure如下图：</p>
<p><img src="/images/Fhtyn80-JLdW1KKkJa8qXUshS6GA.jpg" alt=""></p>
<p>其中，首先Q和P分别使用Word Embeddding和Char Embedding（利用CharLSTM）作为输入，输入biGRU中产生context aware word representation。这一步就是Q&amp;P Encoding。</p>
<p>而后，使用gated attention-based RNN，获得question aware word representation，即<script type="math/tex">v^P_t</script>。这个gated-attention RNN，是在RNN和attention基础上增加一个gate，即<script type="math/tex">g_t</script>。这一步被称为Q&amp;P Matching。</p>
<script type="math/tex; mode=display">
v^P_t=RNN(v_{t-1}^P,[u^P,c_t])</script><script type="math/tex; mode=display">
g_t = sigmoid(w_g[u^P_t,c_t])</script><script type="math/tex; mode=display">
[u^P_t,c_t]^*=g_t\odot [u^P_t,c_t]</script><p>然后这里得到的表示对长时依赖（long-term dependency）通常无法捕获，这又是我们所需要的，所以还需要对表示进行改善（refine representation）。</p>
<p>这里又使用了另一个gated-attention RNN，把文章的表示本身和文章的表示一起计算出注意力向量，这里相当于以我现在针对问题对文章的理解，再看一遍文章，更新我的理解。这个更新后的理解就是<script type="math/tex">h^P</script>。这个步骤被称为self-matching。</p>
<p>最终，连接一个预测层预测答案，论文中的预测层针对SQuAD，预测其实位置<script type="math/tex">p^0</script>和结束位置<script type="math/tex">p^1</script>。</p>
<p><img src="/images/FiVbgeT0PaCViIAced8hJaSoALLz.jpg" alt=""></p>
<p><img src="/images/FgReUSYO7wZM060G2b_JyI5ju_aJ.jpg" alt=""></p>
<p><img src="/images/FnEL8-vlQcgfBBQPKgJArr0Y6bz8.jpg" alt=""></p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>SQuAD：斯坦福2016年提出的阅读理解数据集，wiki百科中产生的问题和答案，答案是原文中的一段连续短语 。</p>
<p>MS-MARCO：微软2016年提出的阅读理解数据集，Bing搜索的问题和几个相关的文档，答案由人工产生，不一定在原文里面。</p>
<p><img src="/images/FiSfhKy8-Zm-ZI36ZnAF9OANXuMz.jpg" alt=""></p>
<h4 id="5、小结"><a href="#5、小结" class="headerlink" title="5、小结"></a>5、小结</h4><p>R-net利用了单词和字符级别的编码，利用了两次gated-attention RNN来学习出一个表示，可以理解为：带着对问题的理解去读文章，获得了一个对文章的初步理解（question aware representation），这个初步理解利用了question2passage-attention；然后带着这个初步理解再次阅读文章本身，利用passage2passage-attention更新我对于文章每个部分的理解。最后利用这个学习到的表示通过Pointer-Network-like的网络预测（SQuAD）。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>paperWeekly</tag>
        <tag>机器阅读理解</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>使用助记符帮助阅读-《Mnemonic Reader for Machine Comprehension》</title>
    <url>/2017/05/23/deeplearning/NLP-RC-ArXiv2017-M-reader-%E3%80%8AMnemonic%20Reader%20for%20Machine%20Comprehension%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>文章最早于2017年5月发表于ArXiv，是学长参加SQuAD的模型，集成模型在2017年5月获得了第三名。文章地址：<a href="http://arxiv.org/abs/1705.02798" target="_blank" rel="noopener">http://arxiv.org/abs/1705.02798</a></p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>要解决的问题就是SQuAD这种给定上下文d和问题q，预测答案（原文中一段连续短语）的阅读理解问题。</p>
<p>文中根据如何计算注意力（单向或双向），进行推理的次数这两个方面对已有模型进行了分析。</p>
<p>已有的模型利用GRU或者LSTM加上注意力机制编码包含问题语义的上下文信息，但由于这些网络本身的限制，长距离依赖的语义常常无法被编码；</p>
<p>已有的基于Pointer Network的模型在预测答案的范围（span）的时候，比如Answer Pointer和Ruminating Reader没有将pointer向量的计算与问题直接联系起来，而且多使用single-hop（single-turn）的计算方式。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><ul>
<li>一、引申阅读：Pointer Network</li>
</ul>
<p>Ptr-Net这个模型能够解决Sequence2Sequence模型中输出可变且与输入长度相关的序列这个问题。使用的方式就是把经典的Sequence2Sequence模型<em>(Bahdanau et al. 2014)</em>中的注意力权重直接用来计算条件概率。</p>
<p><img src="/images/Fl3vn57MeFYYz5nNWHkX0VT5rYv-.jpg" alt=""></p>
<p><img src="/images/FlHlrLnMvehbaAb5b7llLqF4USaS.jpg" alt=""></p>
<p>这个模型的名字叫做Mnemonic Reader，其中Mnemonic是助记符的意思。</p>
<ul>
<li><p>Input Layer：输入问题和上下文，经过embedding（word和char）以及highway network，输出context-aware-representations，同时增加一个feature，就是对于问题中的（非常用）词是否在答案中出现，以及反向。</p>
</li>
<li><p>Interactive Alignment Layer：通过自己定制的注意力计算，获取了question-aware-context-representation。</p>
</li>
<li><p>Self Alignment Layer：通过context2context的注意力计算，获取了进一步的表示self-aware-context-representation，这个表示可以认为是获取了原文中的长距离依赖。</p>
</li>
<li><p>Mnemonic Pointing Layer：query-memory-vector:<script type="math/tex">m^s=\tilde q</script>通常等于问题表示的最终时刻隐状态，query-category-vector:<script type="math/tex">v</script>编码问题的类型。利用下面第一个公式计算出预测的开始位置<script type="math/tex">P^s</script>，并利用这个开始位置用下面第二个公式计算出结束位置<script type="math/tex">P^e</script>。这样最终预测的答案就是原文中<script type="math/tex">[P^s,P^e]</script>的单词序列。其中memory模块就是一个向量<script type="math/tex">m</script>，其初值是<script type="math/tex">m^s</script>，使用<script type="math/tex">m_{new}=G(m_{old},u)</script>的更新函数更新，其中u是evidence-vector，映射G的计算如下面第三张图片中的公式：</p>
</li>
<li><p>此外，文中还提到了将answer-span的预测扩展到multi-hop的方式。</p>
<p><img src="/images/FqbtucBzzNUbgLl_aMofDon0HUA-.jpg" alt=""></p>
<p><img src="/images/Ftl77RNCE3tLYvRUPjupwOTpI--S.jpg" alt=""></p>
<p><img src="/images/Fk_Fx4fc_ipu_JZwCL01MJ_XRenV.jpg" alt=""></p>
</li>
</ul>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>在SQuAD数据集上获得了接近SOTA的结果，并进行了Abalation-Analysis，结果如下：</p>
<p><img src="/images/FlhaTY7y33q4ckxI8rkCQ-cBYnZd.jpg" alt=""></p>
<p>作者进行了几点分析：</p>
<ul>
<li>统计出SQuAD数据集中答案90%都是OOV，所以字符嵌入表示很重要。</li>
<li>interactive alignment层对性能影响最大，因为编码问题的语义信息是正确回答问题的关键。</li>
<li>增加了一个EM的特征对性能提高也很关键，这一点在<em>(Weissenborn et al. 2017)</em>中也提到了。</li>
</ul>
<h4 id="5、方法总结"><a href="#5、方法总结" class="headerlink" title="5、方法总结"></a>5、方法总结</h4><p>M-Reader通过soft-alignment的注意力机制和query-aware+self-aware的上下文表示解决长时依赖问题（long-distance dependency）；并使用基于记忆网络的query-dependent pointer来预测答案以利用问题的显示和隐含信息。在SQuAD数据集上获得了很好的效果。</p>
<p>最后作者提到了，对长度越长的答案，现有的模型（M-Reader和BiDAF）预测性能就会越差，这也是一个可以改进的点。另外，将common knowledge融合进模型也是另一个很重要的思路。</p>
<p>PS：这篇文章和同一时间发表的R-net结构异曲同工，大神们的思路果然类似。</p>
<h4 id="6、参考文献"><a href="#6、参考文献" class="headerlink" title="6、参考文献"></a>6、参考文献</h4><ul>
<li>Yichen Gong and Samuel R. Bowman. 2017. <strong>Ruminating reader</strong>: Reasoning with gated multi-hop attention. arXiv preprint arXiv:1704.07415 . </li>
<li>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. <strong>Pointer networks</strong>. In Proceedings of NIPS. </li>
<li>Shuohang Wang and Jing Jiang. 2017. Machine comprehension using <strong>match-lstm and answer pointer</strong>. In Proceedings of ICLR. </li>
<li>Dirk Weissenborn, Georg Wiese, and Laura Seiffe.<strong>Fastqa</strong>: A simple and efficient neural architecture for question answering. arXiv preprint arXiv:1703.04816 . </li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>机器阅读理解</tag>
        <tag>attention</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>《Teaching Machines to Read and Comprehend》</title>
    <url>/2017/03/07/deeplearning/NLP-RC-NIPS2015-%E3%80%8ATeaching-Machines-to-Read-and-Comprehend%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>本文发表在NIPS-2015。是<a href="https://github.com/terryum/awesome-deep-learning-papers" target="_blank" rel="noopener">awesome-deep-learning-papers</a>中自然语言处理的论文。作者是Google-Deepmind团队成员。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>对于使用机器学习方式的问答系统，现在缺少大规模的标注数据集用来训练和测试。</p>
<p>机器阅读理解，就是在一篇文章最后提出一个问题，要求给出答案，也就是估计条件概率$P(a|c,q)$，其中$a$是答案，$c$是上下文，$q$是上下文文档。通常，为了准确的估计模型对上下文和问题的理解，模型不能带有先验知识。</p>
<p>而阅读理解的带标注语料库，就是很多$(a,c,q)$三元组。</p>
<p>本文提出了一种生成大规模语料库的方法，并通过实验证明了其有效性。</p>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>从CNN和Daily Mail的网站爬取了很多文章，并将要点中的<strong>实体</strong>用占位符代替从而生成Query。</p>
<p>对于每篇文章，利用抽象式的方法提取Summary，并随机去掉一个实体作为Query。</p>
<p>为了提供一个能够评估单篇文章理解能力的语料库，采用了匿名化和随机化实体（Entity）的策略：</p>
<ul>
<li>1.使用共引用系统（Co-reference System）分析并将文章中的实体匿名化；</li>
<li>2.每次载入$(a,c,q)$三元组时都会其中的实体进行随机变换。</li>
</ul>
<p>语料库随机化和匿名化的结果如下图所示。</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=5033a8197c8ed579be214a0d121c706c" alt="1"></p>
<p>生成语料库的代码在<a href="http://www.github.com/deepmind/rc-data/" target="_blank" rel="noopener">Github</a>上。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>实验设计：</p>
<ul>
<li>1.选择两个Baseline模型，即“c中出现最多的实体”和“c中出现最多且q中无出现的实体”；</li>
<li>2.符号匹配模型，框架语义解析，学习到一些启发式的解析规则（如下表），并定义一个后退的策略；</li>
</ul>
<p><img src="/images/FqJE27UaAX9XfXSaDSHmiapbR999.jpg" alt="2"></p>
<ul>
<li>3.神经网络模型；<ul>
<li>LSTM</li>
<li>Attention-Based-LSTM-Model</li>
<li>Impatient-LSTM</li>
</ul>
</li>
</ul>
<p><img src="https://ooo.0o0.ooo/2017/03/07/58be5c732e1eb.jpg" alt="3"></p>
<p>实验的结论：</p>
<ul>
<li>1.Attention大法好；</li>
<li>2.数据集有用，能够训练出不错的模型。</li>
</ul>
<p><img src="https://ooo.0o0.ooo/2017/03/07/58be660d1c584.jpg" alt="4"></p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://zhuanlan.zhihu.com/p/21343662" target="_blank" rel="noopener">PaperWeekly-知乎</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>机器阅读理解</tag>
      </tags>
  </entry>
  <entry>
    <title>学习如何停止阅读-《ReasoNet Learning to Stop Reading in Machine Comprehension》</title>
    <url>/2017/05/22/deeplearning/NLP-RC-ReasoNet-NIPS2016-%E3%80%8AReasoNet%20Learning%20to%20Stop%20Reading%20in%20Machine%20Comprehension%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>文章由微软研究院发表在NIPS2016，提出了一种面向Cloze-style-QA任务的模型。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>面向的任务：CNN和DailyMail这种完形填空类型的问答数据集。</p>
<p>已有的方法：本文将已有的方法分为两类，即：</p>
<ul>
<li>single turn reasoning单轮推理：诸如MemN2N（Window-based），AS-Reader，Stanford-AR，Epi-Reader这种直接利用注意力机制，将关注点集中在上下文的某一部分，而后利用注意力权重和上下文表示获得中间的推理结果表示（编码了问题语义信息的上下文表示），用这个中间推理结果预测答案。</li>
<li>multi turn reasoning多轮推理：诸如MemN2N（Multi-hop），GA-Reader，IA-Reader，AoA-Reader等，相对于single-turn更加复杂精细，对需要更多逻辑推理的问题效果更好。其基本原理是在每一轮利用上一轮推理从上下文中获取的新信息和问题q的表示，得到新的推理信息，在若干轮推理后预测出答案的模型。</li>
</ul>
<p>上面两类模型，第一种在复杂问题上明显效果不如第二种；而第二类模型，已有的方法都采用了固定推理的轮数，而人在面临阅读理解问题的时候，会根据题目和上下文的难度动态的决定推理的次数，这篇论文就是从这个角度来改进已有的multi-turn-reason-models。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>本文提出的模型ReasoNet模拟人类的推理过程，带着问题反复阅读原文，并动态的决定推理的轮数，根据当前获得的信息（是否足够获得一个置信度足够高的答案）决定是否停止推理过程。</p>
<p>而这个终止判断是通过引入一个推理的终止状态的，由于引入了这样一个离散的与output不相关的量，所以经典的BP无法用来优化，文中引入了一个强化学习算法CR（对比奖励）来训练模型。</p>
<p>下图是模型框架，分为下面几部分：</p>
<p><img src="/images/FvanAOA-1VJF9_PB7YCQ8CTDt4_Q.jpg" alt=""></p>
<ul>
<li>Internal State：内部状态S，初始状态<script type="math/tex">s_1</script>是Query Encoder产生的Query-Representation，t时刻状态由RNN产生<script type="math/tex">s_t=RNN(s_{t-1},x_t;\theta_s)</script>；</li>
<li>Memory：外部记忆，<script type="math/tex">M={m_i}_{i=1..|D|}</script>，在这个任务中就是Document Encoder产生的每个单词的Context-Aware-Representation；</li>
<li>Attention：注意力向量（也叫上下文向量）<script type="math/tex">x_t</script>是由当前状态和外部记忆进行计算得出的：<script type="math/tex">x_t=f_{att}(s_t,M;\theta_x)</script>；</li>
<li>Termination Gate：根据当前内部状态产生一个binary随机变量<script type="math/tex">T_t\sim P(\cdot|f_t(s_t;\theta_t))</script>，如果为1，那么结束推理预测答案；如果为0，那么继续推理；</li>
<li>Answer：如果TG=1，那么预测答案<script type="math/tex">a_t\sim P(\cdot|f_a(s_t;\theta_a))</script>.</li>
</ul>
<p>ReasoNet的计算过程（随机推理过程）如下所示：</p>
<p><img src="/images/Flj6oUHhFuxijJ6m3Ou3LYNV0G6k.jpg" alt=""></p>
<p>网络的参数包含四个部分（其实应该是六个部分，还有Query Encoder和Document Encoder的参数），网络的期望奖励是：</p>
<p><img src="/images/FuKS79E9b3DhDicIWK2ppnuF2jhU.jpg" alt=""></p>
<p>其中如果答案正确那么最终时刻的reward即<script type="math/tex">r_T=1</script>，否则为0，中间时刻的reward都为0。至于上面的Reward对<script type="math/tex">\theta</script>的梯度，就需要利用RL的算法进行计算了。这部分现在对RL不了解，暂时跳过。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>在CNN/Daily-mail以及Graph-Reachability任务中都达到了SOTA水平。</p>
<p><img src="/images/FhifmAAJnJoTQ9rmtL1j8FBJ95n3.jpg" alt=""></p>
<p>第二个任务，是由于相关文献表明CNN/Dailymail中问题可能过于简单，无法检测multi-turn模型的推理能力，因此文中抽取了图数据库中有向图，让模型通过推理预测节点之间的连通性，这个任务可以引申到知识图谱中的图推理任务中。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>机器阅读理解</tag>
        <tag>完形填空</tag>
        <tag>attention</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>ICLR2017-Machine Comprehension三篇论文选读</title>
    <url>/2017/05/08/deeplearning/NLP-RC-ICLR2017-papers/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>本次阅读了下面三篇发表在ICLR2017上的机器阅读理解文章。</p>
<p><a href="https://arxiv.org/abs/1611.01603" target="_blank" rel="noopener">《Bi-Directional Attention Flow For Machine Comprehension》</a></p>
<p><a href="https://arxiv.org/abs/1702.04521" target="_blank" rel="noopener">《Frustratingly Short Attention Spans In Neural Language Modeling》</a></p>
<p><a href="https://arxiv.org/abs/1610.06454" target="_blank" rel="noopener">《Reasoning With Memory Augmented Neural Networks For Language Comprehension》</a></p>
<h4 id="2、Bi-Directional-Attention-Flow-For-Machine-Comprehension"><a href="#2、Bi-Directional-Attention-Flow-For-Machine-Comprehension" class="headerlink" title="2、Bi-Directional Attention Flow For Machine Comprehension"></a>2、Bi-Directional Attention Flow For Machine Comprehension</h4><p>华盛顿大学的研究人员提出的一种基于Attention的阅读理解框架BiDAF。代码在这里<a href="https://github.com/allenai/bi-att-flow" target="_blank" rel="noopener">https://github.com/allenai/bi-att-flow</a>。</p>
<ul>
<li>输入，假定输入上下文<script type="math/tex">X=\{x_1,\cdots,x_T\}</script>，问题<script type="math/tex">Q=\{q_1,\cdots,q_J\}</script>。</li>
<li>字符嵌入层：利用CharCNN将每个单词映射到向量空间中。</li>
<li>单词嵌入层：利用预训练的Glove词向量模型将单词映射到向量空间中。</li>
<li>上下文嵌入层：输入上两层的输出，利用上下文去调整并输出每个单词包含上下文信息的向量表示。</li>
<li>注意力层：输入问题q的向量表示<script type="math/tex">U\in R^{2d\times J}</script>和上下文的向量表示<script type="math/tex">H\in R^{2d\times T}</script>，利用注意力机制，输出D中每个单词基于问题的特征向量G，以及上下文嵌入层的输出。首先利用H和U计算出相似度矩阵<script type="math/tex">S\in R^{T\times J}</script>，其中</li>
</ul>
<script type="math/tex; mode=display">
S_{tj}=\alpha(H_{:t},U_{:j})\in R</script><p>进一步的，利用S计算出上下文对于问题的注意力C2Q，以及问题对于上下文的注意力Q2C，C2Q代表对于每个上下文单词每一个问题单词的相关程度，Q2C则反之。</p>
<p>利用S每一行进行softmax，得到C2Q的权重<script type="math/tex">a_t\in R^J</script>，而用这个权重可以得到<script type="math/tex">\tilde U\in R^{2d\times T}</script>，其中<script type="math/tex">\tilde u_{:t}=\sum_ja_{tj}U_{:j}</script>，也就是对应上下文每个单词时，q的表示。</p>
<p>Q2C并不是和C2Q完全对应着来，而是<script type="math/tex">b=softmax(max_{col}(s))\in R^T</script>，也就是取了一个解释起来略复杂的softmax来计算注意力权重。而后计算出对于问题的表示<script type="math/tex">\tilde H\in R^{2d\times T}</script>，其中每一列<script type="math/tex">\tilde h = \sum_tb_th_{:t}</script>。</p>
<p>最后，利用上面计算出来的这些结果计算输出<script type="math/tex">G\in R^{x\times T}</script>，<script type="math/tex">G_{:t}=\beta(H_{:t},\tilde U_{:t},\tilde H_{:t})</script>。这个G也就是基于问题的上下文特征向量。</p>
<ul>
<li>模型层：利用LSTM对注意力层的输出进行建模。输出<script type="math/tex">M \in R^{2d\times T}</script>。</li>
<li>输出层：利用softmax和FC层输出结果。对于QA任务（默认答案是原文中连续的一段），预测其开始和结束的index。</li>
</ul>
<p><img src="/images/FklvkRV6mgKtmXqhCIVmtIrLSCRN.jpg" alt=""></p>
<p>这篇文章提出的模型的创新点在于：</p>
<ul>
<li>利用了不同级别的表示：字符级别、词级别以及上下文级别。</li>
<li>用一种新的方式将注意力机制加入模型中。<ul>
<li>并没有过早的利用attention层将上下文总结为一个定长向量，而是记录下了每一个时间步的attention，并流入下一层。（类似AS-Reader和AOA）</li>
<li>attention的计算，仅仅与当前时间步的上下文和问题相关，与之前时间步的状态无关。</li>
<li>计算了双向的attention，即问题对上下文的注意力和上下文对于问题的注意力，利用了更多的信息。（类似AOA）</li>
</ul>
</li>
</ul>
<p>总的来说，这篇文章利用attention的思想和2016年稍早些发表的文章Attention-over-Attention有些类似，都利用了相似度矩阵以及双向的注意力。最终，这篇论文在斯坦福QA数据集（SQuAD，2016）和CNN/DailyMail完形填空任务中取得了state-of-the-art的成绩。</p>
<p><img src="/images/Frl_yGjGzNcPT8vupn-By273GhBQ.jpg" alt=""></p>
<h4 id="3、Frustratingly-Short-Attention-Spans-In-Neural-Language-Modeling"><a href="#3、Frustratingly-Short-Attention-Spans-In-Neural-Language-Modeling" class="headerlink" title="3、Frustratingly Short Attention Spans In Neural Language Modeling"></a>3、Frustratingly Short Attention Spans In Neural Language Modeling</h4><p>来自伦敦大学学院研究人员，有关Attention和Memory的一篇论文。在Memory Augmented模型中改进了Attention机制。</p>
<p>已有的attention机制被用来捕捉长时依赖，但是输出向量需要同时满足：</p>
<ul>
<li>对单词的分布进行encode，以预测下一个单词；</li>
<li>作为key来计算注意力权重向量；</li>
<li>encode上下文，用来帮助预测下一个单词。</li>
</ul>
<p>本文提出了一种假设即对输出表示的这种过度利用可能让网络难以训练，因此做了以下几个事情：</p>
<p>1）提出了几种改造的attention机制。</p>
<p>2）发现了尽管效果超过了已有的使用memory的网络，但是其实主要利用了过去五个time-step的特征表示；</p>
<p>3）找到了一种简单有效的网络结构，使用过去五个输出的表示拼接起来预测下一时刻的word。</p>
<ul>
<li>LM的标准Attention：引入Memory，但是把LSTM的过去L个time-step的输出<script type="math/tex">h\in R^k</script>作为Memory的内容<script type="math/tex">Y_t\in R^{k\times L}</script>。从公式中可以看到，无论是计算注意力权重向量<script type="math/tex">\alpha_t\in R^{L}</script>，还是计算上下文向量<script type="math/tex">r_t</script>，还是计算最终的表示<script type="math/tex">h_t^*</script>，都是用了LSTM的同一个输入<script type="math/tex">h_t</script>，这也让模型难以优化。</li>
<li>K-V Attention：把<script type="math/tex">h_t = \begin{bmatrix}k_t\\vt\\\end{bmatrix}</script>分为两个部分，其中<script type="math/tex">k_t</script>负责计算注意力权重，而<script type="math/tex">v_t</script>负责计算上下文向量，但是计算表示<script type="math/tex">h_t^*</script>还是用到了<script type="math/tex">h_t</script>。</li>
<li>K-V-P Attention：<script type="math/tex">h_t = \begin{bmatrix}k_t\\vt\\p_t\\\end{bmatrix}</script>，其中<script type="math/tex">p_t</script>和上下文向量一起编码最终的表示。</li>
<li>N-gram RNN：使用前N个time-step的输出向量直接拼接来计算<script type="math/tex">h_t^*</script>。</li>
</ul>
<p>最终在Wikipedia和CBT数据集上进行了测试，wikipedia不了解，不过CBT上的结果只能说一般吧。比稍微复杂一些的RNN模型都要差一点。</p>
<p><img src="/images/FvNZV6oaYk3X6dUp2OVcQPViRLc7.jpg" alt=""></p>
<h4 id="4、Reasoning-With-Memory-Augmented-Neural-Networks-For-Language-Comprehension"><a href="#4、Reasoning-With-Memory-Augmented-Neural-Networks-For-Language-Comprehension" class="headerlink" title="4、Reasoning With Memory Augmented Neural Networks For Language Comprehension"></a>4、Reasoning With Memory Augmented Neural Networks For Language Comprehension</h4><p>ICLR2017上来自马萨诸塞州大医学院的一篇论文。利用<strong>假设检验</strong>这一认知过程中的过程基于Memory-Augmented神经网络，在CBT和WDW数据集上取得了state-of-the-art的结果。</p>
<p>首先，文章引入了认知神经学领域的假设检验过程，这是一个首先提出一个形式化的假设而后对这个假设进行测试的过程，涉及到注意力、Working Memory和认知控制等。依靠注意力和短期记忆来维护、保持、更新假设，而认知控制用来检查并忽略错误的假设。</p>
<p>主要利用了2016年提出的神经语义编码器（NSE）来实现，目标任务是Cloze形式的完型填空任务。可以认为是一个利用假设检验循环的推理过程。</p>
<p>对之前的方法，本文将其分为两种：</p>
<ul>
<li>Single-step模型：包括AttentiveReader，StanfordAttentiveReader，ASReader都属于这一类，其特征是只读取上下文一次，通过一次计算过程预测答案；</li>
<li>Multi-step模型：包括ImpatientReader，EpiReader，GAReader，IAAReader，AoAReader和本文提出的模型都是这类模型。</li>
</ul>
<p>本文的推理次数没有预先确定，而是根据上下文和问题动态确定。</p>
<ul>
<li>memory初始化：首先将上下文d和问题q放入LSTM中编码，并将其输出作为初始化的memory，其中q的编码记忆会更新而d不变：</li>
</ul>
<script type="math/tex; mode=display">
M^q_0=BiLSTM^q(Q) \in R^{k\times |Q|}</script><script type="math/tex; mode=display">
M^d=BiLSTM^d(D)\in R^{k\times |D|}</script><ul>
<li><p>假设检验：</p>
<ul>
<li>构造假设：利用之前对q、d的记忆、q和d在前一时刻的状态<script type="math/tex">s_{t-1}^q,s_{t-1}^d</script>和NSE的read机制，构造当前时刻q和d的状态。利用compose机制计算出上下文。</li>
</ul>
<p><img src="/images/FtJhgFeF1Po4288RnZEx5lyign57.jpg" alt=""></p>
<script type="math/tex; mode=display">c_t$$由compose模块计算，并写入。</script><p>c_t=compose^{MLP}(s^q_t,s^d_t,r_t)<br>$$</p>
<ul>
<li>检验假设：</li>
</ul>
</li>
</ul>
<p>write模块更新当前时刻q的记忆，同时承担着检验更新后的假设，并决定是否停止推理过程的任务。本文提出了两种策略：</p>
<p><img src="/images/FnAdCFFzB8nAxbwUOmZf_qdHTl4E.jpg" alt=""></p>
<p>1.Query Gating，如上图a。这里是利用t-1时刻q的记忆算出gating权重，决定如何更新。使用这种策略时，推理的步数T需要作为超参数指定。</p>
<p><img src="/images/FkV_ukJcH8Obhn0kEaquKr2dzBFQ.jpg" alt=""></p>
<p>2.Adaptive Computation，如上图b。首先计算出一个终止分数<script type="math/tex">e_t\in R</script>。在t步之后结束推理过程的可能性是：<script type="math/tex">p_t=e_t\prod_{i=1}^{t-1}(1-e_i)</script>，而T作为超参数表示最大允许步数，超过之后强行结束。</p>
<p><img src="/images/Fsqj6R2ewS9ozU1AtckI8b0qgU5Y.jpg" alt=""></p>
<ul>
<li>答案预测：</li>
</ul>
<p>这里借鉴了AS-Reader的思想，将第t步问题到文档的对齐权重<script type="math/tex">l_t^d</script>进行softmax从而获得答案的条件概率。</p>
<p>最后，在CBT和WDW数据集上进行了测试。</p>
<p><img src="/images/FjCux0vIJroh3UXgHPXgc0aDnyCq.jpg" alt=""></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>机器阅读理解</tag>
        <tag>完形填空</tag>
        <tag>问答系统</tag>
        <tag>memory</tag>
        <tag>论文阅读</tag>
        <tag>ICLR</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-RNN-NIPS2017-《LightRNN- Memory and Computation-Efficient Recurrent Neural Networks》</title>
    <url>/2017/03/15/deeplearning/NLP-RNN-NIPS2017-%E3%80%8ALightRNN-Memory-and-Computation-Efficient-Recurrent-Neural-Networks%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>文章发表于NIPS2017，是南理工和微软亚洲研究院的几位同学的工作。针对之前RNN训练速度和内存占用方面的不足，提出了LightRNN的改进模型，并进行了语言模型上的对比实验。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>RNN在应用于NLP的时候，如果词典规模过大，RNN的训练会变得很慢而且内存占用高。比如，存储1000万个1024维词向量的矩阵，假如每个scalar是32bit，那么总共会占用40GB的内存。</p>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><ul>
<li>一、模型</li>
</ul>
<p>用一个矩阵来存储所有词向量，每个词由其所在的行和列的向量共同表示，这样V个词向量总共只需要$2\sqrt(V)$个向量来表示，相比于之前的V个向量，大大减少了存储量。也加速了计算。</p>
<p><img src="/images/FlopFkXx4zmFwDc-svQguvtdGOKh.jpg" alt="1"></p>
<p>而在Encoder-Decoder模型中，通过两个基本单元分别计算当前词行向量的概率$P_r(w_t)$和列向量的概率$P_c(w_t)$，然后将两个概率相乘得到$P(w_t)$。由于参数矩阵的维度大幅度减小，模型训练速度也大大提高。</p>
<p><img src="/images/FiqxV7mBfQk3lKJZu79tmmdW_cMn.jpg" alt="2"></p>
<ul>
<li>二、训练</li>
</ul>
<p>那么，怎么样将词分配到词表里呢？</p>
<p>这里提出了一个bootstrap的方法：</p>
<p>（1） 开始时，把单词随机分配到词表中；</p>
<p>（2） 在现有词表的分配条件下，训练输入和输出词向量，直到收敛；若满足停止条件，则结束训练；否则，跳到（3）；</p>
<p>（3） 保持训练的词向量不变，优化词表的分配，使损失函数最小化。跳到（2）。</p>
<p>具体的说，在语言模型中，模型的损失函数就是每个单词的副对数似然概率之和，目标就是最小化它，也就是让下面这个NLL最小：</p>
<script type="math/tex; mode=display">NLL=\sum_{t=1}^T-logP(w_t)</script><p>现在，根据本文提出的模型，$NLL=l_r(w,r(w))+l_c(w,c(w))$，其中$l_r$和$l_c$只是两个指代符号，就是行损失和列损失。</p>
<p>而固定词向量不变，调整词表分配以最小化loss的步骤可以看作如下的优化问题：</p>
<p><img src="/images/FhObUnHIpufNFzpapBNZ5yF4v-U7.jpg" alt="3"></p>
<p>其中，$l(w,i,j)$表示词w在表格中(i,j)这个位置时的loss，而且有$l(w,i,j)=l_r(w,i)+l_c(w,j)$，而后两者在计算第（2）步的时候就已经计算出来了。</p>
<p>而$a(w, i,j)=1$表示将w分配到(i,j)这个位置。</p>
<p>引入一个权重二分图G，则上述的优化问题等价于G上的【<strong>权重完美匹配问题</strong>】，能够用MCMF算法在$O(|V|^3)$的时间内解决，或者用另一种近似算法在$O(|V|^2)$的时间内解决，而后者的开销远小于RNN本身的优化开销，因此训练会很快。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>任务：语言模型</p>
<p>评价方法：PPL</p>
<p>语料：ACLW-2013、BillionW</p>
<p>优化算法：SGD+截断BPTT、学习率初始1.0并动态下降</p>
<p>其他Trick：dropout</p>
<p>训练环境：GPU-K20</p>
<p>框架：CNTK</p>
<p>结果：和state-of-art的RNN模型PPL相当，但Memory和训练时间少很多。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>《Zero-Shot Relation Extraction via Reading Comprehension》</title>
    <url>/2017/06/28/deeplearning/NLP-RE-%E3%80%8AZero-Shot%20Relation%20Extraction%20via%20Reading%20Comprehension%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>PaperWeekly第二十周论文阅读，论文2017年6月发表于ArXiv。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>如果关系抽取任务中，关系类型确定且已经预定义了，那么可以利用<strong>众包</strong>或者<strong>远程监督</strong>的方式构建带标签数据用来训练模型，但是这两种方法对于没有预定义而且训练中无法观测的关系无能为力（即标题中的zero-shot），这也是这篇文章希望解决的问题。</p>
<p>这篇文章提出，实体<script type="math/tex">x,y</script>之间的每种关系<script type="math/tex">R(x,y)</script>都可以规约为一个简单的问题<script type="math/tex">q_x</script>，而其答案是<script type="math/tex">y</script>。比如说<em>配偶</em>这种关系，头实体为奥巴马，那么可以规约为“奥巴马的夫人是谁？”，而其答案是“米歇尔”。通过这样的方法，把关系抽取化为一个阅读理解问题，从而让模型能够预测实体间新的关系类型。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>问题：</p>
<ul>
<li>关系到问题的规约映射如何产生？众包产生训练数据，每种关系对应10个问题。</li>
<li>RC要求问题的答案在给定上下文中，如何解决？利用RC模型，判断问题是否能够回答。</li>
</ul>
<p>给定一个实体e，一个关系R，一个句子s，给出句子中符合<script type="math/tex">R(e,a)</script>的<strong>所有</strong>实体a的名称集合A（以text-spans的形式），如果找不到答案则可以为<script type="math/tex">\emptyset</script>。</p>
<p>现在的问题就是，如何找到一个映射<script type="math/tex">R(e,?)\rightarrow q</script>。这里采用了基于模板的方法，把头实体e作为模板的参数填充进去形成问题。</p>
<p>采用这个方法，把所有训练集转化为(e,R,s,q,A)形式的五元组，其中e和R只在获取答案的时候有用，其实训练样本就是(s,q,A)的三元组，然后用训练集训练阅读理解模型。</p>
<p>测试的时候，对于一种<em>新的关系</em>R’，只需要将其使用新的模板转化为问题，就可以用阅读理解模型预测。</p>
<p>采用的机器阅读理解模型是2016年Seo等人的BIDAF模型。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>对于模型需要解决的问题，其准确率和F1值都很好。</p>
<p><img src="/images/Fh0Aqw3szjoMjMWgg1LesWge5b-0.jpg" alt=""></p>
<h4 id="5、方法的优缺点"><a href="#5、方法的优缺点" class="headerlink" title="5、方法的优缺点"></a>5、方法的优缺点</h4><p>优点：</p>
<ul>
<li>训练时没有的关系类型，在新数据集上也能抽取。</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>关系抽取</tag>
        <tag>机器阅读理解</tag>
        <tag>QA</tag>
        <tag>SQuAD</tag>
      </tags>
  </entry>
  <entry>
    <title>两种网络-MatchLSTM &amp; PtrNets</title>
    <url>/2017/06/13/deeplearning/NLP-RC-SQuAD-%E3%80%8AMatchLSTM&amp;PtrNets%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、MatchLSTM"><a href="#1、MatchLSTM" class="headerlink" title="1、MatchLSTM"></a>1、MatchLSTM</h4><p>最早由(Wang &amp; Jiang,2016)发表在NAACL上的一篇论文提出，用于解决NLI（Natural Language Inference）问题。</p>
<ul>
<li>premise，前提，代表上下文；</li>
<li>hypothesis，假设，代表一个陈述性的结论。</li>
</ul>
<p>模型的结构如下图，其工作的过程是：</p>
<ul>
<li>1.用两个LSTM处理h（假设）和p（前提）序列。得到两个隐藏状态序列<script type="math/tex">h^s</script>和<script type="math/tex">h^t</script>。（s对应前提，而t对应假设）</li>
<li>2.用下面的公式计算注意力向量<script type="math/tex">a_k</script>， 这里计算得到的e经过归一化和加权求和得到假设<script type="math/tex">h^t</script>对前提<script type="math/tex">h^s</script>在每个时刻的注意力向量<script type="math/tex">a_k</script>。</li>
</ul>
<p><img src="/images/FlgZlpVvlxyB5yFCsBr7QxGM8wSr.jpg" alt=""></p>
<ul>
<li>3.上式中<script type="math/tex">h^m</script>的计算， 用下面的公式计算，其中输入<script type="math/tex">m_k</script>是<script type="math/tex">[a_k,h^t_k]</script>的拼接。作者把这个结构称为mLSTM，把最后时刻的<script type="math/tex">h^m</script>作为输出以预测label。</li>
</ul>
<p><img src="/images/Flel3Uqro1-XL15roShCUDOZkjdN.jpg" alt=""></p>
<p><img src="/images/FuuDW11Xv4QZGQA0p5Vb73F1rg4T.jpg" alt=""></p>
<h4 id="2、Point-Network-PtrNet"><a href="#2、Point-Network-PtrNet" class="headerlink" title="2、Point Network(PtrNet)"></a>2、Point Network(PtrNet)</h4><p>Vinyals et al.(2015)在NIPS发表的论文中提出的，用于解决预测输出序列的条件概率，且输出序列中的字符是输入序列的元素的问题。AS-Reader、AoA-Reader和R-Net等模型都用了PtrNet作为输出层的结构。</p>
<p><img src="/images/Fvs9x5Q3zo3HrZLLKSa-c_ABGJUI.jpg" alt=""></p>
<p>PtrNet是修改了Seq2Seq模型得来的。现在设encoder的隐藏状态是<script type="math/tex">e_j</script>，decoder的隐藏状态是<script type="math/tex">d_i</script>。从上面的公式可以看出，PtrNet直接把注意力向量进行softmax归一化得到了条件概率。</p>
<p>PtrNet可以看做是Content-based-Attention的特殊形式。</p>
<h4 id="3、参考文献"><a href="#3、参考文献" class="headerlink" title="3、参考文献"></a>3、参考文献</h4><ul>
<li>Shuohang Wang and Jing Jiang. Machine comprehension using <strong>match-lstm</strong> and <strong>answer pointer</strong>. arXiv preprint arXiv:1608.07905, 2016b. </li>
<li>Shuohang Wang and Jing Jiang. Learning natural language inference with <strong>LSTM</strong>. In Proceedings of the Conference on the North American Chapter of the Association for Computational Linguistics, 2016.</li>
<li>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. <strong>Pointer networks</strong>. In Proceedings of the Conference on Advances in Neural Information Processing Systems, 2015. </li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>机器阅读理解</tag>
        <tag>attention</tag>
        <tag>inference</tag>
        <tag>Point Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-Seq2Seq-《Sequence to Sequence Learning with Neural Networks》</title>
    <url>/2017/03/10/deeplearning/NLP-Seq2Seq-%E3%80%8ASequence-to-Sequence-Learning-with-Neural-Networks%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>GoogleBrain团队，在2014年提出的Seq2Seq方法，对于NMT任务，使用RNN作为一种端到端方法。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>解决机器翻译（NMT）任务，其实已经有一些研究使用RNN及其变种在机器翻译任务中。</p>
<p>对于神经网络的方法，由于需要输入和输出维度固定不变（提前知道），因此如何将神经网络模型应用在序列处理任务中也需要研究。</p>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>使用一个4层LSTM将输入映射到固定长度的向量；使用另一个4层的LSTM将第一个LSTM的输出映射到结果中。其中，第二个LSTM的t时刻输出是一个长度为V的向量，其中V是词汇表长度，每一位表示条件概率。</p>
<p>在进行预测（翻译）时，使用beam search方法，通过条件概率选择最终结果序列。</p>
<p>一个小Trick：将输入中句子的词汇<strong>倒序输入</strong>进行LSTM，这样处理长句子更容易让模型优化，结果就是BLEU分数更高。</p>
<p><img src="/images/FrvlXvE6dNxIOAIM9ZM5f4gdPJFg.jpg" alt="1"></p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>4层LSTM，每层1000个Cell，1000维词向量，输出向量参数80000（输出词典个数）。</p>
<ul>
<li>使用正态分布U(0,0.08)初始化LSTM参数</li>
<li>7.5个Epoch，使用SGD+Momentum训练</li>
<li>mini_batch=128</li>
<li>防止LSTM梯度爆炸，当梯度的2范数$||g||_2$超过阈值时进行缩放</li>
<li>将长度相近的句子放在同一个mini_batch中</li>
</ul>
<p>WMT‘14翻译任务中获得了不错的BLEU分数（几乎State-of-art），在词汇表之外的词汇会被“UNK”符号代替。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://www.zhihu.com/question/54356960" target="_blank" rel="noopener">Beam Search过程</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>Seq2Seq</tag>
      </tags>
  </entry>
  <entry>
    <title>paperWeekly知识图谱阅读小组-《Distant Supervision for Relation Extraction with Sentence-level Attention and Entity Descriptions》</title>
    <url>/2017/05/10/deeplearning/NLP-RelationExtraction-AAAI2017-%E3%80%8ADistant%20Supervision%20for%20Relation%20Extraction%20with%20Sentence-level%20Attention%20and%20Entity%20Descriptions%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>中科院自动化所的科研人员发表在AAAI2017上的一篇文章。paperWeekly知识图谱阅读小组的本周阅读论文。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>利用远程监督的方式，从无标签的文本中进行关系抽取。远程监督就是一种利用已有知识库和无标签语料库中实体的对齐，自动生成标签并训练的方式。</p>
<p>已有的方法忽略了两个重要的问题：</p>
<ul>
<li>1.如何选择具有实体e1和e2的句子，如何确定这些句子对模型训练的权重；</li>
<li>2.没有使用实体的描述语句。</li>
</ul>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>把问题形式化为如下形式：</p>
<ul>
<li>对每个实体e1和e2，搜索语料库中同时包含这两个实体的所有句子的集合<script type="math/tex">B={b_1,...,b_q}</script>；</li>
<li>根据这个集合判断e1和e2具有什么关系，并抽取该关系；</li>
<li>该模型对于两个实体之间具有多种关系（1对多）的情况无法解决。</li>
</ul>
<p>模型如下：</p>
<p><img src="/images/FiFdD6Flo_SR8FZ5w8_QfOpjaBxc.jpg" alt=""></p>
<p>1.利用PCNN的变种APCNN将B中的句子编码为向量。其中包括：</p>
<ul>
<li>将单词映射到包含位置信息和词向量的向量表示；</li>
<li>利用n个卷积核对向量矩阵进行卷积；</li>
<li>利用piecewise-max-pooling对两个实体分隔成的三个部分分别进行最大池化，最后得到一个向量。</li>
</ul>
<p>2.利用注意力机制产生最终的特征向量。</p>
<ul>
<li>利用两个实体的表示相减获得关系的向量表示；</li>
<li>利用APCNN产生的句子表示和关系的表示，计算出注意力权重；</li>
<li>根据注意力权重和句子表示，计算出最终向量。</li>
</ul>
<p>3.利用softmax求出每种关系的概率。</p>
<p>4.利用实体的描述符，通过一个单卷积-池化层的CNN产生其向量表示，并尽量减少实体的向量表示和描述符的向量表示的差别。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>在Freebase和NYT语料库上进行了实验，获得了SOTA的结果。</p>
<h4 id="5、方法的优缺点"><a href="#5、方法的优缺点" class="headerlink" title="5、方法的优缺点"></a>5、方法的优缺点</h4><p>缺点：该模型对于两个实体之间具有多种关系（1对多）的情况无法解决。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>知识图谱</tag>
        <tag>关系抽取</tag>
        <tag>paperWeekly</tag>
        <tag>attention</tag>
        <tag>PCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-RNTN-EMNLP2013-《Recursive deep models for semantic compositionality over a sentiment treebank》</title>
    <url>/2017/03/22/deeplearning/NLP-RNTN-EMNLP2013-%E3%80%8ARecursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>Richard Socher、Andrew.Ng等斯坦福大学的研究人员在EMNLP2013发表的关于递归张量神经网络的论文。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>关于递归神经网络（RNN），一般认为分为时间递归神经网络（也就是循环神经网络，Recurrent Neural Network）和结构递归网络（Recursive Neural Network），本文的模型属于后者，后者通常用于NLP的语义解析等过程。</p>
<p>本文的工作和NLP的5个方面相关：</p>
<ul>
<li>语义向量空间，最主要的方法就是比较词汇之间的相似度度量，比如TF-IDF、同一语义上下文环境下共现频率以及利用词向量进行比较等；</li>
<li>向量空间组合</li>
<li>形式逻辑</li>
<li>深度学习</li>
<li>情感分析</li>
</ul>
<p>本文要解决的，是以往情感分析方法无法准确的判断<strong>情感词作用范围</strong>以及词序对句子情感程度的影响，造成准确率无法提高的问题。</p>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>Stanford Sentiment TreeBank语料库能够给出递归式的语义解析以及情感打分，是进行这个研究的基础。</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=9376cccc34bea508c257c47f456921bb" alt="1"></p>
<p>本文提出了一种张量递归神经网络（RNTN），能够接受<strong>任意长度</strong>的输入。</p>
<ul>
<li>一、经典RNN（1996,2011这个作者提的）</li>
</ul>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=fe830bbdad64d921dc533dee527b35b0" alt="2"></p>
<p>当一个n-gram的上下文输入模型时，会被解析为一颗二叉树，每个单词是叶子节点，而其父节点由叶子节点的向量表示（d维向量）通过语义合成函数g计算得到。模型自下而上计算其他非叶子节点，直到root。</p>
<p>这里，模型的词向量是随机初始化的，并且词向量矩阵是一起训练的。（但是也可以用预先训练的词向量）</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=b7e77c2d56d42a1e05fb6edb3b8e1130" alt="1"></p>
<p>上式就是RNN的一个计算过程，而且其中每一个非叶子节点，同时也要送到一个softmax分类器中进行情感分类。</p>
<ul>
<li>二、MV-RNN（2012，也是由这个作者提出的）</li>
</ul>
<p>经典的RNN中，不同的词节点之间交互不够。（作者认为不够充分）需要改进。</p>
<p>在MV-RNN中，每一个节点被一个(向量，矩阵)二元组表示，而不是RNN中的向量。这样，组合函数g就被参与计算的word参数化了。</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=011692f246717b2c733744f47b9f47c8" alt="3"></p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=118af855a590ed37b1a4e3a1b35f3626" alt="4"></p>
<p>对于一个上面的树，MV-RNN这样计算其中非叶子节点的向量$(R^d)$和矩阵$(R^{d*d})$。这样矩阵和向量又乘又非线性变换，交互够了 。</p>
<ul>
<li>三、RNTN</li>
</ul>
<p>但是MV-RNN的参数数量太多，而且与词典数V有关，这就太慢了。于是，RNTN中变成了这种形式，相当于现对b和c做了二次变换而后再进行线性和非线性变换。而且减少了参数数量。</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=273c5aeea25bf708a7321191bcac251e" alt="4"></p>
<p>模型训练：TBPTS（Tensor-Back-Prop-Through-Structure），通过每个节点的softmax误差反向传播更新参数。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>经过实验证明，在斯坦福的情感数据集上，RNTN相比于其他模型能够取得更高的精确度，能够捕捉到包含正面或者负面情感的词的作用范围和句子中的情感变化。比如，RNTN甚至知道一段话里如果有个BUT，那么BUT之前无论多么正面的评价，其作用范围也不会超过BUT。:sweat_smile:</p>
<p>但是，作者也表明，对于一些复杂的句式，RNTN会无法判断，比如对于下图的几种情况。</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=1fc05f6e4eff76de652a67f849346e80" alt="5"></p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li><a href="https://zh.wikipedia.org/wiki/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener">递归神经网络-维基百科</a></li>
<li><a href="https://en.wikipedia.org/wiki/Recursive_neural_network" target="_blank" rel="noopener">Recursive Neural Network-Wikipedia</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-doc2vec-ICML2014-《Distributed Representations of Sentences and Documents》</title>
    <url>/2017/03/22/deeplearning/NLP-doc2vec-ICML2014-%E3%80%8ADistributed-Representations-of-Sentences-and-Documents%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>Google的Mikolov和Le发表在ICML2014上的论文，提出了doc2vec即文档向量的Representation。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>神经网络在NLP领域应用广泛，而其输入是固定长度的向量，那么固定长度的向量如何存储一个文档的语义，更好的表示一个文档，就是本文希望解决的问题。</p>
<p>现有的词袋模型（bag-of-words，BOW）在表示句子、段落和文章的时候无法体现出语序和抽取语义。</p>
<p>而在词向量基础上提出的已有方法主要有：</p>
<ul>
<li>使用词向量加权平均来表示文档向量，这样仍然丢失了文档的语序信息。</li>
<li>使用解析树来解析生成文档向量（2010），但是这种方法只能用于单个句子。</li>
</ul>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><ul>
<li>0、模型描述</li>
</ul>
<p>本文受最近的利用神经网络训练词向量方法（word2vec）的启示，提出一种无监督的学习方法，对不同长度的文档能够学习到固定长度paragraph-vector来表示其语义。</p>
<p>D表示文档向量矩阵（每一列是一个doc-vector），W表示词向量矩阵（每一列是一个word-vector），其中D可以看作是对于上下文的记忆，因此这个模型也叫做段落向量的分布式记忆模型（PV-DM）。</p>
<p>其中，多个paragraph共享词向量，也就是W不变。</p>
<p>一个paragraph中，context以滑动窗口的形式变化，但是D不变。</p>
<p>context也是一个固定长度的向量，由D和W通过串联或者平均得到，再通过softmax或者分层softmax找到该上下文环境下的下一个词。</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=87293712e68dc0136ad5fe12db21fd23" alt="1"></p>
<ul>
<li>一、模型训练</li>
</ul>
<p>训练时，随机从一个paragraph中抽取一个context，然后预测下一个词，再把误差反向传播到底层，利用SGD更新参数。</p>
<ul>
<li>二、模型预测</li>
</ul>
<p>预测时，我们的目的是对于一个新的paragraph，计算出其文档向量。这时，我们固定模型其他部分的参数，依次选择其中的context并更新paragraph-vector直到满足收敛条件或终止条件。</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=6fda32a2e71a8ac6ecc9c4fb22729b17" alt="2"></p>
<p>最后，介绍了上面这个【文档向量的词袋版本（PV-DBOW）】，利用文档向量预测出随机的几个词向量来学习文档向量，类似于word2vec中的skip-gram。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>通过以下两个任务进行实验：</p>
<ul>
<li>1.【情感分析】，Stanford情感分析数据集和IMDB</li>
<li>2.【信息检索】</li>
</ul>
<p>实验证明，利用文档向量能够让简单的判别模型得到state-of-the-art的结果，证明了doc2vec的有效。</p>
<h4 id="5、方法的优缺点"><a href="#5、方法的优缺点" class="headerlink" title="5、方法的优缺点"></a>5、方法的优缺点</h4><p>优势：</p>
<ul>
<li>不需要label，毕竟是通过context预测下一个词学习文档向量的，自带label。</li>
<li>考虑到了语序。比BOW模型有优势。</li>
</ul>
<p>缺点：</p>
<ul>
<li>来一个新的doc，就需要计算一个vec，计算量有点大。</li>
<li>而且两个doc如果语义相似是不是应该vec也相似呢，但是文章中没有提，应该是无法训练出这种结果，那么这就不科学了。</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>Bilinear Attention Networks</title>
    <url>/2019/11/17/deeplearning/NLP-bilinear-attentiton-networks/</url>
    <content><![CDATA[<h3 id="Bilinear-Attention-Networks"><a href="#Bilinear-Attention-Networks" class="headerlink" title="Bilinear Attention Networks"></a>Bilinear Attention Networks</h3><p>Pub in NeuraIPS’18, by Jin-Hwa Kim, 首尔国立大学。</p>
<p>VQA的任务，主要关注attention部分。下图来自原论文Figure 1。</p>
<p><img src="/images/image-20191117174946648.png" alt="image-20191117174946648"></p>
<h4 id="1-输入矩阵X和Y，分别表示文本和图像两个部分的feature-map。"><a href="#1-输入矩阵X和Y，分别表示文本和图像两个部分的feature-map。" class="headerlink" title="1.输入矩阵X和Y，分别表示文本和图像两个部分的feature map。"></a>1.输入矩阵X和Y，分别表示文本和图像两个部分的feature map。</h4><p>其中文本特征矩阵X的长度是<script type="math/tex">l_1</script>(对应原文的<script type="math/tex">\rho</script>)，图像特征图Y的长度是<script type="math/tex">l_2</script>（对应原文的<script type="math/tex">\phi</script>，这里的<script type="math/tex">\phi</script>是图像检测的object数量）。<script type="math/tex">X\in R^{N, l_1}, Y\in R^{M, l_2}</script>。</p>
<h4 id="2-计算Raw-Attention-Weight。"><a href="#2-计算Raw-Attention-Weight。" class="headerlink" title="2.计算Raw Attention Weight。"></a>2.计算Raw Attention Weight。</h4><p>目标是得到一个矩阵<script type="math/tex">F\in R^{l_1, l_2}</script>，而后可以从dim1或者dim2进行softmax：</p>
<ul>
<li>dot attention：<script type="math/tex">X^T\cdot Y</script>，需要两个feature的维度相同，而且缺少feature维度的线性变换。</li>
<li>bilinear attention：<script type="math/tex">X^T\cdot W\cdot Y</script>，可以认为线性变换之后的X feature map与Y进行dot attention，但是W参数量可能很大。<script type="math/tex">W\in R^{N, M}</script></li>
<li>low-rank bilinear attention: <script type="math/tex">X^T\cdot U\cdot V^T\cdot Y</script>,  把W分解为<script type="math/tex">U\cdot V^T</script>，其中<script type="math/tex">U\in R^{N, d}, V\in R^{M, d}</script>。</li>
<li>Optimized：上面的计算等价于<script type="math/tex">(X^TU)\cdot(V^TY)</script>，也就是Figure 1中的计算。</li>
<li>low-rank bilinear pooling：如果我们把上面一步的矩阵运算分开来，也就是一个向量一个向量的计算，并引入pooling矩阵，那么得到：<script type="math/tex">f_{ij} = P^T((U^TX_i)\circ(V^TY_j))</script>，<script type="math/tex">P\in R^{d, G}</script>，这里得到的<script type="math/tex">f_{ij}\in R^{G}</script>，也就是得到了G个raw attention weight。</li>
</ul>
<h4 id="3-计算attention-weight并使用。"><a href="#3-计算attention-weight并使用。" class="headerlink" title="3.计算attention weight并使用。"></a>3.计算attention weight并使用。</h4><p>目标是得到softmax之后的attention weight，并得到最终的特征输出。</p>
<ul>
<li>计算attention weight：从2中的输出<script type="math/tex">f_{ij}\in R^{G}</script>，如果G&gt;1就是multi-head attention，对dim1或者dim2进行softmax之后得到<script type="math/tex">\alpha_{ij}</script>，这里假设希望得到X的表示，那么<script type="math/tex">\alpha_{i}\in R^{l_2, G}</script>就表示<script type="math/tex">X_i</script>在Y上的G种注意力权重。</li>
<li>unitary attention networks：计算结果是first:【X aware Y representation】或者second:【Y aware X representation】，和上一步一样假如目标是后一个的话，那么计算方式为<script type="math/tex">\hat X_i = ||_{g=1}^G\sum_j\alpha_{ijg}Y_j</script>，也就是G个注意力权重加权求和的向量拼接在一起。<script type="math/tex">\hat X\in R^{l_1, g * M}</script>。</li>
<li>bilinear attention map：论文中引入的与2中很相似的计算方式，但又有些不同，首先single-head的attention map计算方式：<script type="math/tex">A_{ij} = P^T((U^TX_i)\circ(V^TY_j))</script>，和2中最后一步相同，而后把这个公式推广到multi-glimpse形式：<script type="math/tex">A_{gij} = softmax(P_g^T((U^TX_i)\circ(V^TY_j)))</script>，不同的glimpse阶段使用不同的A。注意这里的glimpse和multi-head有所区别。</li>
<li>bilinear attention networks：上上一步我们得到的那个<script type="math/tex">\hat X</script>，它只是缩减了其中一个输出Y的channel，但是如果是进行分类问题，希望一次性得到一个<script type="math/tex">f = R^{K}</script>的表示向量怎么办呢？公式5：<script type="math/tex">f'_k = (X^TU')^T_kA(Y^TV')_k</script>，其中<script type="math/tex">U'\in R^{N, K}, V'\in R^{M, K}, A\in R^{l_1, l_2, K'}, f'\in R^{K}</script>。最后再pooling一次得到输出:<script type="math/tex">f = P^Tf', f\in R^{C}</script>。整个过程表示为<script type="math/tex">f = BAN(X,Y;A).</script></li>
<li>Residual learning:<script type="math/tex">f_{i+1} = BAN_i(f_i, Y;A_i)\cdot 1^T + f_i, f_0 = X, 1\in R^{l_1}</script>，首先增加了输出输出之间的连接，其次输出的向量被复制为<script type="math/tex">l_1</script>份并加到输入。最终还是以X的表示为基础，不断使用X和Y的表示来增强原有的表示，整个过程持续G次（称为G-glimpse），最终分类器使用最后一个阶段的输出的sum：<script type="math/tex">\sum_{i=1}^{l_1} f_G</script>。</li>
</ul>
<h4 id="最后的瞎想："><a href="#最后的瞎想：" class="headerlink" title="最后的瞎想："></a>最后的瞎想：</h4><p>关于两个个feature map：X和Y进行interaction，不加残差的情况下其实有点奇怪，因为想得到【Y aware X representation】，那么希望计算的是fusion了Y的X的信息，最终每个位置却变成了Y的表示的加权求和，X的信息被丢掉了（只用来计算了attention weight）。就像Graph Attention Networks，每个节点的信息等于其邻居信息的加权求和却没有自己的原始信息。所以在进行feature fusion的时候应该加入残差连接，保留主要信息，而interaction得到的表示作为其补充。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>attention</tag>
        <tag>VQA</tag>
      </tags>
  </entry>
  <entry>
    <title>最早的memory?-memory系列之-《Neural Turing Machines》</title>
    <url>/2017/04/18/deeplearning/NLP-memory-2014-%E3%80%8ANeural%20Turing%20Machines%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>本文是Google的研究工作，发表在arXiv2014上。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>计算机程序的三个基本原理：初等变换、逻辑流控制、外部存储。</p>
<p>之前的机器学习研究，很大程度上忽视了后面两者。</p>
<p>而在神经认知领域，working memory是认知领域与初等变换结合的很紧密的一个方向，也涉及到信息的短时操作。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p><img src="/images/Fsw4NfiTidyY7IKuY4tfGHhVekyp.jpg" alt="1"></p>
<p>网络分为两个基本组件：controller和memory，两个部分的所有操作都是<strong>可导的</strong>，能够用GD来训练。对memory的操作基于RNN的输入，为了保证读写操作可导，将其认为是在所有内存单元的加权重读写。</p>
<p>Memory：在t时刻是一个<script type="math/tex">N \times M</script>的矩阵，N是位置，而M是向量的维度。</p>
<p>Controller：一个前馈神经网络或者RNN。</p>
<p>Read：返回一个读向量<script type="math/tex">r_t</script>，<script type="math/tex">w_t(i)</script>是t时刻对i位置的记忆的归一化权重，<script type="math/tex">r_t=\sum_iw_t(i)M_t(i)</script>。</p>
<p>Write：写入记忆的操作借鉴了LSTM，分为擦除和增加两个操作，先后进行。</p>
<p>擦除操作，其中<script type="math/tex">e_t</script>是擦除向量，<script type="math/tex">w_t(i)</script>是每一行memory在t时刻的writer head权重。</p>
<p><img src="/images/Fk1aOMirK1q85y2XTOVufS8yxh8w.jpg" alt="2"></p>
<p><img src="/images/Fk__SSIjoFosh_c2awHj_kde-f3I.jpg" alt="3"></p>
<p>寻址机制：也就是刚刚的读写头权重的计算方式。见下面那张图。</p>
<p>按内容聚焦：按内容聚焦的时候，每个head（头）产生一个长度M的向量，和memory的每一行<script type="math/tex">M(i)</script>通过相似度函数K进行相似度比较，其中<script type="math/tex">\beta _t</script>能够放大或者衰减聚焦的精度。</p>
<p><img src="/images/FiXtDRQQ-S56CB-oXXf64TK-LqAC.jpg" alt=""></p>
<p>按地址聚焦：</p>
<p>利用平滑滤波进行权重更新（类似动量梯度下降）。</p>
<script type="math/tex; mode=display">
w^g_t=g_tw^c_t+(1-g_t)w_{t-1}</script><p>接着进行一个卷积移位的操作（乘以一个控制器提供的权值，加权求和）：</p>
<script type="math/tex; mode=display">
\tilde w_t(i) = \sum_{j=0}^{N-1} w_t^g(j)s_t(i-j)</script><p>最后进行锐化，得到最后的权重：</p>
<script type="math/tex; mode=display">
w^t(i) =\frac {\tilde w_t(i)^{\gamma _t}}{\sum_j\tilde w_t(j)^{\gamma _t}}</script><p><img src="/images/FmVKhyjstUJ2XD2t-HdAMO1q5rLc.jpg" alt=""></p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>对下面几个任务使用NTM进行了测试：</p>
<ul>
<li>复制，测试NTM能否存储和回想起任意信息的长序列。实验表明，NTM在序列长度超过20还是可以持续复制，但是LSTM就不行。</li>
<li>循环复制，NTM模拟嵌套函数的能力。</li>
<li>联想性回忆，测试NTM能否模拟<strong>指针/索引</strong>行为。</li>
<li>动态n-grams：测试NTM能否通过更新memory完成任务。</li>
<li>优先级排序：测试NTM完成复杂任务的能力。</li>
</ul>
<h4 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h4><p>这篇文章，提出了神经网络图灵机。并利用一些实验证明了其能力，从这些实验中可以看出NTM可以看作是RNN和外部Memory的配合，而这个配合也确实提高了RNN在一些任务上的能力。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://zhuanlan.zhihu.com/p/22513016" target="_blank" rel="noopener">知乎专栏</a></p>
<p><a href="http://blog.csdn.net/rtygbwwwerr/article/details/50548311" target="_blank" rel="noopener">CSDN博客</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>attention之外还需要memory-memory系列之-《MEMORY NETWORKS》</title>
    <url>/2017/04/13/deeplearning/NLP-memory-ICLR2015-%E3%80%8AMEMORY%20NETWORKS%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>Facebook AI的研究人员发表在ICLR2015上的论文，引入了一种新的学习模型——memory networks，并利用该模型解决QA及其他自然语言理解任务。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>大多数机器学习模型缺少一种能够将信息以某种方式写入长时记忆组件、并从中读出加以利用的方式。虽然RNN号称能够记住过去的信息，但是这种信息是以隐层状态和权重的方式存在的，信息量不足。</p>
<p>在长文本理解、视频内容理解等任务中，这种长时记忆是必需的。</p>
<p>已有的方法：</p>
<p>2011年之前传统的QA任务模型使用文档作为memory，通过信息检索方式获取答案；</p>
<p>2014年，利用知识库作为memory，把问题映射到逻辑查询；</p>
<p>同时，2014年之后，基于神经网络的方法也得到了了使用，但是没有提过memory。（LSTM不算的话）</p>
<p>2014年和本文同时发表在arXiv上的《Neural Turing Machine》是和本文最相关的工作。</p>
<p>2014年的RNNSearch这个翻译模型中的对齐表，也可以认为是一种memory，但是这里的memory只限于一定的位置。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>Memory Network定义：</p>
<ul>
<li>x：输入，可以是字符、单词或者句子。</li>
<li>m：对象列表（字符串列表或者向量列表），代表memory。</li>
<li>I：输入特征图，把输入转化为内部特征表示。<script type="math/tex">I(x)</script>。可以使用标准的预处理过程，比如解析、共现分析、实体识别等。</li>
<li>G：泛化，用新的输入升级m。<script type="math/tex">m_i=G(m_i,I(x),m), \forall i</script>。G可以是一个简单的按块存储，比如<script type="math/tex">G_{H(x)}=I(x)</script>，也就是把表示存储在H(x)行，H是一个行选择器一样的模块。而且在G中，每次更新应当有所选择，只更新一部分信息；如果memory存储满了，H也可以按照重要度覆盖最不重要的memory。</li>
<li>O：输出特征图，给定输入和当前m的状态，给出输出。<script type="math/tex">o=O(I(x),m)</script>。O应该从memory中读取相关内容，并进行推理。</li>
<li>R：返回，从O转化为期望的返回格式。<script type="math/tex">r=R(o)</script>。R应该产生实际的输入，比如QA中，产生实际的单词，R可以是一个RNN。</li>
</ul>
<p>MemNN的实现：</p>
<p>可以看到，对Memory Networks的定义，论文中给的很抽象，不过接下来，论文给出了一个实际实现的MemNN（Memory Neural Network）。</p>
<p>MemNN的输入是句子（Fact或者问题），这个句子（的向量表示）会被存储在下一个空闲的memory slot中。也就是这个实现不做更新，而是直接存储新的内容。O的作用是找到K个与x最相关的memory。</p>
<script type="math/tex; mode=display">
o_1 = O_1(x,m) = argmax_{i=1,..,N} s_O(x,m_i)</script><script type="math/tex; mode=display">
o_2 = O_2(x,m) = argmax_{i=1,..,N} s_O([x,m_{o1}],m_i)</script><p>其中<script type="math/tex">s_O</script>是一个表征i和mi相似度的函数。最终O的输出是<script type="math/tex">[x,m_{o1},m_{o2}]</script>。</p>
<p>而R的目标是从W个候选词中找到正确的输出。</p>
<script type="math/tex; mode=display">
r=argmax_{w\in W} s_R([x,m_{o1},m_{o2}],w)</script><p>这里，对于两个相似度度量函数<script type="math/tex">s_O,s_R</script>，使用同一种形式：</p>
<script type="math/tex; mode=display">
s(x,y)=\phi_x(x)^TU^TU\phi_y(y)</script><p>MemNN的训练：</p>
<p>训练时，数据集中包含正确的support sentence选择，使用margin ranking损失函数和SGD训练。如下图公式，其中<script type="math/tex">\bar f,\bar f',\bar r,\gamma</script>分别是所有可能的错误support sentence和answer sentence以及margin。</p>
<p><img src="/images/FjY9OZc7grAp1Cz6SczwUor3Bxix.jpg" alt=""></p>
<p>扩展：</p>
<ul>
<li>上述实现是假定输入是句子级别的，如输入是单词级别的，那么需要增加一个segment函数，论文中提供了一个简单的示例；</li>
<li>通过hash提高memory的读写效率（trick）；</li>
<li>获取memory被写入（覆盖）的时间（方便debug和分析）；</li>
<li>如果遇到之前没有见到过的单词，那么就像语言模型中一样，利用其左上下文和右上下文的向量表示它。</li>
</ul>
<h4 id="4、论文的实验、结论和未来工作"><a href="#4、论文的实验、结论和未来工作" class="headerlink" title="4、论文的实验、结论和未来工作"></a>4、论文的实验、结论和未来工作</h4><p>在QA任务上进行了实验，结果远超普通的RNN/LSTM。</p>
<p>未来会在更多任务上进行此类网络的实验，会研究更多的变种，会尝试弱监督式学习。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-charLSTM-ACL2016-《A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation》</title>
    <url>/2017/03/30/deeplearning/NLP-charLSTM-ACL2016-%E3%80%8AA-Character-Level-Decoder-without-Explicit-Segmentation-for-Neural-Machine-Translation%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>蒙特利尔大学的Junyoung Chung 以及Bengio（三作）发表在ACL2016上，针对机器翻译任务提出了一种字符级别的RNN译码器。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>已有的phrased-based和神经网络的机器翻译系统，都是基于词级别的模型。</p>
<p>本文就是想探究，有没有可能更细粒度的模型即字符级别的模型也可以work。（就不需要分词了）</p>
<p>已有的方法（encoder-decoder模型）：</p>
<script type="math/tex; mode=display">
正向Encoder：\vec z_t =\vec \phi(e_x(x_t), \vec z_{t-1})</script><script type="math/tex; mode=display">
反向Encoder： z^\leftarrow_t =\phi^\leftarrow(e_x(x_t),  z^\leftarrow_{t+1})</script><script type="math/tex; mode=display">
Encoder的输出：C = [z_1,...,z_{T_x}],其中z_t = [\vec {z_t}, z^\leftarrow_t]</script><script type="math/tex; mode=display">
Decoder：h_{t'} = \phi (e_y(y_{t'-1},h_{t'-1},c_{t'}))</script><script type="math/tex; mode=display">
P(y_{t'}|y_{<t'},X) \propto e^{f^{y_{t'}}_{out}(e_y(y_{t'}-1),h_{t'},c_{t'})}</script><p>其中，<script type="math/tex">e_x,e_y</script>分别是源语言和目标语言的词向量映射，而<script type="math/tex">c_{t'}</script>是一种软对齐机制（即注意力机制）。</p>
<p>其中，已有模型将单词或者sub-word（2015）作为$x_t$，即使将字符作为$x_t$，也依赖分词这样一个过程（2015）。</p>
<p>那么为什么以往的模型以词为单位进行分析呢？</p>
<ul>
<li>1.单词是意义的最小单位，从字符映射到其意义可能更加困难，比如“quite”、“quit”和“quiet”三个单词从字符层面上很像但意义却完全不同。</li>
<li>2.数据稀疏性，基于计数的模型会受到数据稀疏的影响，比如一个后继序列的概率取决于其在语料库中出现的次数，这样的模型将字符作为基本单元的话状态空间会更大。</li>
<li>梯度消失，由于字符的序列会更长，RNN及其变种中梯度消失问题更严重。</li>
</ul>
<p>既然字符级别的模型有那么多问题，为什么还要用呢？本文进行了分析。</p>
<p>使用词级别的模型的缺点：</p>
<ul>
<li><p>1.没有一个完美的分词算法，导致了“run”，“running”，“runs”这种一个词的变种被低效的表示为三个词向量，如果在较低级别表示，可以把“run”表示为一个词向量，而把“s”、“ing”表示为另一个较短的词向量。</p>
</li>
<li><p>2.对新词（低频词）效果不好。</p>
</li>
<li><p>3.即使常用词，如果其的变种出现次数很少，则模型对该变种处理效果不好。</p>
</li>
</ul>
<p>而字符级别模型的优点有：</p>
<ul>
<li>1.使用RNN能够避免字符级别模型带来的数据稀疏性影响。</li>
<li>2.随着LSTM和GRU的出现，长距离依赖并不是不可以接受。</li>
</ul>
<p>而且，这方面的研究也逐渐出现，最近（2015、2016）的工作中，出现了使用CNN基于字符级别进行文档分类、甚至直接使用unicode比特流进行词性标注（POS-tagging）和命名实体识别。</p>
<p>挑战也有：</p>
<ul>
<li>1.源语言中怎么样将组成句子的字符映射为句子的表示。</li>
<li>2.目标语言如何生成句子（按字符），毕竟一个句子可能有300-1000个字符那么长。</li>
</ul>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>在译码器上使用基于字符级别的RNN。</p>
<ul>
<li>GRU：测试已有的RNN结构是否能够用于字符级别的译码</li>
<li>bi-scale：测试是否有更好的译码器结构。</li>
</ul>
<p>下面是这个网络的结构示意图和公式。</p>
<p>我的理解是，faster layer用来处理字符，slower layer代表词或者词根，因此只有faster layer处理完一个chunk的时候才会reset，从而引起slower layer的update。</p>
<p><img src="/images/FgljiRYHgev9A4959OQo4dzqiIu7.jpg" alt="1"></p>
<p><img src="/images/FlDRE8x_c8CujwCg3tQDXdFRlLfE.jpg" alt="2"></p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>在WMT’15上四种语言翻译对上进行了实验。</p>
<p>源语言使用byte-pair编码（BPE，2015）序列，目标语言使用BPE或者字符序列。</p>
<p>训练时词语选择使用beam search。</p>
<p>结果如下表，其中采用BLEU作为判断标准，得分越高越好，单模型最高分使用粗体，集成模型最高分使用下划线表示。</p>
<p><img src="/images/FolXbQHfKpHnCJUDmN4F_gQcT0N2.jpg" alt="3"></p>
<h4 id="5、方法的优缺点"><a href="#5、方法的优缺点" class="headerlink" title="5、方法的优缺点"></a>5、方法的优缺点</h4><p>文章中总结出来字符级别模型的优点：</p>
<ul>
<li>通过随计算词频下降字符级别和sub-word级别模型词的对数概率差值，可以发现字符级别模型对低频词或者变种词处理效果更好。</li>
<li>的确能够达到一个不错的翻译效果。</li>
</ul>
<p>文中实验的不足：</p>
<ul>
<li>没有对word-level的模型进行实验分析；</li>
<li>没有在encoder上使用char-level的RNN。</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>End2End-memory系列之-《end-to-end memory networks》</title>
    <url>/2017/04/18/deeplearning/NLP-memory-NIPS2015-%E3%80%8Aend-to-end%20memory%20networks%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>本文是Facebook的研究工作，发表在NIPS2015上。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>文章认为，智能模型的关键在以下两点：</p>
<ul>
<li>在完成任务或者回答问题的时候如何执行多步的计算；</li>
<li>如何描述序列中的长时依赖。</li>
</ul>
<p>已有的工作：</p>
<ul>
<li>memory network，不连续，无法端到端的训练因此无法用于很多实际任务中。而且该模型需要数据集中具有更多的信息。</li>
<li>RNNSearch，产生一个输出的时候没有进行多步计算（hop），而本文实验表明，多步计算对于取得良好效果很关键。而且，长时依赖用RNN的状态表示，是不可见和不稳定的。再有就是，RNNSearch使用的注意力机制用于整句话中，而本文的memory可以用于更大的范围</li>
<li>神经图灵机NTM，相对于NTM，本文的模型更简单，不需要锐化等操作，而且在实验方面，本文进行的是文本推理任务，而NTM进行了复制、排序等任务。</li>
</ul>
<p>本文提出了一个类似于《Memory Networks》的网络，但是能够端到端的训练，可以看做是RNNsearch的扩展。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>接受输入<script type="math/tex">x_1,x_2,...,x_n</script>（这些输入存储到memory中)，问题<script type="math/tex">q</script>，产生答案<script type="math/tex">a</script>。</p>
<p>单层模型（描述了该模型的工作原理）：</p>
<ul>
<li>首先计算输入的向量表示：把<script type="math/tex">x_i</script>转化为d维向量表示<script type="math/tex">m_i</script>，把问题<script type="math/tex">q</script>转化为d维向量表示<script type="math/tex">u</script>。（最简单的方式）可以利用两个embedding矩阵A和B。文章中的实验部分包含对句子表示计算方式的进一步阐述。</li>
<li>使用内积计算u和每个<script type="math/tex">m_i</script>的相似度。这里的结果p表征了一种类似于注意力的权重。</li>
</ul>
<script type="math/tex; mode=display">
p_i=softmax(u^Tm_i)=\frac{e^{u^Tm_i}}{\sum_je^{u^Tm_j}}</script><ul>
<li>利用另一个embedding矩阵C，把<script type="math/tex">x_i</script>转化为输出向量<script type="math/tex">c_i</script>，计算从memory得到的输入向量o：</li>
</ul>
<script type="math/tex; mode=display">
o = \sum_ip_ic_i</script><ul>
<li>进而，使用o和u通过softmax得到最终的结果：</li>
</ul>
<script type="math/tex; mode=display">
\hat a=softmax(W(o+u))</script><p>整个模型如下图所示。其中参数矩阵A、B、C、W通过SGD算法最小化<script type="math/tex">crossEntropy(a,\hat a)</script>进行学习。</p>
<p><img src="/images/Fqwqq_bNAbeGeA-LjZXR0X7LtO14.jpg" alt=""></p>
<p>多层模型如上图右边所示，其中，</p>
<script type="math/tex; mode=display">
u_{k+1} = o^k + u^k</script><script type="math/tex; mode=display">
\hat a = softmax(W(o^k+u^k))</script><p>而且每一层都有对应的A、C矩阵<script type="math/tex">A^k,C^k</script>。这些权重的取值，论文中探究了两种：</p>
<ul>
<li>adjacent：也就是<script type="math/tex">A^{k+1}=C^k</script>,<script type="math/tex">W^T=C^K</script>,<script type="math/tex">B=A^1</script></li>
<li>layer-wise：<script type="math/tex">A^{k+1}=A^k=...=A^1</script>,<script type="math/tex">C^{k+1}=C^k=...=C^1</script>,<script type="math/tex">u_{k+1}=Hu_k+o_k</script></li>
</ul>
<p>在layer-wise的权值关系下，这个模型可以认为是RNN的一种扩展，但是每次产生输入中引入的计算步更多。</p>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>上面的模型还有一点没有具体讲，就是句子表示的产生方式（如何用向量表示句子<script type="math/tex">x_i=\{x_{i1},...,x_{in}\}</script>）：</p>
<ul>
<li>BOW即词袋模型的表示方式：把每个单词的表示加在一起，比如：<script type="math/tex">m_i=\sum_jAx_{ij}</script>，这样做的缺点是无法获得词序信息。</li>
<li>PE即词序编码的表示方式：首先产生一个向量<script type="math/tex">l_j</script>表示第j个单词的位置信息，其中<script type="math/tex">l_{kj} = (1-j/J) - (k/d)(1-2j/J)</script>。将此信息用于句子表示的计算中，比如：<script type="math/tex">m_i=\sum_jl_jAx_{ij}</script>。</li>
</ul>
<p>此外，在memory中有时还需要进行时序推理，因此增加了对m的时序编码：</p>
<p><script type="math/tex">m_i=\sum_jAx_{ij}+T_A(i)</script>，其中<script type="math/tex">T_A(i)</script>是矩阵<script type="math/tex">T_A</script>的第i行，该矩阵用来编码时序信息。</p>
<p>上述的模型在文章中叫做MemN2N。在《Towards AI-complete question answering:A set of prerequisite toy tasks. 》这篇2015年论文的数据集上进行了QA实验。并和LSTM、MemNN等模型在上述数据集上进行了比较。其错误率低于除了强监督式学习的MemNN之外的其他baseline。</p>
<p>此外，MemN2N还在语言模型任务上同RNN、LSTM、SCRN在PTB和Text8数据集上进行了对比。PPL指标超过了这些模型。</p>
<p>这些实验说明，本文提出的可以使用反向传播进行端到端训练的包含memory的网络可行而且具有不错的性能。不足之处在于，同MemNN这种强监督式学习的模型相比，性能还有所不如。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>当你的词表过大时，请看-《On Using Very Large Target Vocabulary for Neural Machine Translation》</title>
    <url>/2017/04/12/deeplearning/NLP-vocab-ACL-IJCNLP2015-%E3%80%8AOn%20Using%20Very%20Large%20Target%20Vocabulary%20for%20Neural%20Machine%20Translation%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>蒙特利尔大学的研究人员，发表在IJCNLP2015会议上的一篇论文。讨论了关于词表过大时的一种优化方法。</p>
<h4 id="2、要解决的问题及已有方法"><a href="#2、要解决的问题及已有方法" class="headerlink" title="2、要解决的问题及已有方法"></a>2、要解决的问题及已有方法</h4><p>因为机器翻译最后一层是softmax分类（其实很多自然语言处理任务都是这样），对于每个词其条件概率如下：</p>
<script type="math/tex; mode=display">
P(y_t|y_{<t},x) = \frac {1}{Z} exp \{w^T_t\phi (y_{t-1},z_t,c_t)+b_t\}</script><p>其中Z是所有词的条件概率分子之和。因为对于每个词，都要计算一遍，开销比较大（尤其是训练的时候，开销那么大不能忍）。</p>
<p>已有的解决方法主要有：</p>
<ul>
<li>利用其它的方法，去逼近softmax的概率，比如：基于noise-contrastive estimation （NCE）的方法</li>
<li>分层softmax</li>
</ul>
<p>这两种方法都只能减少训练时的开销，无法减少测试时的开销。</p>
<p>还有一种方法，就是减少词表的大小，比如说对于低频词用OOV代表。</p>
<a id="more"></a>
<h4 id="3、提出的解决方法"><a href="#3、提出的解决方法" class="headerlink" title="3、提出的解决方法"></a>3、提出的解决方法</h4><p>softmax的开销，大多是因为上面提到的公式中的归一化常数Z的计算。提出一种方法，在每次更新时只选择词表的一个子集<script type="math/tex">V^{'}</script>，</p>
<p>对上面的公式取log梯度，得到：</p>
<script type="math/tex; mode=display">
\triangledown logP(y_t|y_{<t},x) = \epsilon(y_t) - \sum_{k:y_k\in V}P(y_k|y_{<t},x)\triangledown \epsilon(y_k)</script><p>其中：<script type="math/tex">\epsilon(y_t) = w_t^T\phi(y_{t-1},z_t,c_t)+b_t</script>，也就是decoder在t时刻的仿射函数输出。</p>
<p>而本文所提出的方法，主要思想就是利用重要性采样采样出一小部分样本，逼近上面公式的负项（第二项）。</p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=caebf9cfe6576171fcc7d6a4e4225b8b" alt=""></p>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=2e4fcd0800fc522b7a30e36f27fa1d52" alt=""></p>
<p>Q是y的分布，<script type="math/tex">v'</script>是这个分布中采样出数据的集合。</p>
<p>在训练之前，将训练数据分区并在每个分区i定义一个子集$V’_i$。</p>
<p>那么怎么选择这个子集呢，在训练时译码的过程中，由于我们知道当前词的正确答案，因此只需要找出这个正确答案加上一些错误的词即可。</p>
<p>那么错误的词怎么选择呢，可以使用一个已有的词对齐模型，对当前的源语句，从对齐模型选择最频繁出现的K个词；对每个单词，解码时选择最频繁出现的K’个词。K和K’可以根据需要设置。这个就是本文提出的一种采样方法。</p>
<p>本文提出的解决方法的优点：</p>
<ul>
<li>1.训练的时间开销与词表大小无关；</li>
<li>2.对内存的需求较小，便于使用GPU计算；</li>
</ul>
<h4 id="4、对该方法的实验评估"><a href="#4、对该方法的实验评估" class="headerlink" title="4、对该方法的实验评估"></a>4、对该方法的实验评估</h4><p>实验的模型：RNNSearch，RNNSearch-LV。其中前者词表数量约为50000，而后者约为500000。</p>
<p>在使用candidate list时，后者的K和K’分别是30000/5000和10/20，</p>
<p>最后，还提出了一种替换编译语句中UNK的方法，就是利用词对齐模型中原词汇对齐的概率最高词替换这个UNK。</p>
<p>训练结果BLEU：在RNNSearch-LV上效果比RNNSearch好，说明词表增加是有用的，而且本文的抽样方法不会导致性能下降。</p>
<p>训练时间：用平均译码时间衡量，如下图所示，说明了词表大的情况下，用本文的抽样方法，可以极大的节约训练时间。</p>
<p><img src="/images/FuK0YFcKhUBYFwac6x6o-yv_iZij.jpg" alt=""></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>天气太热-略读一篇-《Recent Trends in Deep Learning Based Natural Language Processing》</title>
    <url>/2017/08/17/deeplearning/NLP-%E3%80%8ARecent%20Trends%20in%20Deep%20Learning%20Based%20Natural%20Language%20Processing%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1、文章来源"><a href="#1、文章来源" class="headerlink" title="1、文章来源"></a>1、文章来源</h4><p>2017年8月发在ArXiv上的一篇综述。对各种深度学习模型和策略在NLP各种任务上的应用和演进进行了介绍。链接：<a href="https://arxiv.org/abs/1708.02709" target="_blank" rel="noopener">https://arxiv.org/abs/1708.02709</a></p>
<h4 id="2、内容"><a href="#2、内容" class="headerlink" title="2、内容"></a>2、内容</h4><p>内容大略是按照模型来划分的。</p>
<ul>
<li>1.分布式的向量化表示</li>
</ul>
<p>计算机只能识别向量化表示的元素。由于语言模型中词汇数量过大，因此需要将词汇编码到更低维度的向量空间中。Word Embedding、Character Embedding等不同层次的元素分布式表示是很多NLP任务近年来效果不断提升的基础。</p>
<a id="more"></a>
<ul>
<li>2.卷积神经网络、循环神经网络、循环神经网络和递归神经网络</li>
</ul>
<p>CNN能够有效的挖掘上下文窗口中的语义信息，抽取句子中的主要含义，但是也存在参数多需要大量数据、长距离上下文信息的编码和位置信息的编码等问题。文中对经典CNN及windows-based-CNN、DCNN、TDNN等变种在情感分析、文本分类等任务上的有效应用进行了描述。</p>
<p>RNN的结构符合语言内在的序列特征，而且能够处理任意长度的文本序列。RNN及其变种LSTM、GRU等在本文处理任务中得到了非常普遍的应用。</p>
<ul>
<li>3.注意力机制和记忆模块（略）</li>
</ul>
<ul>
<li>4.深度强化学习和生成网络</li>
</ul>
<p>对一些不可微的优化问题，将强化学习和深度神经网络结合的方式（尤其是在一些生成模型中）取得了不错的效果。</p>
<p>VAE和GAN这种生成模型也因为在生成图像方面的成功被尝试引入NLP任务。</p>
<p>最后，文章引用乔姆斯基的一段话表达了他对于符号逻辑和统计分析的看法。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>Tutorial on Variational AutoEncoders</title>
    <url>/2017/11/17/deeplearning/Tutorial%20on%20Variational%20AutoEncoders/</url>
    <content><![CDATA[<p>本文是<a href="https://arxiv.org/abs/1606.05908" target="_blank" rel="noopener">《Tutorial on Variational AutoEncoders》</a>一文部分翻译的内容。</p>
<h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h4><p>generative model，学习高维数据的概率分布<script type="math/tex">P(X)</script>。学习到不同维度之间的相互依赖关系，比如手写数字的生成模型如果生成了8的左边一半像素，那么剩下的像素也就能够随之确定。</p>
<p>latent variable，给予生成模型一些信息用来生成数据。比如一个生成手写数字的生成模型，隐变量z就可以是从[0,…,9]中随机采样的整型，确定了隐变量模型才能生成像素点。</p>
<p>但是隐变量又不可能那么简单，因为对于一个能够表达出数据集特性的生成模型，对于每个数据点，生成模型都要能够产生出一个隐变量来生成一个<strong>非常</strong>相似的东西。对手写数字数据集来说，同样的一个8，可能大一点或者小一点，往左歪一点或者往右歪一点，字迹是粗一点还是细一点。所以，生成模型中隐变量需要表达更多的信息，需要一个高维度的向量来表示隐变量<script type="math/tex">z\in R^Z</script>，隐变量要能够自动的学习到这些信息的表示，而且这些影响手写数字生成的信息之间的相互影响和依赖，即隐变量的latent structure，也应该自动的学习。</p>
<h4 id="2-目标函数"><a href="#2-目标函数" class="headerlink" title="2.目标函数"></a>2.目标函数</h4><p>VAE的过程是：从概率密度函数P(z)中采样隐变量z之后，从确定性函数<script type="math/tex">f(z;\theta)</script>得到的值，这个值要大概率和数据集中的数据很相似。</p>
<p>所以我们的目标是对数据集中的所有数据x，最大化：</p>
<script type="math/tex; mode=display">
P(X) = \int P(X|z;\theta)P(z)dz</script><p>其中的条件概率<script type="math/tex">P(X|z;\theta)</script>等于<script type="math/tex">N(X|f(z;\theta), \sigma I)</script>，也就是在标准VAE中，如果输出是实数向量，通常输出的分布是均值为<script type="math/tex">f(z;\theta)</script>方差为<script type="math/tex">\sigma I</script>的高斯分布。</p>
<p>VAE要最大化P(x)，就要解决两个问题：</p>
<ul>
<li>隐变量z如何表示？</li>
</ul>
<p>使用标准正态分布<script type="math/tex">N(0,I)</script>，因为只要有一个n维标准正态分布的z，再找到一个足够复杂的函数g，那么g(z)就可以表示任意一个n维分布。</p>
<p>对上面的公式而言，<script type="math/tex">f(z;\theta)</script>就是一个多层神经网络组成的函数逼近器。可以将将隐变量映射到最后的输出。</p>
<ul>
<li><script type="math/tex">P(X)</script>的积分如何计算？</li>
</ul>
<p>现在我们得到了<script type="math/tex">P(z)=N(0,I)</script>，可以直接计算<script type="math/tex">P(X)</script>了吗？还是不行，假如采用抽样的方式抽样出很多z来计算<script type="math/tex">P(X)</script>的话就会发现，对于绝大多数z来说，<script type="math/tex">P(X|z)</script>都接近于0（因为z是标准正态分布的采样），这种方式非常低效。</p>
<p>VAE的解决这个问题的核心思想就是，试图抽样出有更大可能性产生X的z。而做到这一点是通过另一个分布<script type="math/tex">Q(z|X)</script>来实现的，这个分布产生的Q能够以较大的可能产生X。</p>
<p>但是我们使用了Q分布后，<script type="math/tex">P(X)</script>积分公式就变成了<script type="math/tex">E_{z\sim Q}P(X|z)</script>，我们要找到后者和<script type="math/tex">P(X)</script>之间的关系。根据<script type="math/tex">P(z|X)和Q(z|X)</script>的KL散度的公式进行推导，可以推出：</p>
<p><img src="/images/FpGiJ4m_kuLDnwaI2bEnTa0Gy5eM.jpg" alt=""></p>
<p>其中左侧的部分就是目标函数，我们要最大化这个目标函数就是在最大化对数似然概率的同时，最小化我们预测的分布Q和真实分布P之间的差异。</p>
<h4 id="3-目标函数优化"><a href="#3-目标函数优化" class="headerlink" title="3.目标函数优化"></a>3.目标函数优化</h4><p>那么<script type="math/tex">Q(z|X)</script>是一个什么分布呢？通常的选择是，也是一个多元高斯分布<script type="math/tex">N(z|\mu(X),\sigma(X))</script>，其中的<script type="math/tex">\mu和\sigma</script>都是由神经网络学习到的映射。</p>
<p>所以目标函数等号右侧的最后一项就变成了两个多元高斯分布的KL散度，因为我们假定<script type="math/tex">P(z)</script>是标准多元高斯分布。在得到了Q分布之后，这一项是能够计算出来的（当然也能够利用梯度下降来优化）。</p>
<p>而右侧的第一项，也就是<script type="math/tex">E_{z\sim Q}[logP(X|z)]</script>，通过当前时刻的Q分布，从中采样出若干个z，然后从这若干个z中计算出<script type="math/tex">P(X|z)</script>作为期望的逼近值。</p>
<p>但是这样做的话，由于采样操作对<script type="math/tex">\mu</script>和<script type="math/tex">\sigma</script>不可导，反向传播无法进行，因此，实际的VAE中使用了一个叫做reparameterization的改进，以从<script type="math/tex">N(0,I)</script>中抽样出数值<script type="math/tex">\epsilon</script>后，用<script type="math/tex">\epsilon</script>乘以方差再加上均值的结果代替从<script type="math/tex">N(z|\mu(X),\sigma(X))</script>中抽样，如下图。</p>
<p><img src="/images/Fr5BnYnPD05J7luly4ixXxUg5b9B.jpg" alt=""></p>
<p>所以最终，VAE的优化目标就是，让最终的输出<script type="math/tex">f(z)</script>与目标<script type="math/tex">x</script>足够接近的同时，隐变量的分布<script type="math/tex">Q\sim N(\mu, \sigma^2)</script>接近真实先验分布<script type="math/tex">P\sim N(0,I^2)</script>。</p>
<h4 id="4-条件变分自编码器"><a href="#4-条件变分自编码器" class="headerlink" title="4.条件变分自编码器"></a>4.条件变分自编码器</h4><p>为了解决某种问题，比如说给定某个人的手迹，要求生成另一些与他的手迹很像的手迹。这时候需要制定输入计算输出<script type="math/tex">P(Y|X)</script>，为了解决这类问题，从变分自编码器衍生出条件变分自编码器。</p>
<p><img src="/images/FkmcbSMY5VyFhPyRC2VRvKtbDTmq.jpg" alt=""></p>
<p>模型在Encoder和Decoder的输入中，增加了条件输入X，如上图。当然这张图只是一个简化版本，实际的CVAE有很多论文提出了很多不同的版本。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>变分自编码器</tag>
        <tag>VAE</tag>
        <tag>生成模型</tag>
        <tag>variational Bayesian method</tag>
      </tags>
  </entry>
  <entry>
    <title>《MEMEN： Multi-layer Embedding with Memory Networks for Machine Comprehension》</title>
    <url>/2017/12/20/deeplearning/%E3%80%8AMeMEN-Multi-layer%20Embedding%20with%20Memory%20Networks%20for%20Machine%20Comprehension%E3%80%8B/</url>
    <content><![CDATA[<h4 id="1-论文的概述及背景"><a href="#1-论文的概述及背景" class="headerlink" title="1.论文的概述及背景"></a>1.论文的概述及背景</h4><p>浙江大学的学生在艾耕科技实习时产出的一篇论文。</p>
<p>现有的MRC的模型，存在两点不足：</p>
<ul>
<li>对编码（encoding）层缺少句法信息和命名实体信息的嵌入，影响了编码质量。比如，苹果可以代表水果也可以代表苹果公司，取决于具体语境，但词向量是相同的，这就需要对句法、语义和命名实体的分析和标注来提高编码质量。</li>
<li>很多模型使用了注意力机制，通常是使用一个向量来表示查询语句的每个单词或是整个查询语句，这无法体现查询语句中关键字的权重，因为用一个向量来表示整个查询语句会损失信息，而表示每个单词那么在注意力计算过程中会增加噪音。</li>
</ul>
<p>本文针对这两点进行了改进。</p>
<h4 id="2-模型概述"><a href="#2-模型概述" class="headerlink" title="2.模型概述"></a>2.模型概述</h4><p><img src="/images/FlY4LAAp4lP56WWu4apTCwtUA-aC.jpg" alt=""></p>
<p>模型的整体结构如上图所示，其中包括三个部分：</p>
<ul>
<li><p>编码层：<strong>利用skip-Gram模型训练出P和Q的NER和POS向量</strong>，同one-hot向量相比这种做法更能体现tag的语法和语义影响。把【词向量，字符向量，tag embedding】输入到双向LSTM中得到编码后的向量表示。</p>
</li>
<li><p>交互层：利用注意力机制进行交互。</p>
<ul>
<li>Integral Query Matching：利用Q的向量表示u作为attend vector，P的向量矩阵<script type="math/tex">r^P</script>作为memory vector，计算注意力权重并对<script type="math/tex">r^P</script>进行加权求和得到加权向量表示<script type="math/tex">m^1</script>。</li>
<li>Query-Based Similarity Matching：首先计算出对齐矩阵（alignment matrix）A，而后通过在每行进行softmax的方式，计算出<script type="math/tex">r_Q</script>的加权向量表示<script type="math/tex">m^2</script>。</li>
<li>Context-Based Similarity Matching：通过对对齐矩阵A的每列，也就是每个Q进行max函数选择，选择Q中最相关的单词，与<script type="math/tex">r^P</script>进行内积相乘得到向量表示<script type="math/tex">m^3</script>。</li>
<li>通过一个线性映射f，计算出输出<script type="math/tex">M=f(m^1,m^2,m^3)</script>，而后通过一个gate来过滤M，并将过滤后的M作为RNN的输入，即可得到输出O：</li>
</ul>
<script type="math/tex; mode=display">
g_t = sigmoid(W_gM + b) \\
O_t = BiLSTM(O_{t-1}, g_t\odot M)</script></li>
<li><p>输出层：利用Pointer Network作为输出层，以boundary的形式预测答案。</p>
<ul>
<li>初始状态是query-aware representation。</li>
<li>利用GRU和O计算出答案的span。</li>
</ul>
</li>
</ul>
<h4 id="3-实验和结论"><a href="#3-实验和结论" class="headerlink" title="3.实验和结论"></a>3.实验和结论</h4><ul>
<li>在TrivialQA和SQuAD两个数据集上进行了实验，结果很好；</li>
<li>经过在GPU上训练速度的对比，同MatchLSTM相比，由于没有了self-match这一过程，训练时间大大缩短；</li>
<li>Ensemble的模型是用不同的学习率和Dropout比率训练14个模型，并采取其中置信度最高的候选答案；</li>
<li>经过Ablation analysis，Query-Based Similarity Matching对准确率的影响最大。</li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器阅读理解</tag>
        <tag>attention</tag>
        <tag>SQuAD</tag>
        <tag>TrivialQA</tag>
      </tags>
  </entry>
  <entry>
    <title>利用Test Adaptation提高模型泛化性</title>
    <url>/2020/11/14/deeplearning/Test%20Adaption%20Robust%20Learning/</url>
    <content><![CDATA[<h3 id="Fully-Test-Time-Adaptation-by-Entropy-Minimization"><a href="#Fully-Test-Time-Adaptation-by-Entropy-Minimization" class="headerlink" title="Fully Test Time Adaptation by Entropy Minimization"></a>Fully Test Time Adaptation by Entropy Minimization</h3><p> 来自ArXiv，by DeepAI和伯克利，ICLR’21高分论文。</p>
<p>一、Motivation：</p>
<ul>
<li>1.focus on一个独特的任务设置，这篇工作希望解决的setting是，当模型训练好之后，在没有任何监督信息的测试数据上使用时，只有parameter和test data，如何自适应的进行预测从而在测试数据上表现最佳。下表展示了fully test-time adaptation和之前的几种setting的区别。</li>
<li>2.之前的类似任务上的方法为什么不能直接应用过来，它们的不足（fine-tune着力于训练阶段，领域自适应也需要训练数据，TTT比较相似但是在训练&amp;测试同时进行adaption）。</li>
</ul>
<p><img src="/images/1605540714424-9396b766-85cb-4fa1-be3a-2adb14578d94.png" alt=""></p>
<blockquote>
<p>背景知识：</p>
<p>1.高熵代表不确定性（根据最大熵原理在满足已知条件的模型中，预测熵最大的模型是最好的模型）【label smoothing或者dropout都可以看作是一种软最大熵约束】；低熵代表高置信度。</p>
<p>2.领域自适应的方法一般是应用在训练时，主要利用特征对齐、对抗不变性、共享代理目标等学习一个目标领域的表示。即使是最近的source-free的方法也需要利用生成建模和伪标签等、效率比较低。</p>
</blockquote>
<p>二、创新点&amp;设计思路：</p>
<ul>
<li><p>引入测试熵</p>
<ul>
<li>熵非常general与任务无关，但相比于自监督学习，熵又是task-specific的不需要手工设计目标。（自监督：补全context、rotation预测、添加噪声的自编码目标）而且，熵和performance具有反向相关性。</li>
</ul>
</li>
</ul>
<p><img src="/images/1605616031989-970335be-01bd-4ff7-872c-1d8c7760b2e4.png" alt=""></p>
<ul>
<li><p>测试阶段的normalization</p>
<ul>
<li>从测试数据中以滑动平均的方式估计μ和σ，利用熵作为目标函数学习β和γ。adaption一个Epoch。</li>
</ul>
</li>
</ul>
<p><img src="/images/1593175280706-8ef80f38-7e3f-4424-bdcb-91c2d6d1e859.png" alt=""></p>
<p>三、实验设计</p>
<p>1.数据：不同规模图像分类数据 &amp; 图像分类的领域自适应/迁移学习数据集 &amp; 语义分割迁移学习数据集，训练数据被使用多种方式corruption，比如增加对比度、增加高斯噪声等。</p>
<p><img src="/images/1605614741573-f3615372-a1e9-4be9-8745-66ec58a5a9a3.png" alt=""></p>
<p>2.结论</p>
<ul>
<li>Tent不仅能够提升鲁棒学习和迁移学习的效果，而且开销很低</li>
<li>测试集entropy和loss之间有一定的正相关</li>
<li>ablation：不normalization只re-scale比不过BN，直接修正所有参数没效果，但没放出数据</li>
<li>比较：与BN相比，BN倾向于修复corrupt的feature，而Tent倾向于生成接近Oracle的feature分布。</li>
</ul>
<p><img src="/images/1605604797533-ae921188-4b97-4637-a4c5-f783c899a1a4.png" alt=""></p>
<p><img src="/images/1605605078058-b5a1e882-13e5-402e-a422-285bc7fb9235.png" alt=""></p>
<p><img src="/images/1605614412634-d0193dad-c008-4eda-8fe5-db0ae2fa3364.png" alt=""></p>
<p>四、几个了解或者Survey的相关工作：</p>
<ul>
<li><p>训练时使用预测熵：</p>
<ul>
<li>pseudo labeling（PL，2013）：训练时利用置信度修正标签并利用修正后的标签继续训练</li>
<li>ProSelfLC（2020）: Progressive Self Label Correction for Training Robust Deep Neural Networks，利用模型的预测熵来反映模型对预测置信度，在噪声标签和模型预测分布之间找到平衡</li>
</ul>
</li>
<li><p>利用测试熵，同期的两个工作：</p>
<ul>
<li><p>测试时利用熵进行归一化的工作-BN（2020，<a href="https://arxiv.org/pdf/2006.16971.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2006.16971.pdf</a> NIPS’20，<a href="https://arxiv.org/pdf/2006.10963.pdf）" target="_blank" rel="noopener">https://arxiv.org/pdf/2006.10963.pdf）</a></p>
</li>
<li><p>第一个工作很直接，把训练集的一阶距和二阶距作为先验，利用测试数据估计一个新的均值和方差，并增加一个超参数决定先验的权重。</p>
</li>
<li>第二个工作也很直接，用测试时的一个batch的统计信息作为均值和方差。</li>
</ul>
</li>
<li><p>robust learning和自适应学习的baseline：</p>
<ul>
<li>鲁棒学习和噪声学习的区别：前者标签是对的，只是输入被部分破坏了；后者输入是对的，但标签打错了。</li>
<li>UDA-SS（2019）：自监督的领域自适应方法，通过交叉训练试图学习到corruption鲁棒的表示。</li>
<li>ANT（2019，ICCV’20）：对抗学习的框架，训练一个噪声生成器生成尽量能fool网络的噪声，网络需要尽量判别出改生成器生成的噪音图像。</li>
<li>TTT（2019，ICML’20）：找到一个良好定义而且不简单的代理任务（比如识别图像的旋转），增加一个feature extractor模块，而后在训练时增加原始域中样本的代理任务判别，在测试时通过自监督的方式更新feature extractor模块的参数。</li>
</ul>
</li>
<li><p>神经网络的Normalization，比如Layer Norm、Batch Norm或者instance Norm。</p>
<ul>
<li>batch norm，在激活层之前进行，首先使用滑动窗口得到的μ和σ归一化，而后使用训练后的参数β和γ进行re-scale只保留高阶信息。测试时使用训练数据的μ和σ，β和γ不变。参数是channel-wise的（对图像来说）。</li>
</ul>
</li>
</ul>
<p>启发：</p>
<blockquote>
<p>模型训练时学到的分布不仅仅是预测结果，也表明了模型的confidence，这些信息不仅在训练时而且在预测时都是有用的。（迁移学习、领域自适应、少样本or零样本学习、元学习）</p>
</blockquote>
<p>疑问（包括reviewer的）：</p>
<blockquote>
<p>测试集的统计信息（即使只是输入特征的均值的滑动平均）能否被用于模型调整。</p>
<p>在什么情况下会失效？熵是否总是（或大部分时间）提供了正确的信息。</p>
</blockquote>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>泛化性</tag>
        <tag>Distribution Adaptation</tag>
      </tags>
  </entry>
  <entry>
    <title>与NER相关的联合抽取模型[转]</title>
    <url>/2017/07/15/forward/joint%20models/</url>
    <content><![CDATA[<p>转自<a href="lanzhuzhu.github.io">博客</a>.</p>
<h1 id="Joint-Named-Entity-Recognition-and-Disambiguation"><a href="#Joint-Named-Entity-Recognition-and-Disambiguation" class="headerlink" title="Joint Named Entity Recognition and Disambiguation"></a>Joint Named Entity Recognition and Disambiguation</h1><h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><p>Gang Luo 1 , Xiaojiang Huang 2 , Chin-Yew Lin 2 , Zaiqing Nie 2     Microsoft   [EMNLP 2015] cites[45]</p>
<h2 id="What"><a href="#What" class="headerlink" title="What:"></a>What:</h2><p>抽取NER，并链接到知识库，一般分开，当做先后两步做。而本文联合NER and linking tasks两步一起做，能获得他们之间的共同依赖。本文提出<strong>JERL</strong>, Joint Entity Recognition and Linking，同时做实体识别和实体消歧，实验证明在这两个数据集上CoNLL’03/AIDA均有提高。</p>
<h2 id="Pros-and-cons"><a href="#Pros-and-cons" class="headerlink" title="Pros and cons"></a>Pros and cons</h2><p>优点：</p>
<p>能利用实体识别和实体链接之间的信息，能够克服如下问题（分开训练和优化实体识别和实体链接任务的缺点）：1. 实体识别的错误会传播到实体链接，且不可恢复；2.实体识别不能利用实体链接的信息；3.实体识别和链接可能产生不一样的结果输出。实体边界识别和实体类型预测都与通过实体链接与实体的知识关联，因此建模这种依赖关系能解决不一致，提高性能。</p>
<p>Sil13年是利用最好的NER系统，<strong>来结合KB，生成候选实体</strong>，然后利用链接算法进行重排序，没有看到链接的结果如何帮助影响ner。</p>
<p>本文模型从训练阶段 ，就联合建模NER and linking两个任务，因此能更好地考虑实体类型和置信度信息【conﬁdences information】之间的共同依赖，一起决定ner 和链接。</p>
<p>缺点：</p>
<p>优化耗时，增加了问题的复杂性，不够有效率。要求合理的假设和估计来处理训练和推断过程。</p>
<p>3 contributions:</p>
<ol>
<li><p>识别出NER 和linking之间的共同依赖，并证明两个任务能一起提高性能；</p>
</li>
<li><p>第一个提出<strong>JERL</strong>，提出有效的一起训练和推断的算法；</p>
</li>
<li><p>CoNLL’03数据集上效果最好，证明了NER能利用知识库和链接技术提高。</p>
</li>
</ol>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p>  实体识别一般是一个序列标注任务，实体链接是重排序任务；难点是如何把序列标注和排序问题结合起来。</p>
<p>JERL是一个概率图模型，计算最大概率的参数。</p>
<p><a href="https://medium.com/@neerajsharma_28983/intuitive-guide-to-probability-graphical-models-be81150da7a" target="_blank" rel="noopener">一个概率图模型的直观解释</a>。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文通过联合计算实体识别和实体链接的概率，将序列标注和排序问题联合起来，计算某种组合的最大概率。方法复杂，但是通过挑选合适的特征和参数训练，取得了这个数据集上最好的结果。</p>
<h1 id="CoType-Joint-Extraction-of-Typed-Entities-and-Relations-with-Knowledge-Bases"><a href="#CoType-Joint-Extraction-of-Typed-Entities-and-Relations-with-Knowledge-Bases" class="headerlink" title="CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases"></a>CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases</h1><h2 id="Source-1"><a href="#Source-1" class="headerlink" title="Source:"></a>Source:</h2><p>Www2017</p>
<p>Xiang Ren † Zeqiu Wu † Wenqi He † Meng Qu † Clare R. Voss ‡ Heng Ji ♯ Tarek F. Abdelzaher † <strong>Jiawei Han</strong> †</p>
<p>伊利诺伊香槟分校。cites[12]</p>
<h2 id="Main-idea"><a href="#Main-idea" class="headerlink" title="Main idea"></a>Main idea</h2><p>任务是从大量文本中抽取感兴趣类型的实体和关系。传统方法依赖于标注数据和依次抽取，移植到新的领域需要新的专家知识，而且错误会在pipeline中传播。本文通过distant supervision构造的噪音数据来联合抽取某种类型的实体和关系，问题是通过DS方法获得的类型标签是上下文不可知的，数据是有噪音的。本文提出领域独立的框架COTYPE抽取实体名字，他将entity mentions, relation mentions, text features and type labels联合编码到两个低维空间中，每个空间都是相似类型的有相似表达。COTYPE，通过学习到的embeddings,去估计测试的名字。形成从文本语料和知识库中去学习embeddings的联合优化问题。 对噪音标签数据采用novel partial-label损失函数？。引进翻译函数的目标去获得实体和关系间的交叉限制。实验在三个数据集（news, 生物医学）不同领域都很有效，F1提高了25%。</p>
<p>总的来说，就是联合抽取实体的类别和关系的类型。关系实例就是某一种类型的关系。</p>
<p>针对某一类问题，细致地分析，科学地实验解决。</p>
<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>给定POS-tagged语料D，知识库KB，目标实体类别的层次结构Y，目标关系类型集R【后两者都在KB中】。联合抽取能够完成的任务是：从D中检测出实体M；利用KB生成训练集<script type="math/tex">D_L</script> ;使用训练集<script type="math/tex">D_L</script>和他的上下文句子s,估计出每个实体对的关系类型以及每个实体的single type-path，类型路径。</p>
<p>简而言之，就是利用KB，抽取出语料中的实体，实体以及相应关系的类型。举例如图，实体名字确定，但是由于他们所属的类型不同，他们的关系也会在不同的语句中不一样。所以利用KB，联合抽取出实体类型和关系。<img src="https://ws2.sinaimg.cn/large/006tNc79gy1fi2977rdcij30vr0n1793.jpg" alt=""></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ol>
<li>通过POS限制的文本分割算法，在KB获得的正样例上运行，检测到候选的实体集M。<ul>
<li>利用了基于词性、短语质量等特征，去挖掘频繁的模式【词序列和POS序列的模式】。抽取特征训练随机森林分类器，来估计候选短语和候选POS模式的质量。使用估计的分段质量分数来挑选。总之，这是一个基于特征和模式匹配的方法抽取NER。</li>
</ul>
</li>
<li>从M中产生候选的关系名称，并抽取出每个关系名的候选文本特征。应用远程监督产生标注的训练集。</li>
<li>联合嵌入关系和实体名称，文本特征，类型标签到两个低维空间，每个空间中，相近的物体倾向于共享类型。</li>
<li>估计出关系的类型和候选实体的类型。</li>
</ol>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>优点：问题、专用名词描述、还有问题定义很规范。</p>
<p>缺点：</p>
<p>只能解决部分实体和关系的抽取问题，要求实体的类型和关系都在KB中出现过，而某些特定领域的KB不完善，很稀疏，或者只有很少的关系，此时本文方法就不能很好适用。</p>
]]></content>
      <categories>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>关系抽取</tag>
        <tag>命名实体识别</tag>
        <tag>实体消歧</tag>
        <tag>Knowledge Base</tag>
      </tags>
  </entry>
  <entry>
    <title>将Memory Network用于RC和QA的五篇论文</title>
    <url>/2017/11/21/deeplearning/five%20papers%20about%20memory%20and%20machine%20reading%20comprehension/</url>
    <content><![CDATA[<h4 id="1-《Large-scale-Simple-Question-Answering-with-Memory-Networks-》"><a href="#1-《Large-scale-Simple-Question-Answering-with-Memory-Networks-》" class="headerlink" title="1.《Large-scale Simple Question Answering with Memory Networks 》"></a>1.《Large-scale Simple Question Answering with Memory Networks 》</h4><p>Facebook AI在15年6月发在arXiv上的论文：<a href="https://arxiv.org/pdf/1506.02075。主要阅读模型部分。" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.02075。主要阅读模型部分。</a></p>
<blockquote>
<p>simple QA，即一类QA问题，这类问题能够转化为涉及一个entity和一个relation的查询，不涉及复杂的推理过程（比如结合多个KB中的fact和原文才能得出答案），只需要找到correct evidence（比如KB中的一个三元组）就可以正确回答。</p>
</blockquote>
<p>文章首先说明了虽然reasoning的研究最近很热，但是simple QA这类问题尚未解决，而且解决了之后应用范围也很广（通过更好的知识图谱支持）。</p>
<p>这篇文章主要工作：</p>
<ul>
<li>构造了包含大量simple-QA样本的数据集（SimpleQuestions）进行实验，研究了<strong>多任务学习</strong>和<strong>迁移学习</strong>在寻找correct evidence过程中的影响。</li>
<li>同时基于<strong>memory networks</strong>框架实现了一个QA模型，并获得了不错的效果。</li>
</ul>
<p>因为该框架下的模型能够具备复杂推理的能力，从而可以继续深挖更难的QA任务。</p>
<blockquote>
<p>FreeBase和Reverb，两个知识图谱，包含很多<script type="math/tex">(s,r,o)</script>格式的三元组（facts）。</p>
</blockquote>
<p>模型概述：</p>
<ul>
<li>预处理KB：进行了三个预处理步骤。（对于两个知识图谱的fact进行了相似的处理）<ul>
<li>1.group。为了回答有很多答案的问题，把具有相同头实体和关系的facts聚合为一个新的fact。</li>
<li>2.remove intermediate node。将复杂关系化简为一个三元组（fact），使更多QA问题变为Simple QA问题。</li>
<li>3.vectorize。把预处理之后形如<script type="math/tex">(s,r,(o_1,o_2,...,o_k))</script>的三元组向量化为一个高维向量，维度是所有实体数量和关系数量之和，并给予一个初始值（类似n-hot）。</li>
<li>4.vectorize questions。把所有作为输入的问题向量化。维度是词表的长度加上代表KB中实体的别名的n-grams的长度之和，用n-hot的形式表示问题向量。</li>
</ul>
</li>
</ul>
<ul>
<li>输入模块I：进行上面的预处理步骤，从而把FreeBase的fact填充到memory中。或者把问题的vector表示传递给下一个模块。</li>
<li>泛化模块G：把Reverb的facts预处理为向量，扩展memory。（迁移学习）</li>
<li>输出模块O：由输入问题，找到对应的supporting fact，分为两步。<ul>
<li>第一步通过实体链接找出一个待选的facts集合；</li>
<li>第二步通过embedding model找到与question向量最相关的fact并输出。</li>
</ul>
</li>
<li>响应模块R：在本文模型中，直接选择输出模块输出的fact中的尾实体集合即可。</li>
</ul>
<p>总结一下，本文把同一年提出的MemNN应用到QA领域，并取得了不错的效果，证明了MemNN可以应用于NLP的一些任务，而且Memory中即使存储着海量高维向量，也是能够训练的。</p>
<h4 id="2-《Gated-End-to-End-Memory-Networks》"><a href="#2-《Gated-End-to-End-Memory-Networks》" class="headerlink" title="2.《Gated End-to-End Memory Networks》"></a>2.《Gated End-to-End Memory Networks》</h4><p>墨尔本大学和施乐欧洲研究中心2016年10月发表在arXiv上的论文：<a href="https://arxiv.org/pdf/1610.04211。提出了一个在MemN2N基础上改进的模型，并在2个涉及到推理过程的对话数据集上得到了SOTA的结果。" target="_blank" rel="noopener">https://arxiv.org/pdf/1610.04211。提出了一个在MemN2N基础上改进的模型，并在2个涉及到推理过程的对话数据集上得到了SOTA的结果。</a></p>
<p>这里首先我们来回顾一下<strong>MemN2N</strong>的结构，主要是若干层的<strong>supporting memory</strong>，每一层都有<strong>input memory</strong>和<strong>output memory</strong>，负责把上下文编码到记忆中，input memory进一步和question向量计算出分布在memory上的注意力权重，output memory和注意力权重加权求和得到每一层的response向量。通过使用soft attention而不是MemNN中的hard attention，用可微的读取（写入）memory的操作实现端到端的记忆神经网络。最后一层的response向量通过仿射变换和softmax计算出<strong>答案的分布</strong>。</p>
<p><strong>shortcut connections</strong>，能够解决梯度消失问题，让深度神经网络更易训练。其中Highway Network和Residual是两个同时提出的shortcut connections的模型。Highway中将原本前馈神经网络的 <script type="math/tex">y=H(x)</script> 变成 <script type="math/tex">y=H(x)\odot T(x)+x\odot C(x)</script> ，其中T和C是两个门，residual可以认为是Highway的一种特殊形式 <script type="math/tex">y=H(x) + x</script>。</p>
<p><img src="/images/Fgj26dq1GyfZCrC74DcoDBG5-8HO.jpg" alt=""></p>
<p><strong>模型改进</strong>：MemN2N的模型中，第k个hop到第k+1个hop之间的转换关系是，第k个hop的输出（response向量） <script type="math/tex">o_k</script> 加上第k个hop的输入<script type="math/tex">u_k</script>作为第k+1个hop的输入。也就是<script type="math/tex">u_{k+1}=u_k+o_k</script>。这可以认为是一种类似residual的shortcut连接，但是这种连接不具有动态特性。所以本文改进成了<strong>动态门机制</strong>的计算方式(GMemN2N，过程如上图所示)：</p>
<ul>
<li>首先计算出<script type="math/tex">T^k=\sigma(f(u_k))</script>，也就是gate的值。</li>
<li>而后计算k+1层hop的输入，<script type="math/tex">u_{k+1} = u_k\odot T^k + o_k \odot (1-T^k)</script></li>
</ul>
<p>本文用了一种简单的改进方式改进了MemN2N模型，但从实验结果来看提高的比较多。</p>
<h4 id="3-《Reading-Comprehension-Using-Entity-based-Memory-Networks》"><a href="#3-《Reading-Comprehension-Using-Entity-based-Memory-Networks》" class="headerlink" title="3.《Reading Comprehension Using Entity-based Memory Networks》"></a>3.《Reading Comprehension Using Entity-based Memory Networks》</h4><p>日本的NTT通讯科学实验室和东京大学于2016年12月发表在arXiv上的论文：<a href="https://arxiv.org/pdf/1612.03551。提出了一种基于实体的记忆神经网络用于QA任务，该模型能够更好的捕捉**细粒度信息**和实体间的**细微关系**。" target="_blank" rel="noopener">https://arxiv.org/pdf/1612.03551。提出了一种基于实体的记忆神经网络用于QA任务，该模型能够更好的捕捉**细粒度信息**和实体间的**细微关系**。</a></p>
<p>之前用于NLP任务（尤其是QA任务）的记忆神经网络模型的问题在于，memory中存储的是句子向量，这样那些低于句子级别的文本单元之间的关系就会被忽略。本文的模型选择只存储entity，避免引入冗余信息和噪声。</p>
<p>对于<script type="math/tex">\{context_1,...,context_n\}, question \rightarrow answer</script>形式的QA任务，模型具体的处理过程如下（参考下图）：</p>
<ul>
<li>首先，对于<strong>上下文</strong>句子<script type="math/tex">context_i</script>，用自编码器（或其他模型）计算出句子的向量表示<script type="math/tex">\overrightarrow{context_i}</script>。而后抽取其中的所有实体<script type="math/tex">e_1,...,e_j</script>并用预训练的词向量初始化每个实体的<strong>状态向量</strong>。而后不断的<strong>更新状态向量</strong>直到使用所有实体的状态向量能够<strong>重建</strong>句子的向量表示为止，把这些实体的状态向量存储到memory中。</li>
<li>而后，将<strong>问题</strong>的向量表示<script type="math/tex">\overrightarrow{question}</script>计算出来，并找出memory中与<strong>问题相关</strong>的所有实体状态向量。用这些状态向量通过一个神经网络计算出<strong>特征向量</strong>，并以此为基础预测问题的答案。</li>
</ul>
<p><img src="/images/Fk4EkpYxI8SuvWrNOcEFqDAkQcSa.jpg" alt=""></p>
<p>模型采用监督式学习训练，而且在数据集上需要预先标注句子中的实体，这里实验中把所有的名词和代词作为实体。在bAbi，MCTest数据集上进行了实验。</p>
<h4 id="4-《Two-Stage-Synthesis-Networks-for-Transfer-Learning-in-Machine-Comprehension-》"><a href="#4-《Two-Stage-Synthesis-Networks-for-Transfer-Learning-in-Machine-Comprehension-》" class="headerlink" title="4.《Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension 》"></a>4.《Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension 》</h4><p>斯坦福、微软研究院等机构的人员发表在EMNLP2017上的论文：<a href="https://arxiv.org/pdf/1706.09789。文章提出了SynNet模型，进行关于QA问题上的迁移学习。" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.09789。文章提出了SynNet模型，进行关于QA问题上的迁移学习。</a></p>
<p>通过生成（问题，答案）对的方式，将在一个QA数据集（SQuAD）上预训练的模型在另一个无标记数据集（NewsQA）上微调训练，实现效果较好的迁移学习。</p>
<p>在源域（source domain）上的数据集训练时，样本以(P,Q,A)即段落、问题、答案三元组的形式给出，其中<em>答案是段落中的一个连续片段</em>。</p>
<p>在目标域（target domain）上的数据集训练时，样本以P即<strong>无标记段落</strong>的形式给出。</p>
<p><img src="/images/FuMnCnW4GLo9Tp3WNhsI6KvdP72p.jpg" alt=""></p>
<p>模型SynNet（如上图）主要负责给定目标域数据集上的段落P，生成问题Q和答案A，分为两个步骤：</p>
<ul>
<li>答案生成：利用一个双向LSTM网络，对段落中的单词（embedding的形式）进行<strong>标记</strong>，所有标记为非None的<strong>连续单词序列</strong>作为候选答案，输入问题生成模块。</li>
<li>问题生成：采用一个<strong>encoder-decoder</strong>结构的网络，最大化联合条件概率<script type="math/tex">\prod_{i} P(q_i|P,A,q_{1..i-1})</script>：<ul>
<li>encoder：输入P的word embedding向量（增加一维表示答案的0/1特征），经过双向LSTM处理得到隐状态h。</li>
<li>decoder：由h和前一时刻输出的问题单词<script type="math/tex">q_{i-1}</script>得到注意力向量<script type="math/tex">r_i</script>。采用两个预测器，一个直接产生预定义词表中的词，另一个由一个pointer network从段落P中复制一个词。</li>
<li>loss：由于产生的问题单词序列Q没有标签，所以借鉴论文<em>《Latent predictor networks for code generation. 》</em>中的无监督学习算法最小化交叉熵损失。</li>
</ul>
</li>
<li>机器阅读理解模型：直接使用了开源的<a href="https://github.com/allenai/bi-att-flow" target="_blank" rel="noopener">BiDAF</a>，但其他模型也可以。</li>
<li>训练：<ul>
<li>在源域（source domain）s上的数据集训练时，样本以(P,Q,A)即段落、问题、答案三元组的形式给出，其中<em>答案是段落中的一个连续片段</em>。</li>
<li>在目标域（target domain）t上的数据集训练时，样本以P即<strong>无标记段落</strong>的形式给出。</li>
<li>1.在s上训练好MC模型，在t上训练好SynNet。</li>
<li>2.<strong>微调</strong>MC模型，使用了一个叫做“<strong>data-regularization</strong>”的trick。每输入s上的k个minibatch，输入t上生成的1个minibatch（实验中k取4），避免生成的数据噪音过多影响模型效果。</li>
<li>3.最后在测试时，由多个训练时保存的模型ensemble，增加模型稳定性。</li>
</ul>
</li>
</ul>
<p>额，这篇论文和memory没什么关系，提出了一种迁移学习的方法并应用在QA数据集上，论文的实验和分析部分非常详尽。</p>
<h4 id="5-《EviNets-Neural-Networks-for-Combining-Evidence-Signals-for-Factoid-Question-Answering-》"><a href="#5-《EviNets-Neural-Networks-for-Combining-Evidence-Signals-for-Factoid-Question-Answering-》" class="headerlink" title="5.《EviNets: Neural Networks for Combining Evidence Signals for Factoid Question Answering 》"></a>5.《EviNets: Neural Networks for Combining Evidence Signals for Factoid Question Answering 》</h4><p>埃默里大学发表在ACL2017上的一篇论文：<a href="http://aclweb.org/anthology/P17-2047。针对QA模型的答案预测模块进行研究，利用并结合多个来源（比如结构化的知识图谱和非结构化的文本）的support" target="_blank" rel="noopener">http://aclweb.org/anthology/P17-2047。针对QA模型的答案预测模块进行研究，利用并结合多个来源（比如结构化的知识图谱和非结构化的文本）的support</a> evidence给出答案。</p>
<p>针对factoid QA问题，即1中所说的Simple QA问题，基于知识图谱的QA模型能够解决，但是有时知识图谱不够完善，需要其他信息的配合。</p>
<p><img src="/images/FriXraOWxVSKN9yfT09nmin5oZXW.jpg" alt=""></p>
<p>模型结构如上图，对于一个<strong>问题</strong>，使用搜索系统获取与问题相关的<strong>问题相关文本</strong>，实体链接的方式从相关文本获取若干<strong>候选答案实体</strong>，此外再通过知识图谱获取相关实体的<strong>三元组</strong>和<strong>实体类型信息</strong>，这些信息作为模型的输入。其中<strong>supporting evidence</strong>是所有问题相关的文本或者三元组等形式的信息。模型主要过程如下：</p>
<ul>
<li>Evidence Matching：评估<strong>每个supporting evidence和问题的相关程度</strong>（采用对两者的向量表示进行点乘的方法），然后进行softmax归一化。</li>
<li>Evidence Aggregation：聚合权重，对每个候选答案a，其得分取决于：与其相关的supporting evidence中该候选答案的<strong>被提及程度</strong>、次数以及该候选答案<strong>与问题q的相关度</strong>。</li>
<li>Answer Scoring：将聚合起来的每个候选答案的得分输入<strong>全连接网络</strong>计算最终的输出。</li>
</ul>
<p>额，这篇文章和Memory并没有太多的关联。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>机器阅读理解</tag>
        <tag>attention</tag>
        <tag>QA</tag>
        <tag>memory</tag>
        <tag>迁移学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Survey on GNN</title>
    <url>/2019/08/09/deeplearning/Survey%20on%20GNN/</url>
    <content><![CDATA[<h3 id="Survey-on-GNN"><a href="#Survey-on-GNN" class="headerlink" title="Survey on GNN"></a>Survey on GNN</h3><h2 id="一、GNN相关研究Survey"><a href="#一、GNN相关研究Survey" class="headerlink" title="一、GNN相关研究Survey"></a>一、GNN相关研究Survey</h2><h3 id="0-Preliminary"><a href="#0-Preliminary" class="headerlink" title="0.Preliminary"></a>0.Preliminary</h3><p>关于图的一些概念和符号：</p>
<p>图的表示：节点集合$\mathcal{V}$，边集合$\mathcal\epsilon$。</p>
<p>邻接矩阵A，$a_{ij}$表示第i个节点到第j个节点的边的权重。</p>
<p>对角度矩阵（度矩阵）D，$d_{ii}$表示第i个节点的度。</p>
<p>拉普拉斯矩阵L = D - A，D是对角度矩阵。A是邻接矩阵。归一化的拉普拉斯矩阵是图的鲁棒的数学表示。</p>
<p>转移矩阵$P = D^{-1}A$</p>
<p>K近邻：$N_k(i)$，邻居节点$N_1(i) = N(i)$</p>
<p>关于GNN的基础概念和符号：</p>
<p>$F^V, F^E$，节点和边的特征值</p>
<p>$h^t_v$，第t层GNN中节点v的隐藏状态（或称属性）</p>
<p>$e^t_{ij}$，第t层中ij节点连接的边的隐藏状态（或称属性）</p>
<h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>主要参考：THU的《Deep Learning on Graphs: A Survey》《Graph Neural Networks: A Review of Methods and Applications》和Philip Yu组的《A Comprehensive Survey on Graph Neural Networks》这三篇最近的综述。中间穿插一些其他经典或最新论文的研究内容。</p>
<p>GNN是什么？把基于神经网络的模型使用在图领域（输入是图，在图结构上的推理，生成图结构的数据等）的相关方法。</p>
<p>为什么要用GNN？</p>
<ul>
<li>常规的图像声音视频等都是将数据转换到欧几里得空间（欧几里得结构数据），而有些数据（社交网络、知识图谱）无法进行欧几里得结构化。</li>
<li><p>如果输入是图，那么CNN/RNN需要把节点的属性以一定的顺序stack起来处理，但是图的节点本质上是无序的。</p>
</li>
<li><p>其次，边在传统NN的处理中只是被当做节点的属性，而图结构中它其实蕴含更多信息（如依赖信息等）。</p>
</li>
<li><p>最后，高阶AI需要像人类一样从日常经验中利用推理图做决定或者判断（作者认为，无引用），这方面GNN才能做到（从无结构化数据中生成图）。</p>
</li>
</ul>
<p>GNN分类，根据处理的任务类型：</p>
<ul>
<li>Node-focus：节点分类、边预测、节点推荐等，</li>
<li>Graph-focus：图的分类（图的同构问题）、预测图的属性、图生成等。</li>
</ul>
<p>GNN分类，根据</p>
<p>GNN分类：</p>
<ul>
<li>半监督：GNN，GCN，利用了图中节点的属性和标记来训练网络的参数。</li>
<li>无监督：GAE，自编码器具有图结构</li>
<li>最近的进展，GRNN，GRL（也可以归类为半监督，但是相关的方法还较少）</li>
</ul>
<blockquote>
<p>第二部分主要是一些GNN发展过程中比较重要的论文的阅读笔记。</p>
</blockquote>
<h3 id="2-GNN"><a href="#2-GNN" class="headerlink" title="2.GNN"></a>2.GNN</h3><p>以最初的GNN为例，来说明图神经网络是如何建模并学习的。</p>
<p>输入与输出：图中的每一个节点具有一个属性值$F^V$以及一个状态值$s_i$，每条边也具有一个属性值$F^E$。节点和边的属性值或者属性向量是输入的特征，我们希望求得每个节点的标签$\hat y_i$，状态值$s_i$是我们求解的中间结果。就像下面的公式所表示的。我们实际上要学习的是映射$\mathcal F和\mathcal O$。对于Graph-focus的任务，增加一个特殊节点表示整个图，一样的可以得到这个节点的标签。</p>
<script type="math/tex; mode=display">
s_i = \sum_{j\in \mathcal N(i)} \mathcal F(s_i, s_j, F^V_i, F^V_j, F^E_{ij}) \\
\hat y_i = \mathcal O(s_i, F^V_i)</script><p>目标函数：所有节点的标签和ground-truth的某种差值之和。</p>
<p>优化过程：首先利用jacobbi方法（把方程重写，并构建一个初始值，一直迭代求新的值直到满足一定条件）迭代求解状态值，直到到达某个不动点；而后利用当前状态值求解损失并反向传播更新参数；重复此步骤。</p>
<p>不足：求解状态值的步骤过于耗时；要求$\mathcal F(\cdot)$是一个压缩映射影响了表达能力；而且求解节点状态值到一个不动点不利用表达节点的信息，因为得到的解会过于smooth（各个节点得到的信息区分度不足，收敛于一个很接近的状态值）。</p>
<h3 id="2-1-GCN"><a href="#2-1-GCN" class="headerlink" title="2.1 GCN"></a>2.1 GCN</h3><p>来自文章《Semi-Supervised Classification with Graph Convolutional Networks》（Max Welling组，ICLR’2017）</p>
<p>方法：</p>
<p>从谱图卷积说起，给定输入的x谱图卷积操作是：$g<em>\theta \star x = Ug</em>\theta(\wedge) U^T x$，其中U是拉普拉斯矩阵分解得到的特征向量矩阵$\wedge$是特征值矩阵，这个操作需要分解整张图的L矩阵，效率太低，因此经过了各种近似，最终这篇论文使用了如下的方式计算（下面的H就是X也就是输入向量）：</p>
<p><img src="/images/1566630959989.png" alt="1566630959989"></p>
<p><img src="/images/1566630988382.png" alt="1566630988382"></p>
<p>这个公式做了这样一个操作：<strong>每个节点拿到邻居节点信息然后聚合到自身embedding上</strong>。在上面的公式中D−12~AD−12D−12A~D−12可以看成是归一化后的邻接矩阵，$H^{(l)}W^{(l)}$相当于给l层所有节点的embedding做了一次线性变换，左乘邻接矩阵表示对于每个节点来说，该节点的特征变为邻居节点特征相加后的结果。</p>
<p>最终，在N层网络之后，每个节点表示向量进行仿射变换之后softmax，就可以进行节点分类任务，并通过误差反向传播更新参数矩阵。</p>
<p>实验：在Citeseer和另外两个文献引用数据集，以及NELL这个小型知识图谱上面进行了节点分类任务。（大约3K-65K个节点，4K-266K条边的规模）</p>
<h3 id="2-2-GraphSAGE"><a href="#2-2-GraphSAGE" class="headerlink" title="2.2 GraphSAGE"></a>2.2 GraphSAGE</h3><p>来自文章《Inductive Representation Learning on Large Graphs》（Leskovec组，Stanford，NIPS’2017）</p>
<p><img src="/images/1566632463526.png" alt="1566632463526"></p>
<p>已有方法的问题：1.慢；2.只涉及到了直推式学习（transductive learning）的setting，因为图数据中的每一个节点可以通过边的关系利用其他节点的信息，这样就产生了一个问题，如果训练集上的节点通过边关联到了预测集或者验证集的节点，那么在训练的时候能否用它们的信息呢? 如果训练时用到了测试集或验证集样本的信息(或者说，测试集和验证集在训练的时候是可见的), 我们把这种学习方式叫做transductive learning, 反之，称为inductive learning. 显然，我们所处理的大多数机器学习问题都是inductive learning, 因为我们刻意的将样本集分为训练/验证/测试，并且训练的时候只用训练样本。然而，在GCN中，训练节点收集邻居信息的时候，用到了测试或者验证样本，所以它是transductive的。</p>
<p>方法：SAGE的意思就是采样并聚合（sample and aggregate），说明了这个方法主要侧重于控制更新节点表示时涉及的邻域数量。首先对邻居采样（有放回的重采样方法直到采样数量足够），而后进行K次聚合，由K-hop的邻居开始到1-hop的邻居依次根据K个聚合网络和聚合参数更新自己的参数，最终目标样本再更新。实验中效果最好的平均聚合器(mean aggregator)，平均聚合器的思路很简单，每个维度取对邻居embedding相应维度的均值。</p>
<p>参数学习:监督学习直接用节点样本进行交叉熵反向传播即可。非监督学习时，论文设计了一个优化目标函数，就是最小化广义邻居节点（定长随机游走能够到达的节点）的embedding，并通过负采样保证非邻居节点的embedding相似度较小。</p>
<p>实验:在一个论文引用数据集上进行论文分类实验，在Reddit数据集上进行帖子主题分类实验（都没有使用内容数据），此外还划分了训练测试集来测试inductive setting下的效果。</p>
<p>主要创新点：利用采样机制克服GCN的缺点，能够大规模应用。inductive框架，训练时只保留训练样本之间的边。</p>
<h3 id="2-3-GAT"><a href="#2-3-GAT" class="headerlink" title="2.3 GAT"></a>2.3 GAT</h3><p>来自文章《GRAPH ATTENTION NETWORKS》（Bengio组，ICLR’2018）</p>
<p>已有方法的不足：</p>
<p>图结构数据常常含有噪声，意味着节点与节点之间的边有时不是那么可靠，邻居的相对重要性也有差异，解决这个问题的方式是在图算法中引入“注意力”机制(attention mechanism), 通过计算当前节点与邻居的“注意力系数”(attention coefficient), 在聚合邻居embedding的时候进行加权，使得图神经网络能够更加关注重要的节点，以减少边噪声带来的影响。</p>
<p>提出的方法：</p>
<p>图注意力机制就给定节点$V_i$，以及它的邻居节点集合$\mathcal N(i)$，将节点集合中的每个节点映射到[0, 1]值域的一个函数，该函数表示邻居节点的相对重要程度。</p>
<p>如下公式所示，节点0和邻居j的注意力权重是由一个参数化函数计算得到的，其中a和W是参数向量/矩阵。</p>
<p><img src="/images/1566659721923.png" alt="1566659721923"></p>
<p>此外，本工作还讨论了多头注意力机制（multi-head attention），该机制与Transformer中类似，不再赘述。</p>
<p>实验：</p>
<p>在Cora，Citeseer，Pubmed这三个Transductive的论文引用数据集（即不区分训练和验证集，节点数量2K-19K，边5K-44K），以及PPI这个包含24张图的Inductive数据集（蛋白质相互作用的数据集）上进行实验。并与GCN，GraphSAGE等SOTA的方法进行了详细对比，实验很充分。</p>
<p>创新点：针对图中包含噪音以及不同邻居节点重要程度不同这一事实，把注意力机制引入到GNN中。</p>
<p>相似工作：Kumparampil等人2018年提出直接利用余弦函数计算节点注意力权重。Lee等人2018年提出一种注意力引导的游走法，每个时间步游走到一个新的邻居节点，通过RNN来编码，并由当前步的隐状态确定下一步游走到哪个节点（和本工作的抽样法相比有所不同）。</p>
<h3 id="2-4-其他一些有意思的工作（略读）"><a href="#2-4-其他一些有意思的工作（略读）" class="headerlink" title="2.4 其他一些有意思的工作（略读）"></a>2.4 其他一些有意思的工作（略读）</h3><p>《Compositional Fairness Constraints for Graph Embeddings》（William Hamilton组，ICML’19）</p>
<p>面向推荐系统这个任务，为了实现公平约束（Fairness Constraints，即推荐系统学习到的表示不应该与某些属性相关，年龄或者性别），在GNN的基础上引入了一个对抗框架。</p>
<p>《Position-aware Graph Neural Networks》（Leskovec组，Stanford，ICML’19）</p>
<p>GraphSAGE的组的工作，希望改进现有GNN无法获取给定节点在整个图中的相对位置这一不足。首先确定图中一系列的锚节点集，然后对锚节点集进行采样，计算给定目标节点到每个锚集的距离，然后学习锚集上的非线性距离加权聚集方案。</p>
<p>《Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks》（Faloutsos组，KDD’19）</p>
<p>聚焦估计知识图谱(KG)中节点的重要性这个任务，该任务对商品推荐和资源分配等多种下游应用十分重要。已有的方法没有充分利用KG中可用的信息，或者缺乏建模实体与其重要性之间复杂关系所需的灵活性。本文提出了一种有监督学习的GNN-based框架，通过predicate-aware注意力机制和灵活的中心性调整来执行重要性分数的聚合，而不是简单的聚合节点表示。</p>
<p>《Graph Contextualized Self-Attention Network for Session-based Recommendation》（xiaofang zhou组，昆士兰，IJCAL’19）</p>
<p>任务：基于会话的推荐系统。方法：利用GNN和子注意力机制结合。该方法动态地为会话序列构造一个图结构，并通过图神经网络(GNN)捕获丰富的局部依赖关系。然后，每个会话通过应用自注意力机制学习长期依赖关系。最后，每个会话都表示为全局首选项和当前会话兴趣的线性组合。</p>
<blockquote>
<p> 之后这三篇综述朝着不同的方向走了，Wenwu Zhu组的文章主要阐述了不同类别的神经网络方法如何被应用改造到图结构上面。</p>
</blockquote>
<h3 id="3-其他一些NN改造为GNN的变种"><a href="#3-其他一些NN改造为GNN的变种" class="headerlink" title="3.其他一些NN改造为GNN的变种"></a>3.其他一些NN改造为GNN的变种</h3><h4 id="1-GCN卷积图神经网络"><a href="#1-GCN卷积图神经网络" class="headerlink" title="(1) GCN卷积图神经网络"></a>(1) GCN卷积图神经网络</h4><p>a.卷积操作：产生节点的表示</p>
<ol>
<li>谱方法，没大看懂，缺点就是计算量大、且不同图之间无法共享参数；</li>
<li>从效率方面的改进：ChebNet</li>
<li>从可扩展性（多图）方面的改进：Neural FPs， DCNN</li>
<li>框架性的，试图提出一种通用的图网络标准化操作：如MPNN</li>
</ol>
<p>b.Readout操作：产生图的表示</p>
<p>这个操作必须具有顺序不变性。即如果图不变，那么节点的顺序（序号）不应该影响最终的值。</p>
<ol>
<li>统计方法，sum，average和max-pool都行但是不够好，也可以加一个全连接层，但是代价就是无法保证顺序不变性。</li>
<li>层次聚类，比如DiffPool</li>
<li>其他方法，比如接受所有其他节点和边的信息并经过NN处理得到全图的表示（MPNN）</li>
</ol>
<p>c.一些重要的模块与改进</p>
<ol>
<li>注意力机制，注意力权重可以为卷积操作中邻居的权重进行学习（而不是预定义为完全一致）</li>
<li>残差连接，在计算节点表示的时候也一样可以用，在神经网络也可以帮助深层网络收敛。</li>
<li>边的特征，一种选择是根据边的类型训练不同的参数，并把结果汇集起来，但这种方式只能处理离散的边特征。DCNN把边转换为头尾节点上连接的两个特殊节点；另一类方法同时学习边和节点的表示，并通过若干函数进行交互。</li>
</ol>
<p>d.通过采样进行加速</p>
<p>因为图的节点的度通常服从幂律分布（长尾分布）</p>
<ol>
<li>总是采集固定数量的邻居节点用来更新当前节点的表达；或者使用随机游走来选择邻居（PinSage）；</li>
<li>在卷积过程中根据节点的度（正则化之后）作为概率采样节点【FastGCN】；</li>
</ol>
<p>e.归纳学习的setting</p>
<p>即在图的一部分节点/边上进行训练，并在另一部分节点/边上测试。</p>
<p>f.随机权重</p>
<p>没看懂，留着之后有需要再看吧。</p>
<h4 id="2-GAE图自编码器"><a href="#2-GAE图自编码器" class="headerlink" title="(2) GAE图自编码器"></a>(2) GAE图自编码器</h4><p>a.自编码器</p>
<p>GAE源于稀疏自编码器，将邻接矩阵$P(i, :)$或其变种作为节点$i$的raw feature，以每个节点的L2重建误差作为损失，得到节点的低维表示向量后使用K-Means可以完成节点聚类任务。</p>
<p>SDNE进一步完整了SAE的理论，指出L2实际上对应着二阶逼近，因此在目标函数中增加一项代表一阶逼近的$\sum_{i,j=1}^N A(i,j)||h_i - h_j||_2$，这一项和拉普拉斯特征映射的目标函数很像。</p>
<p>DRNE利用LSTM来聚合邻居信息。</p>
<p>Graph2Gauss把每个节点映射为一个高斯分布来捕捉节点的随机性（通过学习节点属性到均值/方差的映射函数。优化的目标是KL散度，也就是如果i和j直接距离更近，那么i和j之间的KL散度也应该更小（分布更接近）。</p>
<p>b.变分自编码器</p>
<p>c.其他的一些研究</p>
<h4 id="3-最近的研究"><a href="#3-最近的研究" class="headerlink" title="(3) 最近的研究"></a>(3) 最近的研究</h4><p>a.GRNN</p>
<p>这里的GRNN指的是在图级别使用RNN，而不是在GNN的节点级别使用RNN。</p>
<p>You等人使用两个RNN来生成节点以及边。</p>
<p>b.GRL</p>
<blockquote>
<p> 而Maosong Sun组的这篇文章主要侧重于讲原始GNN在各方面的改进，以及应用场景：</p>
</blockquote>
<h3 id="4-GNN的变种及应用场景"><a href="#4-GNN的变种及应用场景" class="headerlink" title="4. GNN的变种及应用场景"></a>4. GNN的变种及应用场景</h3><h4 id="1-有向图"><a href="#1-有向图" class="headerlink" title="(1) 有向图"></a>(1) 有向图</h4><p>原始GNN应用在具有节点信息的无向图上，而有向边有时蕴含着更多的信息（比如信息的流动方向），ADGPM面向知识图谱，对有向边的头结点和尾节点分别使用不同的权重矩阵$W_c, W_p$来学习更好的结构化信息。</p>
<h4 id="2-异构图"><a href="#2-异构图" class="headerlink" title="(2)异构图"></a>(2)异构图</h4><p>具有不同节点类型的图被称为异构图，可以简单的把节点类型作为feature加入输入信息中，GraphInception提出了一种metapath的思路来处理异构图。</p>
<h4 id="3-具有边信息的图"><a href="#3-具有边信息的图" class="headerlink" title="(3)具有边信息的图"></a>(3)具有边信息的图</h4><p>比如知识图谱中的边，表征不同的关系。</p>
<p>1.可以通过把图转化为二分图，边变成一个特殊节点，和原本连接的头节点/尾节点之间各有一条边来处理，并通过不同的参数矩阵来区分边的类型。</p>
<h4 id="4-Propagation-Type"><a href="#4-Propagation-Type" class="headerlink" title="(4) Propagation Type"></a>(4) Propagation Type</h4><p>分析各种Aggregator和Updater的变种，Aggregator决定从当前节点的哪些相关节点如何使用哪些信息，Updator决定这些信息如何被应用于更新当前节点的hidden state。典型的这两个操作函数公式如下所示（来自GraphSAGE），其中第一个函数可以是对固定数量节点的Mean，Pooling或者LSTM操作：</p>
<script type="math/tex; mode=display">
h^t_{N_v} = Aggregator({h^{t-1}_u, \forall u \in N_v}) \\
h^t_v = \sigma(W^t\cdot[h_v^{t-1}||h^t_{N_v}])</script><p>卷积：一系列的谱方法与非谱方法，谱方法基于图的谱表示而非谱方法直接在图上定义卷积操作。</p>
<p>门限：LSTM和GRU的使用。比如在gather了邻居节点的信息后，使用GRU作为节点状态更新的单元。</p>
<p>Attention：GraphAttentionNetwork使用类似self-attention的机制，在propagation中attend over 邻居的信息，并借鉴了multi-head-attention。比如下面的计算方式，其中$\alpha$是正则化后的注意力权重：（这个式子就是GAT的aggregator，它的update函数不改变节点表示）</p>
<script type="math/tex; mode=display">
h_i' = ||_{k=1}^K \sigma(\sum_{j\in N_i}\alpha^k_{ij}W^kh_j)</script><p>Skip-Connection：为了能够使多层GCN容易收敛的trick。</p>
<h4 id="（5）训练方式的改进"><a href="#（5）训练方式的改进" class="headerlink" title="（5）训练方式的改进"></a>（5）训练方式的改进</h4><p>用原始的GCN举例，它在训练上有这样一些不足：1.需要整个图的拉普拉斯矩阵；2.第N层的节点i的表示是由第N-1层其所有的邻居节点的表示更新得到的，因为每一层邻居可能有很多，所以存在感受野爆炸问题，梯度反向传播会很慢；3.在一个固定的图上训练，泛化性能不足。</p>
<p>对训练方式的改进主要有：</p>
<p>以可学习的聚合函数代替原本整个图的拉普拉斯矩阵；</p>
<p>使用各种采样的方式减少聚合时涉及到的邻居节点数量，从而减少感受野爆炸的问题</p>
<h4 id="（6）框架性的模型尝试"><a href="#（6）框架性的模型尝试" class="headerlink" title="（6）框架性的模型尝试"></a>（6）框架性的模型尝试</h4><p>一些研究者试图提出一种更加General的框架，把多种GNN的模型操作包含在内。有以下这样一些工作：</p>
<p>MPNN（Message Passing NN，2017）：尝试包含GCN等在内的监督学习的GNN模型，包含message passing过程和read out 过程（对应综述中的两个过程）。此外利用最后一层所有节点的表示，来计算整个图的表示。M、U和R可以有不同设置，其中M可以是邻接矩阵和w节点表示的乘积，U可以是GRU，R可以是多层DNN。</p>
<script type="math/tex; mode=display">
m^{t+1}_v = \sum_{w\in N_v} M_t(h^t_v, h^t_w, e_{vw}) \\
h^{t+1}_v = U_t(h^t_v, m^{t+1}_v) \\
\hat y = R({h^T_v|v\in G})</script><p>NLNN（Non-local NN，2017）：该框架能够在图结构上应用多种self-attention操作，f可以是点积、高斯、concat等操作，g可以是简单的仿射变换：</p>
<script type="math/tex; mode=display">
h_i =  \frac{1}{C(h)} \sum_{\forall j} f(h_i, h_j) g(h_j)</script><p>GN（Graph Network，2018）：尝试提出一种包含MPNN和NLNN等方法的通用框架，利用(u,H,E)三元组定义图分别表示全局属性、点集合和边集合。每一个GN Block包含三个更新和三个聚合函数（左边和右边）其中右边的函数接受不定量参数并且参数顺序不影响结果；按顺序进行如下操作：更新边的表示、根据边的表示更新计算与节点i相关的聚合表示$\bar {e’_i}$、更新节点的表示、根据边的表示更新计算全局聚合表示$\bar {e’}$，根据节点的表示更新计算全局聚合表示$\bar {h’}$，更新全局属性$u’$。</p>
<p><img src="/images/1566612142166.png" alt="1566612142166"></p>
<h4 id="（7）应用场景"><a href="#（7）应用场景" class="headerlink" title="（7）应用场景"></a>（7）应用场景</h4><p>交通信息、社交网络、推荐系统以及物理化学领域。下面稍微展开几个领域应用：</p>
<p>知识图谱：利用GNN解决知识图谱补全中的未见实体问题（Out of KB Entity Problem），利用GCN解决多语言知识图谱对齐问题。</p>
<p>NLP中的分类问题：一般是将一句话或者一个段落看做是一张图（节点是一个word），然后用GNN来得到更好的总体编码。</p>
<p>序列标注：引入GNN的方式和分类问题类似，但是得到的顶层node表示可以用来预测标注信息。</p>
<p>文本生成：引入语义或者语法结构，比如在翻译任务中。</p>
<h4 id="8-关于GNN未来的改进"><a href="#8-关于GNN未来的改进" class="headerlink" title="(8) 关于GNN未来的改进"></a>(8) 关于GNN未来的改进</h4><p>现有的GNN无法应用深层神经网络，否则节点的表示会over-smoothing，即收敛到高度相似的值。</p>
<p>GNN如何应用在动态的图结构中而建模效果不会下降太多。</p>
<p>如何应用到大规模数据上。</p>
<h4 id="（9）数据"><a href="#（9）数据" class="headerlink" title="（9）数据"></a>（9）数据</h4><p>实验：在主要研究GNN本身的工作中，基本上CiteSeer、Cora这几个学术论文引用的数据集，以及PPI、QM7b这种生物领域的数据集（分子或者蛋白质）已经成为事实上GNN效果判断的Benchmark。</p>
<h2 id="二、GNN在NLP领域近期研究"><a href="#二、GNN在NLP领域近期研究" class="headerlink" title="二、GNN在NLP领域近期研究"></a>二、GNN在NLP领域近期研究</h2><h3 id="0-GNN-in-NLP-Recent-Studies"><a href="#0-GNN-in-NLP-Recent-Studies" class="headerlink" title="0.GNN in NLP, Recent Studies"></a>0.GNN in NLP, Recent Studies</h3><p>论文来自于ACL’19，论文列表中与GNN相关的有20多篇，选择其中几篇调研。</p>
<h3 id="0-1-《Attention-Guided-Graph-Convolutional-Networks-for-Relation-Extraction》"><a href="#0-1-《Attention-Guided-Graph-Convolutional-Networks-for-Relation-Extraction》" class="headerlink" title="0.1 《Attention Guided Graph Convolutional Networks for Relation Extraction》"></a>0.1 《Attention Guided Graph Convolutional Networks for Relation Extraction》</h3><p>来自Wei Lu组，新加坡科技大学。</p>
<p>主要思路：Dependency trees传递丰富的结构信息，这些信息对于提取文本中实体之间的关系非常有用。然而，如何有效利用相关信息而忽略Dependency trees中的无关信息仍然是一个具有挑战性的研究问题。现有的方法大多使用基于规则的hard-pruning策略来选择相关的部分依赖结构，这个工作提出了一种直接以依赖树为输入的Attention Guided图卷积网络(AGGCNs)模型，期望该模型自动学习如何有选择地关注对关系提取任务有用的相关子结构。</p>
<p>模型框架：输入是依赖树（作为图处理），根据原有的邻接矩阵A，由多头注意力计算出N个代表不同权重的图，而后进入N个全连接层产生N个表示（可以认为代表依赖树的不同特征的N个图），最后由线性组合层产生最终的1个隐藏表示。</p>
<p><img src="/images/1566697021918.png" alt="1566697021918"></p>
<p>实验：在多句和单句关系抽取这两个任务上测试模型效果（TACRED &amp; SemEval），和包括GCN、GraphLSTM在内的方法进行了比较。</p>
<h3 id="0-2-《Cognitive-Graph-for-Multi-Hop-Reading-Comprehension-at-Scale》"><a href="#0-2-《Cognitive-Graph-for-Multi-Hop-Reading-Comprehension-at-Scale》" class="headerlink" title="0.2 《Cognitive Graph for Multi-Hop Reading Comprehension at Scale》"></a>0.2 《Cognitive Graph for Multi-Hop Reading Comprehension at Scale》</h3><p>Jie Tang组，清华&amp;阿里巴巴的工作。</p>
<p>思路：从认知科学中的对偶过程理论得到启发，通过一个隐式的抽取模块和一个显式的推理模块逐步迭代构建认知图。</p>
<p>首先解释一下对偶过程理论，就是说认知神经学的研究认为，人的大脑首先通过注意力寻找相关的信息（这是一个无意识、自发的、直观的过程）System1，而后通过一个可控制的、显示的、有意识的推理过程来找到答案（System2）。System1很快而System2工作时间更慢，需要在working memory中对信息进行顺序的解读和思考。</p>
<p>任务：Multi-Hop QA，其过程如下所示：其中涉及到的具有多种含义的实体，可能在相关的上下文文档中包含所需要的信息（supporting fact）。</p>
<p><img src="/images/1566702171998.png" alt="1566702171998"></p>
<p>方法：System 1负责产生问题+clues的表示，其中clues[x, G]就是认知图中x的前序节点的段落中的某些句子（推理所需要的句子）。同时系统1抽取上下文段落中与问题相关的实体和答案候选，实体以及相应的语义信息被用来组织为认知图（模拟working memory），System2用来对working memory进行推理，实现中就是GNN的节点表示的更新。持续推理过程直到某个答案候选概率超过阈值或认知图超过一定规模。</p>
<p><img src="/images/1566701958352.png" alt="1566701958352"></p>
<p>优势：可解释，答案的推理路径可知。</p>
<p>实验：在HotpotQA数据集上进行了实验，效果比现有方法好非常多。</p>
<h3 id="0-3-《Coherent-Comment-Generation-for-Chinese-Articles-with-a-Graph-to-Sequence-Model》"><a href="#0-3-《Coherent-Comment-Generation-for-Chinese-Articles-with-a-Graph-to-Sequence-Model》" class="headerlink" title="0.3 《Coherent Comment Generation for Chinese Articles with a Graph-to-Sequence Model》"></a>0.3 《Coherent Comment Generation for Chinese Articles with a Graph-to-Sequence Model》</h3><p>北大和腾讯的工作。</p>
<p>任务：自动文章评论，给定一个文章，自动生成一些与文章主题相关的评论内容。</p>
<p>以往方法的不足：对于现有的基于encoder-decoder的模型来说，新闻文档通常太长，这往往会导致一般性和不相关的评论。希望能够生成足够相关并且足够diverse的评论。</p>
<p>思路：总体上是一个Graph-to-Sequence框架，在encoder端首先构建一个文章图，首先进行命名实体识别和关键词提取并把提取后的词作为图的节点，而后判断文章每句话中是否包含关键词，如果包含就把这句话加入到这个词所在的节点，如果不同节点之间包含的句子有相同的，那么就增加一条无向边，边的权重由两个节点内容的相似度得到。而后通过GCN来编码节点表示并得到整个图的表示。</p>
<p><img src="/images/1566712761979.png" alt="1566712761979"></p>
<p>实验：在一个中文数据集腾讯快报上面进行了评论生成实验，并通过人工评价流畅度信息丰富程度来和其他方法比较。</p>
<p>创新点：以一种比较新颖的方式构建了图。 </p>
<h3 id="0-4-《Multi-hop-Reading-Comprehension-across-Multiple-Documents-by-Reasoning-over-Heterogeneous-Graphs》"><a href="#0-4-《Multi-hop-Reading-Comprehension-across-Multiple-Documents-by-Reasoning-over-Heterogeneous-Graphs》" class="headerlink" title="0.4 《Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs》"></a>0.4 《Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs》</h3><p>Xiaodong He&amp;Bowen Zhou组，京东的工作。</p>
<p>针对的任务也是需要multi-hop的RC。该任务给定1个问题，多个相关文档以及多个候选答案，需要从多个文档中进行多次推理才能找到正确答案。</p>
<p>首先，{C}、Q、{S}作为输入，经过GRU对三者分别编码，并抽取文档中的命名实体。而后构建HDE图，图的节点包括所有文档、候选以及命名实体；边的集合由人工定义的7种规则来构建；节点表示由基于co-attention 和 self-attention的上下文编码器初始化。</p>
<p><img src="/images/1566713648318.png" alt="1566713648318"></p>
<p>创新点：建立的图是异构的，由不同粒度的节点组成，包括候选答案、文档以及实体，同时也包含不同类型的边；这样能够充分利用不同粒度的信息进行推理。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Survey</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>两篇命名实体识别的经典模型[转]</title>
    <url>/2017/07/02/forward/%E4%B8%A4%E7%AF%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E7%9A%84%E6%97%A9%E6%9C%9F%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>转自<a href="lanzhuzhu.github.io">博客</a>.</p>
<h1 id="End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF"><a href="#End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF" class="headerlink" title="End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"></a>End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</h1><h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><p>Xuezhe Ma and Eduard Hovy    CMU  ACL2016</p>
<h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>将POS和NER问题转变成一个序列标注问题。用基于神经网络的端到端模型解决序列标注问题。</p>
<h2 id="Pros-and-cons"><a href="#Pros-and-cons" class="headerlink" title="Pros and cons"></a>Pros and cons</h2><p>传统的HMM, CRF的方法，依赖于手工构建的特征，和任务相关的资源。这导致模型很难适应新的任务和领域。近些年有一些非线性神经网络模型用词向量（Word Embedding）作为输入，有前馈神经网络、循环神经网络（RNN）、长短期记忆模型（LSTM）、GRU，取得不错结果。</p>
<p>contributions:</p>
<ol>
<li>a novel neural network architecture for linguistic sequence labeling:</li>
<li>empirical evaluations on benchmark data sets for two classic NLP tasks.</li>
<li>state-of-the-art performance with <strong>truly end-to-end</strong> system.</li>
</ol>
<p>end-to-end的优点：</p>
<ul>
<li><p>no task-specific resources,</p>
</li>
<li><p>no feature engineering,</p>
</li>
<li><p>no data pre-processing beyond pre-trained word embeddings on unlabeled corpora.</p>
</li>
</ul>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods:"></a>Methods:</h2><ol>
<li><p>用 CNN获得Character-level词表示。Figure 1: The convolution neural network for extracting character-level representations of words. Dashed arrows indicate a <strong>dropout layer</strong> applied before character embeddings are input to CNN.</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fh5bifi2x9j30rw0m83ze.jpg" style="zoom:50%"></p>
</li>
<li><p>把步骤一的词表示和事先训练好的词向量拼接起来，输入Bi-directional LSTM，得到每个状态的表示。注意，BLSTM的输入和输出都过了Dropout层（下图未画出）</p>
<p>  ​</p>
</li>
<li><p>用步骤二的输出输入CRF层，最终预测。  </p>
<p>  <img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fh5biq8k3tj30p00zcjss.jpg" style="zoom:50%"></p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ol>
<li><p>Peephole： LSTM 变种，增加了 peephole 到每个门[ I, F, O]上，即，增加细胞状态[<script type="math/tex">C_{t-1}</script>]作为输入。许多论文会加入部分的 peephole。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fhtzwcgx11j31bs0hgjs2.jpg" alt=""></p>
</li>
<li><p>对CRF算法的训练和解码，使用viterbi 算法。参见维基百科。</p>
</li>
</ol>
</li>
</ol>
<h2 id="Model-Parameters"><a href="#Model-Parameters" class="headerlink" title="Model Parameters"></a>Model Parameters</h2><h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><ul>
<li><strong>0.5Dropout Training+Fine Tuning +Early Stopping，</strong></li>
</ul>
<ul>
<li>NER relies more heavily on pretrained embeddings than POS tagging.</li>
</ul>
<ul>
<li>可能由于Word2Vec 的训练用的是Case Sensitive的方式，所以在NER任务上效果就差了。</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fn86kvolrmj30kq0muq41.jpg" style="zoom:50%"></p>
<h1 id="Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs"><a href="#Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs" class="headerlink" title="Named Entity Recognition with Bidirectional LSTM-CNNs"></a>Named Entity Recognition with Bidirectional LSTM-CNNs</h1><h2 id="Source-1"><a href="#Source-1" class="headerlink" title="Source"></a>Source</h2><p>TACL ACL2016,   arXiv 2015</p>
<p>Jason P.C. Chiu University of British Columbia jsonchiu@gmail.com</p>
<p>Eric Nichols   Honda Research Institute Japan Co.,Ltd. e.nichols@jp.honda-ri.com本田车？</p>
<h2 id="Main-idea"><a href="#Main-idea" class="headerlink" title="Main idea"></a>Main idea</h2><p>本文利用一个混合的框架BiLSTM/CNN来自动学习word- 和character-level 特征，而不用特征工程。提出新的方法来编码部分词汇匹配特征到神经网络中。本文系统只使用了分词后的文本，公共word vector, 和开源自动构建的词典，超过之前报告的最好方法。提到其他的竞争系统使用了大量特征工程，特制词汇表，和丰富的实体链接信息。  </p>
<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>从传统的统计模型，是应用模型到手工制作的特征。Collobert et al. (2011b)开始有神经网络的方法，早期方法的简单，前馈网络，没有利用长距离关系和字符级特征。近年LSTM和CNN分别用于NER或POS的例子，缺点是受限于差的word embedding, 没有字符级的CNN用来做英文NER。因此本文提出混合的模型来解决，1. LSTM/CNN+word-character-level 特征。2、 提出词典编码策略和匹配算法，利用部分匹配，并和exact match对比。</p>
<h2 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods:"></a>Methods:</h2><ol>
<li><p>Word embedding and lexicon features: </p>
<p>词典：抽取某一个类别下的命名实体名作为词典，然后对其进行分词，用来部分匹配。对文本中的every n-gram匹配词典中的词entry【直到最长的词】，匹配成功指的是：n-gram的词匹配词典的某个词entry的前缀或后缀，且长度达到词典中entry的一半以上。以及一些条件的修饰。</p>
</li>
<li><p>Character-level Features</p>
<p>将character-embedding和(upper case, lower case, punctuation, other)四种Character Type Feature连接起来，送入CNN。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fi1v29q1mcj30ru0tuac9.jpg" style="zoom:50%"></p>
<p>类似，将输入的词典特征也当做一个向量并入输入向量。不同的是，最后将前向LSTM和后向LSTM的输出分别经过全连接和log-softmax之后，将所得的概率相加，得到最后的概率。【和一般的BiLSTM输出连接之后再接一个softmax不同，哪个效果更好，理论上没有很好的对比解释性？】</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fi1v98gjl1j30ri11iacs.jpg" style="zoom:50%"></p>
<ul>
<li>训练</li>
</ul>
<p>最后的得分是神经网络的分数加上tag转换的值之和。这其实就是CRF的原理，只不过不是完全端到端的模型。</p>
<p>目标函数和梯度可以通过动态编程实现。通过Viterbi算法来找到使分数值最大的序列。</p>
</li>
</ol>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>优点：系统融合了神经网络自动学习和优化特征的优点，且加入了少量人工特征，所以在大的数据集OntoNotes上的实验结果很好。</p>
<p>缺点：构建和应用词典和词向量是值得研究的地方。基于character-level学习的特征有些融合在词典中了，词典使用了部分匹配的方法，可以更灵活的应用已有词典。</p>
<p>拓展模型到相似任务，如extended tagset NER and entity linking.</p>
<h1 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h1><p>Chiu 等人的文章早于前面一篇Xuezhe Ma 的论文，两篇论文的图都很像，后者是利用BiLSTM_CNN来解决解决NER的任务，而Ma的论文在模型的最后加了CRF，泛化用来处理序列标注的类似任务。Chiu 的文章中还是包含了一些外部的知识，如词典特征，字符类型，大小写等； 用partial lexicon matches的方法来编码那些没有出现在预训练的word embedding词表中的词的信息。Chiu的论文算是从传统 基于特征工程，外部知识的方法到端到端的神经网络方法的一个成功尝试，后面的不少论文都在这篇基础上改进。</p>
<p>此外，在此之前还有<br>2015.08.09 arXiv上的Bidirectional LSTM-CRF Models for Sequence Tagging（Zhiheng Huang, Wei Xu, Kai Yu；Baidu research)这篇论文用BiLSTM-CRF的模型处理序列标注的问题，取得了此前的最好结果，论文中还对比了各种模型，如 LSTM ，BI-LSTM，LSTM-CRF，发现还是BiLSTM-CRF在POS 和NER 任务上取得效果最好。但是论文利用了一些外部特征，不算一个端到端的模型。</p>
<p>总之，可以看出，每一次论文的改进都是在前面论文的基础上做了一些微小改进，越来越减少对文本的预处理和对外部知识的依赖。总的来说，从15年Huang的工作到后面，模型的框架变化不大，效果略有提升。</p>
]]></content>
      <categories>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>命名实体识别</tag>
        <tag>序列标注任务</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Git版本控制</title>
    <url>/2015/10/10/git/Git%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/</url>
    <content><![CDATA[<p>博文中并没有关于<code>git</code>原理的介绍，因为仅仅把它当做一个工具来使用，让自己对版本控制的概念和流程有一个了解，有需要时再查阅资料了解进一步使用方法。这篇博文会随着<code>git</code>的使用保持更新。</p>
<h3 id="一、开始使用"><a href="#一、开始使用" class="headerlink" title="一、开始使用"></a>一、开始使用</h3><p>开始使用<code>git</code>作为版本控制工具，所要掌握的必要命令。</p>
<p>1.git本地仓库初始化，dir参数留空则在当前目录创建git仓库。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git init [dir]</span><br><span class="line"><span class="comment"># 示例如下：</span></span><br><span class="line">git init gitLocalRepo</span><br></pre></td></tr></table></figure>
<p>2.接着添加远程仓库，也可以只在本地版本库进行版本控制，不添加远程仓库。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git remote add [RemoteRepoName] [URL]</span><br><span class="line"><span class="comment"># 示例如下：</span></span><br><span class="line">git remote add githubRepo git@github.com:zhanghaoyu1993/zhanghaoyu1993.github.io.git</span><br></pre></td></tr></table></figure>
<p>3.对工程代码进行修改后，使用以下命令添加当前修改的文件到索引(<code>stage</code>)。<code>注意</code>：此命令并未将修改保存到本地或远程版本库中。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git add . <span class="comment"># 索引所有文件变化，在git2.0之后相当于git add --all</span></span><br><span class="line">git add [file1] <span class="comment"># 索引file1的变化（添加file1到版本控制）</span></span><br><span class="line"><span class="comment"># 示例如下：</span></span><br><span class="line">git add *.py</span><br></pre></td></tr></table></figure>
<p>4.提交一个版本至本地仓库，并附加版本描述信息，描述信息应当能够反映此次版本修改的要点。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git commit -m <span class="string">"版本描述信息"</span></span><br></pre></td></tr></table></figure>
<p>5.提交本地版本到远程版本库，该命令会对比本地版本库和远程版本库的不同，并将本地版本库的修改同步至远程版本库。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git push [RemoteRepoName]</span><br><span class="line"><span class="comment"># 示例如下：</span></span><br><span class="line">git push githubRepo</span><br></pre></td></tr></table></figure>
<p>如<code>push</code>出现问题，比如远程版本库有来自其他开发者的push操作导致与本地版本产生矛盾，可使用强制<code>push</code>选项，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git push -f [RemoteRepoName] <span class="comment"># 此命令可能丢失远程版本库中修改</span></span><br></pre></td></tr></table></figure>
<p>6.如果需要将文件从版本控制中删除，使用下面的命令。<code>注意</code>：该命令只是删除索引中的文件，还需要<code>commit</code>和<code>push</code>才能在本地库和远程库中删除相应文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git rm [file]</span><br><span class="line"><span class="comment"># 示例如下：</span></span><br><span class="line">git rm *.exe</span><br></pre></td></tr></table></figure>
<p>7.查看<code>git</code>索引状态，使用<code>git status</code>查看已经索引但是没有提交版本的文件的添加/修改/删除状态。</p>
<p>8.使用pull将远程版本库同步到本地版本库，如果不输入远程版本库URL则使用默认远程版本库。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git pull [RemoteRepoURL]</span><br><span class="line"><span class="comment"># 示例如下：</span></span><br><span class="line">git pull git@github.com:zhanghaoyu1993/zhanghaoyu1993.github.io.git</span><br></pre></td></tr></table></figure>
<p>9.查看版本提交的历史记录。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">log</span></span><br></pre></td></tr></table></figure>
<h3 id="二、分支操作"><a href="#二、分支操作" class="headerlink" title="二、分支操作"></a>二、分支操作</h3><p>Git鼓励大量使用分支，标准做法是在开发新模块时新建一个分支，切换到该分支进行开发，开发模块完成后再合并到<code>master</code>分支中，下面是分支相关操作：</p>
<blockquote>
<p>1.查看分支：<br><code>git branch</code></p>
<p>2.创建分支：<br><code>git branch &lt;name&gt;</code></p>
<p>3.切换分支：<br><code>git checkout &lt;name&gt;</code></p>
<p>4.创建+切换分支：<br><code>git checkout -b &lt;name&gt;</code></p>
<p>5.合并某分支到当前分支：<br><code>git merge &lt;name&gt;</code></p>
<p>6.删除分支：<br><code>git branch -d &lt;name&gt;</code></p>
</blockquote>
<h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><p><strong>1.如何无密码推送到<a href="https://github.com" target="_blank" rel="noopener">github</a>版本库</strong></p>
<p>问题描述：<code>Github</code>版本库命令行推送总是需要输入<code>github</code>的账号和密码，推送的过程比较繁琐。</p>
<p>解决方式：因为使用了HTTPS连接方式的远程仓库地址，必须使用账号密码验证。修改为SSH连接的远程仓库地址，这样就会使用SSH密钥验证。接下来配置Github的SSH密钥即可。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#远程版本仓库地址https://github.com/zhanghaoyu1993/zhanghaoyu1993.github.io.git</span></span><br><span class="line">git remote remove origin</span><br><span class="line">git remote add origin git@github.com:zhanghaoyu1993/zhanghaoyu1993.github.io.git <span class="comment">#SSH仓库地址</span></span><br></pre></td></tr></table></figure>
<p><strong>2.<code>.gitignore</code>文件不起作用</strong></p>
<p>问题描述：在修改了<code>.gitignore</code>文件后，使用<code>git add .</code>命令添加发现本应该被排除的文件或目录还是在<code>git status</code>中。</p>
<p>解决方式：这是由于<code>.gitignore</code>文件只作用于尚未被纳入版本控制的文件，当你使用<code>git add .</code>把某个文件加入了<code>git</code>版本控制时，首先应当使用以下命令来将这些文件移除出版本控制系统，然后对它们的ignore才会生效。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git rm --cached [file]</span><br><span class="line"><span class="comment"># 示例如下：</span></span><br><span class="line">git rm --cached *.pyc</span><br></pre></td></tr></table></figure>
<p><strong>3.忘记Push导致合并分支时出现冲突</strong></p>
<p>由于在实验室机器和自己的笔记本上都可能工作，所以使用git来同步两个环境的工程。问题是这样发生的：周五离开实验室的时候，文件A的最后一次小修改忘记提交到远端仓库中；周末使用笔记本进行工作，对A文件进行了修改并提交到远端仓库；周一回到实验室发现把远端仓库内容fetch到本地后，合并时由于A文件出现冲突无法合并。</p>
<p>解决方式：由于A文件出现在了本地的暂存区里，所以合并失败，把A文件删除就可以了。如果冲突文件很多可以使用<code>git mergetool</code>来合并。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="http://git-scm.com/" target="_blank" rel="noopener">Git-scm</a></li>
<li><a href="http://t.hengwei.me/post/gitignore%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/#问题场景" target="_blank" rel="noopener">楞伽山人博客</a></li>
</ul>
]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Git的原理-1</title>
    <url>/2017/06/05/git/Git%E7%9A%84%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h3 id="一、Git中的Object和Reference"><a href="#一、Git中的Object和Reference" class="headerlink" title="一、Git中的Object和Reference"></a>一、Git中的Object和Reference</h3><p>Git内部有四种object：blob，tree，commit，tag。每一种object都由其SHA-1的值（40bit）作为索引（Key）。</p>
<ul>
<li>blob是文件由zlib压缩之后得到的，每一个文件都是一个blob，如果有两个文件具有相同的内容，那么只会产生一个blob。</li>
<li>tree是目录产生的object，目录下的每个文件和子目录会被tree通过索引引用，如下图，tree同时保存子目录和文件的一些元数据（名称，权限等）。</li>
</ul>
<p><img src="/images/FmE86hz1onSMUSYmRQ4coTK3SGzD.jpg" alt=""></p>
<ul>
<li>commit是一个指向工作目录中root-tree的object，它具有[author，committer，message]这些额外数据，并额外保持一个对parent-commit的引用，这个引用指向前一个commit。</li>
</ul>
<p><img src="/images/FnzVjJ1A-S_1SNZDmlfgKXmPrkhj.jpg" alt=""></p>
<ul>
<li>tag指向一个commit，当然也包含其他一些额外信息。</li>
</ul>
<p>此外，Git中还有Reference类型的数据。object类型的数据一旦存储就不能再改变了（immutable），而你可以对reference类型的数据进行修改、删除的操作。</p>
<ul>
<li>branch：分支存储在.git/refs/heads/目录中，一个branch（和tag一样）指向该分支最后一个commit。如果你切到了这个branch并提交新的commit，那么branch的commit就会更新。而当前的branch是由.git/HEAD确定的。当你建立一个新的branch，Git只是在.git/refs/heads/中新建了一个文件，内容是这个branch指向的commit的SHA-1。同样的，切换分支和删除分支的代价也非常小。</li>
</ul>
<h3 id="二、Git的数据模型"><a href="#二、Git的数据模型" class="headerlink" title="二、Git的数据模型"></a>二、Git的数据模型</h3><p>上面所讲的这些数据之间是怎么交互从而实现版本控制的呢？假设你建立了一个repository，增加了一些文件，执行了一次commit。</p>
<p><img src="/images/Fshr8KugKvo5D-awoabHXkV28c_T.jpg" alt=""></p>
<p>然后你再次增加文件，提交了两次commit，Git存储的对象和引用如下图所示。注意相同内容的blob和tree不会被存储两次。</p>
<p><img src="/images/FmcwXZ-CdcNte6pqYgSjHNpKQtAC.jpg" alt=""></p>
<p>所以，Git使用了16个immutable的object和1个reference，以及一个HEAD完成了三个版本的若干文件的版本控制。你可以通过说，给我当前commit的parent的parent或者是给我the tag指向的commit的parent来checkout到第一个commit的位置。</p>
<p>当你checkout到了某一个tag的时候会发生什么呢？Git会通过你引用的commit，找到commit引用的root-tree，从而遍历这个tree链接到的blob和tree（通过SHA-1索引），这样就找到了你在这个tag下的所有 文件，而后解压缩并替换到你的文件系统中就可以了。</p>
<p><img src="/images/FviqExVCMcQBX-1UBH0HsQy16IpS.jpg" alt=""></p>
<h3 id="三、分支"><a href="#三、分支" class="headerlink" title="三、分支"></a>三、分支</h3><p>介绍几种多分支开发的情况下不同命令Git内部的动作。</p>
<ul>
<li>1.现在假设你的主分支是master，你希望增加一些特性，于是新建了一个experiment分支，并提交了几次。而后老板突然说需要发布一个新的版本，于是你切换回主分支（毕竟experiment的feature还没有work），提交了并打了tag：T1。那么现在我们的Git的commit、branch和tag看起来就像下图这样子。</li>
</ul>
<p><img src="/images/Frx7600bAImimfayP3FuohJGSjIc.jpg" alt=""></p>
<p>现在介绍另一个reference类型：remotes。假如你的仓库是clone而不是init得到的，那么里面会有一个目录.git/refs/remotes/，里面存放着其他人电脑上的分支（毕竟git是一个分布式的版本控制系统）。remote有一个默认的名字origin，也就是你clone的那台电脑，对应目录.git/refs/remotes/origin。而远程分支可以用origin/[branch_name]比如origin/master来访问。</p>
<ul>
<li>2.现在假如你clone了一个remote的仓库，做了一些修改和commit，而后远程仓库有另一些修改和commit（通过master分支和一个叫做idea的分支），之后你通过fetch命令把远程的修改下载到了本地。（这个命令会把你本地没有的refs和objects拉取下来）那么你本地的Git就像下图这样。</li>
</ul>
<p><img src="/images/FiwANRj4z0k78nxHfAsCfYysYkO1.jpg" alt=""></p>
<p>现在我们希望得到远程的master和idea的修改，同时保持我们的修改，需要在这三个分支上进行merge，为了保险，新建一个tryidea分支来merge，merge的结果如下图。</p>
<p><img src="/images/FlsYDvmc4nS29fmTk-_6eXKjuEfL.jpg" alt=""></p>
<ul>
<li>3.现在你和一个叫Jen的开发者同时开发，为了获取你们两个的进度你把他的remote-repo叫做jen，并不时fetch到你的本地，和你的进度merge一下，这么干了几次后，你的本地Git-history变成了这样：</li>
</ul>
<p><img src="/images/FiyePFMtjKHYMYt8RDOxMMC091Vr.jpg" alt=""></p>
<p>看起来有些杂乱，而且如果你们一直并行的开发，Git的历史就会越来越乱。这就是rebase命令起作用的时候。就像下面两幅图所述，rebase在指定一个upstream的branch的时候，会首先找到你当前所在branch和目标branch指向的commit（如下图 R3和C6）的公共祖先的commit（如下图的R2），而后记录R2到你当前的branch指向的commit（C6）中每个commit的变化为n个patch（即R1：C2，C2：C3）。</p>
<p>而后呢，Git以你指定的branch指向的commit（即R2）为基础，不断的应用这n个patch到这个commit，而commit message保持和你之前的message相同，最终形成两个新的commit（C6，C7），这样整个Git的历史看起来就整洁很多。当然，中间可能出现一些conflict需要手动解决。</p>
<p>而之前的那些被rebase过程舍弃的commit，可以被Git的GC-tools删除。</p>
<p><img src="/images/Fto9eG-Csnkn-qOmxXDFBZyka7WM.jpg" alt=""></p>
<p><img src="/images/FsH58IFKVPDeki6UFPCt3hMTN7Gi.jpg" alt=""></p>
<p>最后，经过rebase而不是merge命令，Git的历史如下图1所示。</p>
<p><img src="/images/FtzNCve12g5QQfsEw2eZBVixXzyR.jpg" alt=""></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="http://git-scm.com/" target="_blank" rel="noopener">Git-scm</a></li>
</ul>
]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode: BFS</title>
    <url>/2019/01/25/leetcode/BFS%E9%A2%98%E7%9B%AE/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>写一个二叉树序列化和反序列化的函数，要求以层序遍历。</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>对二叉树来说层序遍历即BFS，使用队列即可，因为没有环所以不需要记录visited。在反序列化的时候首先把字符串转化为TreeNode数组，然后层序遍历root恢复子节点连接，用一个指针记录数组的行进步数。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><h4 id="1-本题，层序遍历。"><a href="#1-本题，层序遍历。" class="headerlink" title="1. 本题，层序遍历。"></a>1. 本题，层序遍历。</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode(object):</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> queue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Codec</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">serialize</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""Encodes a tree to a single string.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: str</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = []</span><br><span class="line">        q = queue.Queue()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">traverse</span><span class="params">(root)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                res.append(<span class="string">'null'</span>)</span><br><span class="line">                <span class="keyword">return</span> </span><br><span class="line">            res.append(str(root.val))</span><br><span class="line">            q.put(root.left)</span><br><span class="line">            q.put(root.right)</span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> q.empty():</span><br><span class="line">                cur = q.get()</span><br><span class="line">                traverse(cur)</span><br><span class="line">        traverse(root)</span><br><span class="line">        <span class="keyword">while</span> res[<span class="number">-1</span>] == <span class="string">'null'</span>: res.pop()</span><br><span class="line">        ret = <span class="string">'['</span> + <span class="string">','</span>.join(res) + <span class="string">']'</span></span><br><span class="line">        print(ret)</span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deserialize</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="string">"""Decodes your encoded data to tree.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :type data: str</span></span><br><span class="line"><span class="string">        :rtype: TreeNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        data = data[<span class="number">1</span>:<span class="number">-1</span>].split(<span class="string">','</span>)</span><br><span class="line">        data = [TreeNode(int(i)) <span class="keyword">if</span> i != <span class="string">'null'</span> <span class="keyword">else</span> <span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">        q = queue.Queue()</span><br><span class="line">        root, i = data[<span class="number">0</span>], <span class="number">1</span></span><br><span class="line">        q.put(root)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> q.empty() <span class="keyword">and</span> i &lt; len(data):</span><br><span class="line">            cur = q.get()</span><br><span class="line">            <span class="keyword">if</span> cur:</span><br><span class="line">                cur.left = data[i]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">                q.put(cur.left)</span><br><span class="line">                <span class="keyword">if</span> i &lt; len(data):</span><br><span class="line">                    cur.right = data[i] </span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">                    q.put(cur.right)</span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="comment"># Your Codec object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"># codec = Codec()</span></span><br><span class="line"><span class="comment"># codec.deserialize(codec.serialize(root))</span></span><br></pre></td></tr></table></figure>
<h4 id="2-jump-game"><a href="#2-jump-game" class="headerlink" title="2. jump game"></a>2. jump game</h4><p>BFS加判断：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> Queue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canReach</span><span class="params">(self, arr: List[int], start: int)</span> -&gt; bool:</span></span><br><span class="line">        n = len(arr)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> arr <span class="keyword">or</span> start &gt;= n <span class="keyword">or</span> start &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> len(arr) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> arr[<span class="number">0</span>] == <span class="number">0</span></span><br><span class="line">        visited = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">        visited[start] = <span class="literal">True</span></span><br><span class="line">        q = Queue()</span><br><span class="line">        q.put(start)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> q.empty():</span><br><span class="line">            cur = q.get()</span><br><span class="line">            <span class="keyword">if</span> arr[cur] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            left = cur - arr[cur]</span><br><span class="line">            right = cur + arr[cur]</span><br><span class="line">            <span class="comment"># print(left, right)</span></span><br><span class="line">            <span class="keyword">for</span> each <span class="keyword">in</span> [left, right]:</span><br><span class="line">                <span class="keyword">if</span> each &lt; <span class="number">0</span> <span class="keyword">or</span> each &gt;= n:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> visited[each]:</span><br><span class="line">                    visited[each] = <span class="literal">True</span></span><br><span class="line">                    q.put(each)</span><br><span class="line">                    <span class="comment"># print(list(q.queue))</span></span><br><span class="line">                    <span class="keyword">if</span> arr[each] == <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4 id="3-jump-game-iv"><a href="#3-jump-game-iv" class="headerlink" title="3.jump game iv"></a>3.jump game iv</h4><p>也是要求最少次数跳到最后一个数字。每个数字可以往左往右或者跳到相同的数字去。</p>
<p>想求最短路径肯定是BFS，记录visited。遍历数组记录每个数字对应的可行选择。</p>
<p>但是这里有一个测试用例没有过，因为数组里连续子数组都是相同数字，实际上没有必要跳到中间的数字上面。（剪枝）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> Queue</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minJumps</span><span class="params">(self, arr: List[int])</span> -&gt; int:</span></span><br><span class="line">        choice_dict = defaultdict(list)</span><br><span class="line">        n = len(arr)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> arr:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        visited = [(<span class="literal">True</span>, <span class="number">0</span>)] + [(<span class="literal">False</span>, <span class="number">0</span>)] * (n - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i, d <span class="keyword">in</span> enumerate(arr):</span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> i != n - <span class="number">1</span> <span class="keyword">and</span> arr[i] == arr[i - <span class="number">1</span>] == arr[i + <span class="number">1</span>]: <span class="comment"># 剪枝，把中间的相同数字去掉</span></span><br><span class="line">                visited[i] = (<span class="literal">True</span>, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                choice_dict[d].append(i)</span><br><span class="line">        q = Queue()</span><br><span class="line">        q.put(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> q.empty():</span><br><span class="line">            cur = q.get()</span><br><span class="line">            cur_step = visited[cur][<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> cur == n - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> cur_step</span><br><span class="line">            l, r = max(cur - <span class="number">1</span>, <span class="number">0</span>), min(cur + <span class="number">1</span>, n - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> each <span class="keyword">in</span> choice_dict[arr[cur]] + [l, r]:</span><br><span class="line">                <span class="keyword">if</span> each == n - <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span> cur_step + <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> visited[each][<span class="number">0</span>]:</span><br><span class="line">                    visited[each] = (<span class="literal">True</span>, cur_step + <span class="number">1</span>)</span><br><span class="line">                    <span class="comment"># print('&#123;&#125; -&gt; &#123;&#125;, &#123;&#125;'.format(cur, each, res))</span></span><br><span class="line">                    q.put(each)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h4 id="4-填充每个节点的下一个右侧节点指针-II"><a href="#4-填充每个节点的下一个右侧节点指针-II" class="headerlink" title="4.填充每个节点的下一个右侧节点指针 II"></a>4.填充每个节点的下一个右侧节点指针 II</h4><p>有常量级别的空间开销要求，层序遍历不满足：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connect_bfs</span><span class="params">(self, root: <span class="string">'Node'</span>)</span> -&gt; 'Node':</span></span><br><span class="line">        q = Queue()</span><br><span class="line">        q.put((root, <span class="number">1</span>))</span><br><span class="line">        prev, prev_l = <span class="literal">None</span>, <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> q.empty():</span><br><span class="line">            cur, layer = q.get()</span><br><span class="line">            <span class="keyword">if</span> layer == prev_l:</span><br><span class="line">                prev.next = cur</span><br><span class="line">            prev, prev_l = cur, layer</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cur:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> cur.left:</span><br><span class="line">                q.put((cur.left, layer + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> cur.right:</span><br><span class="line">                q.put((cur.right, layer + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<p>用上一层来把下一层的next连起来，遍历下一层的时候只需要当作链表就行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connect</span><span class="params">(self, root: <span class="string">'Node'</span>)</span> -&gt; 'Node':</span></span><br><span class="line">        start = root</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> </span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">do</span><span class="params">(cur)</span>:</span></span><br><span class="line">            <span class="keyword">nonlocal</span> n_start, prev</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> n_start:</span><br><span class="line">                n_start = cur</span><br><span class="line">            <span class="keyword">if</span> prev:</span><br><span class="line">                prev.next = cur</span><br><span class="line">            prev = cur</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> start:</span><br><span class="line">            prev, n_start = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">            <span class="keyword">while</span> start:</span><br><span class="line">                <span class="keyword">if</span> start.left:</span><br><span class="line">                    do(start.left)</span><br><span class="line">                <span class="keyword">if</span> start.right:</span><br><span class="line">                    do(start.right)</span><br><span class="line">                start = start.next</span><br><span class="line">            start = n_start</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>Tree</tag>
        <tag>BFS</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:basic calculator II</title>
    <url>/2019/06/20/leetcode/basic%20calculator%20II/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>实现一个对于正整数和“+, -, *, /, (, )”的计算器。每个表达式都是合法的。乘除法优先级较高。</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>实际只需要一个操作数栈存储带正负号的操作数。遇到括号就递归处理。遇到优先级高的操作符乘除就直接算。</p>
<h3 id="三、实现"><a href="#三、实现" class="headerlink" title="三、实现"></a>三、实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = [str(a) <span class="keyword">for</span> a <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span><span class="params">(self, s: str)</span> -&gt; int:</span>        </span><br><span class="line">        <span class="keyword">return</span> cal(list(reversed(s)))</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal</span><span class="params">(s)</span>:</span></span><br><span class="line">    stack = []</span><br><span class="line">    op_stack = []</span><br><span class="line">    v, valid = <span class="number">0</span>, <span class="literal">False</span></span><br><span class="line">    sign = <span class="literal">None</span></span><br><span class="line">    compute_flag = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> s:</span><br><span class="line">        c = s.pop()</span><br><span class="line">        <span class="keyword">if</span> c == <span class="string">' '</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> c <span class="keyword">in</span> nums:</span><br><span class="line">            valid = <span class="literal">True</span></span><br><span class="line">            v = v * <span class="number">10</span> + int(c)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> valid:</span><br><span class="line">                <span class="keyword">if</span> compute_flag != <span class="number">1</span>:</span><br><span class="line">                    v = -v <span class="keyword">if</span> sign == <span class="string">'-'</span> <span class="keyword">else</span> v  <span class="comment"># 加减法直接记录</span></span><br><span class="line">                stack.append(v)</span><br><span class="line">                <span class="keyword">if</span> compute_flag == <span class="number">1</span>:</span><br><span class="line">                    evaluation(stack, op_stack)</span><br><span class="line">            v, valid = <span class="number">0</span>, <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> c == <span class="string">'('</span>:</span><br><span class="line">                stack.append( self.calculate(s))</span><br><span class="line">            <span class="keyword">elif</span> c == <span class="string">')'</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># ['+', '-', '*', '/']</span></span><br><span class="line">                compute_flag = <span class="number">1</span> <span class="keyword">if</span> c <span class="keyword">in</span> [<span class="string">'*'</span>, <span class="string">'/'</span>] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">                <span class="keyword">if</span> compute_flag == <span class="number">1</span>:</span><br><span class="line">                    op_stack.append(c)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    sign = c</span><br><span class="line">        <span class="comment"># print('stack: &#123;&#125;'.format(stack))</span></span><br><span class="line">        <span class="comment"># print('op_stack: &#123;&#125;'.format(op_stack))</span></span><br><span class="line">    <span class="keyword">if</span> valid == <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">if</span> compute_flag != <span class="number">1</span>:</span><br><span class="line">            v = -v <span class="keyword">if</span> sign == <span class="string">'-'</span> <span class="keyword">else</span> v</span><br><span class="line">        stack.append(v)</span><br><span class="line">        <span class="keyword">if</span> compute_flag == <span class="number">1</span>:</span><br><span class="line">            evaluation(stack, op_stack)</span><br><span class="line">    <span class="keyword">return</span> greedy_evaluation(stack, op_stack)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(op1, op2, sign)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> sign == <span class="string">'+'</span>:</span><br><span class="line">        <span class="keyword">return</span> op1 + op2</span><br><span class="line">    <span class="keyword">if</span> sign == <span class="string">'-'</span>:</span><br><span class="line">        <span class="keyword">return</span> op2 - op1</span><br><span class="line">    <span class="keyword">if</span> sign == <span class="string">'*'</span>:</span><br><span class="line">        <span class="keyword">return</span> op1 * op2</span><br><span class="line">    <span class="keyword">if</span> sign == <span class="string">'/'</span>:</span><br><span class="line">        <span class="keyword">return</span> op2 // op1 <span class="keyword">if</span> op2 &gt;= <span class="number">0</span> <span class="keyword">else</span> -(-op2 // op1)  <span class="comment"># 除法判断一下</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluation</span><span class="params">(stack, op_stack)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(stack) &lt; <span class="number">2</span> <span class="keyword">or</span> stack[<span class="number">-2</span>] == <span class="string">'('</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    sign = op_stack.pop()</span><br><span class="line">    op1, op2 = stack.pop(), stack.pop()</span><br><span class="line">    res = helper(op1, op2, sign)</span><br><span class="line">    stack.append(res)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_evaluation</span><span class="params">(stack, op_stack)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(stack)</span><br><span class="line">	<span class="comment"># 就不用憨憨的再一个一个pop</span></span><br><span class="line">    <span class="comment"># res = None</span></span><br><span class="line">    <span class="comment"># while op_stack:</span></span><br><span class="line">    <span class="comment">#     res = res if res != None else stack.pop(0)</span></span><br><span class="line">    <span class="comment">#     sign = op_stack.pop(0)</span></span><br><span class="line">    <span class="comment">#     op2 = stack.pop(0)</span></span><br><span class="line">    <span class="comment">#     res = helper(op2, res, sign)</span></span><br><span class="line">    <span class="comment"># return res if res != None else stack[0]</span></span><br></pre></td></tr></table></figure>
<p>有两个点需要注意：</p>
<p>1.不要用pop(0)，因为是O(n)的开销。</p>
<p>2.加减法就不用存储到op栈了，记录操作符的sign然后给操作数修改一下就行。这样最后算的时候直接sum数据栈就行。</p>
<p>3.如果是用2的方法那么对除法要特殊处理，因为整除除法总是向0靠近。</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>Stack</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode: buy-sell-stock</title>
    <url>/2019/02/18/leetcode/buy-sell-stock/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>这是一个系列问题。</p>
<p>给定一个股票价格的数组，求在k次交易的限制下最佳收益。（变种为无交易限制、增加交易费用、增加交易冷却时间等）</p>
<p>必须在卖出后才能再次买入。</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>状态：在k次交易限制下，需要求最后一天的收益，那么第i天经过了k次交易当前持有股票时的收益记录为<script type="math/tex">dp[i][k][1]</script>，交易次数就是买入次数。</p>
<p>初始值：如果没有进行过交易，那么肯定不持有股票收益是0，<script type="math/tex">dp[:][0][0] = 0</script>，这时候肯定无法持有股票设置收益为负无穷<script type="math/tex">dp[:][0][1] = -inf</script>。我们用<script type="math/tex">dp[0][:][:]</script>记录第一天交易前的状态，很显然<script type="math/tex">dp[0][:][1] = -inf</script>。</p>
<p>递推：已经知道之前的状态，如果此次交易之后持有股票，那么可能是之前一天持有的或者是今天买入的；否则可能是前一天就没有持有或者今天卖出。</p>
<p>返回：<script type="math/tex">dp[n][:][1]</script>中最大的一个。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><p>时间复杂度和空间开销都是<script type="math/tex">O(n\cdot k)</script>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, k: int, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        inf = sys.maxsize</span><br><span class="line">        K = k</span><br><span class="line">        n = len(prices)</span><br><span class="line">        <span class="keyword">if</span> K &gt; n // <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> self.maxProfit_wo_limit(prices)</span><br><span class="line">        dp = [</span><br><span class="line">            [</span><br><span class="line">             [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]  <span class="comment"># 当前是否持有</span></span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> range(K + <span class="number">1</span>)]  <span class="comment"># k笔交易</span></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(n + <span class="number">1</span>)   <span class="comment"># n天</span></span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(K + <span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">0</span>][k][<span class="number">1</span>] = -inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>):</span><br><span class="line">            dp[i][<span class="number">0</span>][<span class="number">1</span>] = -inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, K + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">for</span> state <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                    <span class="keyword">if</span> state == <span class="number">0</span>: <span class="comment"># 今天不持有情况下的最大收益</span></span><br><span class="line">                        dp[i][k][<span class="number">0</span>] = max(dp[i - <span class="number">1</span>][k][<span class="number">0</span>], dp[i - <span class="number">1</span>][k][<span class="number">1</span>] + prices[i - <span class="number">1</span>])</span><br><span class="line">                        <span class="comment"># sell = dp[i - 1][k][1] + prices[i - 1]</span></span><br><span class="line">                        <span class="comment"># if dp[i][k][0] == sell:</span></span><br><span class="line">                            <span class="comment"># print('Sell@&#123;&#125;: &#123;&#125;, profit: &#123;&#125;, deal: &#123;&#125;'.format(i, prices[i - 1], dp[i][k][0], k))</span></span><br><span class="line">                    <span class="keyword">else</span>:  <span class="comment"># 今天持有情况下的最大收益</span></span><br><span class="line">                        dp[i][k][<span class="number">1</span>] = max(dp[i - <span class="number">1</span>][k][<span class="number">1</span>], dp[i - <span class="number">1</span>][k - <span class="number">1</span>][<span class="number">0</span>] - prices[i - <span class="number">1</span>])</span><br><span class="line">                        <span class="comment"># buy = dp[i - 1][k - 1][0] - prices[i - 1]</span></span><br><span class="line">                        <span class="comment"># if dp[i][k][1] == buy:</span></span><br><span class="line">                            <span class="comment"># print('Buy@&#123;&#125;: &#123;&#125;, profit: &#123;&#125;, deal: &#123;&#125;'.format(i, prices[i - 1], dp[i][k][1], k))</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> max(dp[n][k][<span class="number">0</span>] <span class="keyword">for</span> k <span class="keyword">in</span> range(K + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit_wo_limit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        inf = sys.maxsize</span><br><span class="line">        n = len(prices)</span><br><span class="line">        dp = [</span><br><span class="line">             [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]  <span class="comment"># 当前是否持有</span></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(n + <span class="number">1</span>)   <span class="comment"># n天</span></span><br><span class="line">        ]</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">1</span>] = -inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> state <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                <span class="keyword">if</span> state == <span class="number">0</span>: <span class="comment"># 今天不持有情况下的最大收益</span></span><br><span class="line">                    dp[i][<span class="number">0</span>] = max(dp[i - <span class="number">1</span>][<span class="number">0</span>], dp[i - <span class="number">1</span>][<span class="number">1</span>] + prices[i - <span class="number">1</span>])</span><br><span class="line">                    <span class="comment"># sell = dp[i - 1][1] + prices[i - 1]</span></span><br><span class="line">                    <span class="comment"># if dp[i][k][0] == sell:</span></span><br><span class="line">                        <span class="comment"># print('Sell@&#123;&#125;: &#123;&#125;, profit: &#123;&#125;, deal: &#123;&#125;'.format(i, prices[i - 1], dp[i][k][0], k))</span></span><br><span class="line">                <span class="keyword">else</span>:  <span class="comment"># 今天持有情况下的最大收益</span></span><br><span class="line">                    dp[i][<span class="number">1</span>] = max(dp[i - <span class="number">1</span>][<span class="number">1</span>], dp[i - <span class="number">1</span>][<span class="number">0</span>] - prices[i - <span class="number">1</span>])</span><br><span class="line">                    <span class="comment"># buy = dp[i - 1][0] - prices[i - 1]</span></span><br><span class="line">                    <span class="comment"># if dp[i][k][1] == buy:</span></span><br><span class="line">                        <span class="comment"># print('Buy@&#123;&#125;: &#123;&#125;, profit: &#123;&#125;, deal: &#123;&#125;'.format(i, prices[i - 1], dp[i][k][1], k))</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dp[n][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>这里对问题的一些变种可以优化空间开销，比如不限制交易次数的情况。</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode: basic calculator</title>
    <url>/2019/06/18/leetcode/basic%20calculator/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>实现一个对于正整数和“+, -, (, )”的计算器。每个表达式都是合法的。</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>两个栈，操作数栈存储操作数 + “(“，操作符栈存储其他操作符。数字直接入栈，操作符遇到右括号时将对应的左括号删除并求值，其他符号直接求值。</p>
<h3 id="三、实现"><a href="#三、实现" class="headerlink" title="三、实现"></a>三、实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> enum <span class="keyword">import</span> Enum</span><br><span class="line"></span><br><span class="line">State = Enum(<span class="string">'State'</span>, (<span class="string">'begin'</span>, <span class="string">'num'</span>, <span class="string">'op'</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        nums = [str(a) <span class="keyword">for</span> a <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(op1, op2, sign)</span>:</span></span><br><span class="line">            <span class="comment"># 顺序不要搞反了</span></span><br><span class="line">            <span class="keyword">if</span> sign == <span class="string">'+'</span>:</span><br><span class="line">                <span class="keyword">return</span> op1 + op2</span><br><span class="line">            <span class="keyword">if</span> sign == <span class="string">'-'</span>:</span><br><span class="line">                <span class="keyword">return</span> op2 - op1</span><br><span class="line">            <span class="keyword">if</span> sign == <span class="string">'*'</span>:</span><br><span class="line">                <span class="keyword">return</span> op1 * op2</span><br><span class="line">            <span class="keyword">if</span> sign == <span class="string">'/'</span>:</span><br><span class="line">                <span class="keyword">return</span> op2 / op1</span><br><span class="line">            </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">evaluation</span><span class="params">()</span>:</span></span><br><span class="line">            <span class="keyword">if</span> len(stack) &lt; <span class="number">2</span> <span class="keyword">or</span> stack[<span class="number">-2</span>] == <span class="string">'('</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            sign = op_stack.pop()</span><br><span class="line">            op1, op2 = stack.pop(), stack.pop()</span><br><span class="line">            res = helper(op1, op2, sign)</span><br><span class="line">            stack.append(res)</span><br><span class="line"></span><br><span class="line">        stack = []</span><br><span class="line">        op_stack = []</span><br><span class="line">        v = <span class="number">0</span></span><br><span class="line">        compute_flag = <span class="number">0</span></span><br><span class="line">        state = State.begin</span><br><span class="line">        <span class="keyword">if</span> s[<span class="number">0</span>] <span class="keyword">in</span> nums:</span><br><span class="line">            state = State.num</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            state = State.op</span><br><span class="line"></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len(s):</span><br><span class="line">            c = s[i]</span><br><span class="line">            <span class="keyword">if</span> c == <span class="string">' '</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> state == State.num:</span><br><span class="line">                <span class="keyword">if</span> c <span class="keyword">in</span> nums:</span><br><span class="line">                    v = v * <span class="number">10</span> + int(c)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    state = State.op</span><br><span class="line">                    stack.append(v)</span><br><span class="line">                    <span class="comment"># print('stack: &#123;&#125;'.format(stack))</span></span><br><span class="line">                    v = <span class="number">0</span></span><br><span class="line">                    evaluation()</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">elif</span> state == State.op:</span><br><span class="line">                <span class="keyword">if</span> c <span class="keyword">in</span> nums:</span><br><span class="line">                    state = State.num</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> c == <span class="string">'('</span>:</span><br><span class="line">                        stack.append(c)</span><br><span class="line">                        compute_flag = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">elif</span> c == <span class="string">')'</span>:</span><br><span class="line">                        last = stack.pop()</span><br><span class="line">                        stack[<span class="number">-1</span>] = last  <span class="comment"># 删除倒数第二个元素（对应的左括号）</span></span><br><span class="line">                        evaluation()</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        compute_flag = <span class="number">1</span></span><br><span class="line">                        op_stack.append(c)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'illegal state.'</span>)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            <span class="comment"># print('stack: &#123;&#125;'.format(stack))</span></span><br><span class="line">            <span class="comment"># print('op_stack: &#123;&#125;'.format(op_stack))</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> state == State.num:</span><br><span class="line">            stack.append(v)</span><br><span class="line">        <span class="keyword">while</span> op_stack:</span><br><span class="line">            evaluation()</span><br><span class="line">        <span class="keyword">return</span> stack[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>Stack</tag>
        <tag>FSM</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode: combination-sum系列</title>
    <url>/2019/05/25/leetcode/DFS%E9%A2%98%E7%9B%AE/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。</p>
<p>candidates 中的数字可以无限制重复被选取。</p>
<ul>
<li>所有数字（包括 <code>target</code>）都是正整数。</li>
<li>解集不能包含重复的组合。 </li>
</ul>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>首先利用回溯能够没有遗漏的找到所有的组合，但是这样是会重复的。</p>
<p>我们可以首先对数组升序排列，然后对DFS搜索增加一个index，这样就控制了组合里面数的顺序，确保不会重复。</p>
<p>剪枝，如果当前的选择无法组合，那么当前轮之后的选择直接跳过。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><h4 id="1-基础题"><a href="#1-基础题" class="headerlink" title="1.基础题"></a>1.基础题</h4><p>排序 + 回溯的DFS即可。</p>
<p>target = t，len(candidates) = n的时候，DFS时间开销的上界是<script type="math/tex">O(2^n)*n</script>，除了答案以外的空间开销<script type="math/tex">O(t)</script>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combinationSum</span><span class="params">(self, candidates: List[int], target: int)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        res = []</span><br><span class="line">        candidates = sorted(candidates)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(target, ans, i)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> target == <span class="number">0</span>:</span><br><span class="line">                res.append(ans.copy())</span><br><span class="line">            <span class="keyword">for</span> j, each <span class="keyword">in</span> enumerate(candidates):</span><br><span class="line">                <span class="keyword">if</span> j &lt; i:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> target - each &gt;= <span class="number">0</span>:</span><br><span class="line">                    ans.append(each)</span><br><span class="line">                    print(ans)</span><br><span class="line">                    dfs(target - each, ans, j)</span><br><span class="line">                    ans.pop()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        dfs(target, [], <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4 id="2-变形"><a href="#2-变形" class="headerlink" title="2.变形"></a>2.变形</h4><p>进一步，如果题目要求不可以选择相同的元素，而且在数组中可能出现重复元素（也就是依靠索引去重没用了），稍加修改，以下是两种改法：</p>
<p>假如对于测试用例[10,1,2,7,6,1,5], 8来说，排序之后是[1,1,2,5,6,7,10]，这时候我们可以在不同的函数中选择两个相同的1（index=0，index=1），因为索引范围保证了不会重复；但是不能在同一次递归函数的调用时选择两个1，否则就会得到[1,2,5]（index=0）和[1,2,5]（index=1）两组一样的结果了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combinationSum2</span><span class="params">(self, candidates: List[int], target: int)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        res = []</span><br><span class="line">        candidates = sorted(candidates)</span><br><span class="line">        used = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(candidates))]  <span class="comment"># 用used数组来记录当前是否使用了第i个索引</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(target, ans, i, used)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> target == <span class="number">0</span>:</span><br><span class="line">                res.append(ans.copy())</span><br><span class="line">            <span class="keyword">for</span> j, each <span class="keyword">in</span> enumerate(candidates):</span><br><span class="line">                <span class="keyword">if</span> j &lt; i:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> target - each &gt;= <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">if</span> j &gt; <span class="number">0</span> <span class="keyword">and</span> candidates[j - <span class="number">1</span>] == candidates[j] <span class="keyword">and</span> <span class="keyword">not</span> used[j - <span class="number">1</span>]:<span class="comment"># 避免重复，used为假说明同一次函数调用</span></span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    ans.append(each)</span><br><span class="line">                    used[j] = <span class="literal">True</span></span><br><span class="line">                    dfs(target - each, ans, j + <span class="number">1</span>, used)  <span class="comment"># 这里改成j + 1不再选择相同索引的相同元素</span></span><br><span class="line">                    used[j] = <span class="literal">False</span>  </span><br><span class="line">                    ans.pop()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        dfs(target, [], <span class="number">0</span>, used)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>这样需要额外使用一个标记数组，另一种改法是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combinationSum2</span><span class="params">(self, candidates: List[int], target: int)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        res = []</span><br><span class="line">        candidates = sorted(candidates)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(target, ans, i)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> target == <span class="number">0</span>:</span><br><span class="line">                res.append(ans.copy())</span><br><span class="line">            <span class="keyword">for</span> j, each <span class="keyword">in</span> enumerate(candidates):</span><br><span class="line">                <span class="keyword">if</span> j &lt; i:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> target - each &gt;= <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># 避免重复，j &gt; 0改成j &gt; i，这样保证了j - 1符合条件的话一定已经被选择了</span></span><br><span class="line">                    <span class="keyword">if</span> j &gt; i <span class="keyword">and</span> candidates[j - <span class="number">1</span>] == candidates[j]: </span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    ans.append(each)</span><br><span class="line">                    dfs(target - each, ans, j + <span class="number">1</span>)<span class="comment"># 这里改成j + 1不再选择相同索引的相同元素</span></span><br><span class="line">                    ans.pop()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        dfs(target, [], <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4 id="3-变形2"><a href="#3-变形2" class="headerlink" title="3.变形2"></a>3.变形2</h4><p>找出所有相加之和为 <strong><em>n</em></strong> 的 <strong>k</strong>个数的组合。组合中只允许含有 1 - 9 的正整数，并且每种组合中不存在重复的数字。</p>
<p>防止组合中重复的数字：从小到大选择，选了的不再选。（树的不同层）</p>
<p>防止重复的解：同一个选择中不选择两个同样的数字。（树的同一层）</p>
<p>k个数字：记录一下数字的数量（或者ans的长度也ok）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combinationSum3</span><span class="params">(self, k: int, n: int)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(ans, target, remain_nums, j)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> remain_nums == <span class="number">0</span>:  <span class="comment"># 剪枝 + 完成搜索</span></span><br><span class="line">                <span class="keyword">if</span> target == <span class="number">0</span>:</span><br><span class="line">                    res.append(ans.copy())</span><br><span class="line">                <span class="keyword">return</span>    </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">                <span class="keyword">if</span> i &lt; j:  <span class="comment"># 防止重复</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> target - i &lt; <span class="number">0</span>:  <span class="comment"># 剪枝</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> target - <span class="number">9</span> * remain_nums &gt; <span class="number">0</span>:  <span class="comment"># 剪枝</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                ans.append(i)</span><br><span class="line">                dfs(ans, target - i, remain_nums - <span class="number">1</span>, i + <span class="number">1</span>)  <span class="comment"># 下界为[i + 1, ...]防止重复</span></span><br><span class="line">                ans.pop()</span><br><span class="line">        dfs([], n, k, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4 id="4-单词搜索"><a href="#4-单词搜索" class="headerlink" title="4.单词搜索"></a>4.单词搜索</h4><blockquote>
<p>给定一个二维网格和一个单词，找出该单词是否存在于网格中。</p>
<p>单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。</p>
</blockquote>
<p>用visited记录 + DFS。第一个单词的visited记得修改。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">exist</span><span class="params">(self, board: List[List[str]], word: str)</span> -&gt; bool:</span></span><br><span class="line">        n = len(word)</span><br><span class="line">        x, y = len(board), len(board[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># actions = [[defaultdict() * len(board[i])] for i in range(len(board))]</span></span><br><span class="line">        <span class="comment"># for i in range(len(board)):</span></span><br><span class="line">        <span class="comment">#     for j in range(len(board[i])):</span></span><br><span class="line">        <span class="comment">#         actions </span></span><br><span class="line">        visited = [[<span class="literal">False</span>] * len(board[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(board))]</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_a</span><span class="params">(i, j)</span>:</span></span><br><span class="line">            res = []</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                res.append((i - <span class="number">1</span>, j))</span><br><span class="line">            <span class="keyword">if</span> i &lt; x - <span class="number">1</span>:</span><br><span class="line">                res.append((i + <span class="number">1</span>, j))</span><br><span class="line">            <span class="keyword">if</span> j &gt; <span class="number">0</span>:</span><br><span class="line">                res.append((i, j - <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> j &lt; y - <span class="number">1</span>:</span><br><span class="line">                res.append((i, j + <span class="number">1</span>))</span><br><span class="line">            <span class="comment"># print('(&#123;&#125;,&#123;&#125;) actions: &#123;&#125;'.format(i, j, res))</span></span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        </span><br><span class="line">        first = [(i, j) <span class="keyword">for</span> i <span class="keyword">in</span> range(x) <span class="keyword">for</span> j <span class="keyword">in</span> range(y) <span class="keyword">if</span> board[i][j] == word[<span class="number">0</span>]]</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(i, j, cur_index)</span>:</span></span><br><span class="line">            <span class="comment"># print('cur: (&#123;&#125;, &#123;&#125;)&#123;&#125;'.format(i, j, word[cur_index]))</span></span><br><span class="line">            <span class="keyword">if</span> cur_index == n - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            c = word[cur_index + <span class="number">1</span>]</span><br><span class="line">            <span class="keyword">for</span> a_i, a_j <span class="keyword">in</span> get_a(i, j):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> visited[a_i][a_j] <span class="keyword">and</span> board[a_i][a_j] == c:</span><br><span class="line">                    <span class="comment"># print('visit: (&#123;&#125;, &#123;&#125;)&#123;&#125;'.format(a_i, a_j, board[a_i][a_j]))</span></span><br><span class="line">                    visited[a_i][a_j] = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">if</span> dfs(a_i, a_j, cur_index + <span class="number">1</span>):</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    visited[a_i][a_j] = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> f_i, f_j <span class="keyword">in</span> first:</span><br><span class="line">            visited[f_i][f_j] = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> dfs(f_i, f_j, <span class="number">0</span>):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            visited[f_i][f_j] = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4 id="5-全排列2"><a href="#5-全排列2" class="headerlink" title="5. 全排列2"></a>5. 全排列2</h4><blockquote>
<p>给定一个可包含重复数字的序列，返回所有不重复的全排列。</p>
</blockquote>
<p>跟之前的一题一样，对于样例[1,1,2]，用DFS进行选择的时候，出现如下两组重复：[1,1,2], [1,1,2]。<br>左边在第一层选了index=0的1，右边在第一层选择了index=1的1，要避免这种重复。<br>直接在每个递归调用的时候增加一步判断。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">permuteUnique</span><span class="params">(self, nums: List[int])</span> -&gt; List[List[int]]:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        visited = [<span class="literal">False</span>] * n</span><br><span class="line">        res = []</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(i, ans)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> i == n:</span><br><span class="line">                res.append(ans.copy())</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            v = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                <span class="keyword">if</span> visited[j] <span class="keyword">or</span> nums[j] <span class="keyword">in</span> v:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                ans.append(nums[j])</span><br><span class="line">                v[nums[j]] = <span class="number">1</span></span><br><span class="line">                visited[j] = <span class="literal">True</span></span><br><span class="line">                <span class="comment"># print('&#123;&#125;: &#123;&#125; -&gt; &#123;&#125;'.format(i, ans, nums[j]))</span></span><br><span class="line">                dfs(i + <span class="number">1</span>, ans)</span><br><span class="line">                visited[j] = <span class="literal">False</span></span><br><span class="line">                ans.pop()</span><br><span class="line">        </span><br><span class="line">        dfs(<span class="number">0</span>, [])</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4 id="6-组合总和IV"><a href="#6-组合总和IV" class="headerlink" title="6.组合总和IV"></a>6.组合总和IV</h4><blockquote>
<p>给定一个由正整数组成且不存在重复数字的数组，找出和为给定目标正整数的组合的个数。</p>
<p>nums = [1, 2, 3]<br>target = 4</p>
</blockquote>
<p>也是用DFS搜索，不过直接搜索空间太大，直接TLE了（比如[3,33]和target: 100000）。加一个lru_cache，其实因为参数只有一个剩余的target，所以就等于用dp了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combinationSum4</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; int:</span></span><br><span class="line">        nums = sorted(nums)</span><br><span class="line">        n = len(nums)</span><br><span class="line"></span><br><span class="line"><span class="meta">        @lru_cache(maxsize=100000)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(target)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> target == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">            res = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> each <span class="keyword">in</span> nums:</span><br><span class="line">                <span class="keyword">if</span> target - each &gt;= <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># print('&#123;&#125; - &#123;&#125; -&gt; &#123;&#125;'.format(target, each, target - each))</span></span><br><span class="line">                    res += dfs(target - each)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> dfs(target)</span><br></pre></td></tr></table></figure>
<h4 id="7-监控二叉树"><a href="#7-监控二叉树" class="headerlink" title="7.监控二叉树"></a>7.监控二叉树</h4><blockquote>
<p>给定一个二叉树，我们在树的节点上安装摄像头。</p>
<p>节点上的每个摄影头都可以监视<strong>其父对象、自身及其直接子对象。</strong></p>
<p>计算监控树的所有节点所需的最小摄像头数量。</p>
</blockquote>
<p>深度优先从叶子节点开始搜索，通过返回的子节点状态决定是否安装摄像头。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minCameraCover</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span></span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(cur)</span>:</span></span><br><span class="line">            <span class="keyword">nonlocal</span> ans</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cur:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">            <span class="comment"># 0: cur/child没有放置也没被照到, 1: cur/child有放置, 2: cur/child被照到或者是None</span></span><br><span class="line">            l_status, r_status = <span class="number">2</span>, <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> cur.left:</span><br><span class="line">                l_status = dfs(cur.left)</span><br><span class="line">            <span class="keyword">if</span> cur.right:</span><br><span class="line">                r_status = dfs(cur.right)</span><br><span class="line">            <span class="keyword">if</span> l_status == <span class="number">0</span> <span class="keyword">or</span> r_status == <span class="number">0</span>:  <span class="comment"># 有一个儿子节点没有放置</span></span><br><span class="line">                ans += <span class="number">1</span></span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> l_status == <span class="number">2</span> <span class="keyword">and</span> r_status == <span class="number">2</span>:  <span class="comment"># 叶子节点，或者等价于叶子节点</span></span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> l_status == <span class="number">1</span> <span class="keyword">or</span> r_status == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> l_status <span class="keyword">and</span> r_status &gt; <span class="number">0</span>:  <span class="comment"># child都放置了，躺平</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">                <span class="keyword">else</span>:  <span class="comment"># 否则</span></span><br><span class="line">                    ans += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">            <span class="comment"># print('&#123;&#125;, &#123;&#125;'.format(l_status, r_status))</span></span><br><span class="line">        out = dfs(root)</span><br><span class="line">        <span class="keyword">return</span> ans <span class="keyword">if</span> out &gt; <span class="number">0</span> <span class="keyword">else</span> ans + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="8-路经总和III"><a href="#8-路经总和III" class="headerlink" title="8.路经总和III"></a>8.路经总和III</h4><blockquote>
<p>给定一个二叉树，它的每个结点都存放着一个整数值。</p>
<p>找出路径和等于给定数值的路径总数。</p>
<p>路径不需要从根节点开始，也不需要在叶子节点结束，但是路径方向必须是向下的（只能从父节点到子节点）。</p>
<p>二叉树不超过1000个节点，且节点数值范围是 [-1000000,1000000] 的整数。</p>
</blockquote>
<p>两种方式，第一种方式双重递归，在递归到某个节点i的时候以其为root节点寻找所有向下的路径。</p>
<p>第二种方式，在DFS搜索的时候维护root到目前节点为止的h个前缀和的数组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode(object):</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.ans = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pathSum_double_recur</span><span class="params">(self, root, sum)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :type sum: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(cur, t)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cur:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            t -= cur.val</span><br><span class="line">            <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">                self.ans += <span class="number">1</span>  </span><br><span class="line">            dfs(cur.left, t)</span><br><span class="line">            dfs(cur.right, t)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        dfs(root, sum)</span><br><span class="line">        self.pathSum(root.left, sum)</span><br><span class="line">        self.pathSum(root.right, sum)</span><br><span class="line">        <span class="keyword">return</span> self.ans</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pathSum</span><span class="params">(self, root, sum)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :type sum: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(cur, t, prefix)</span>:</span></span><br><span class="line">            <span class="keyword">nonlocal</span> res</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cur:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            prefix = [x + cur.val <span class="keyword">for</span> x <span class="keyword">in</span> prefix]</span><br><span class="line">            prefix.append(cur.val)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> prefix:</span><br><span class="line">                <span class="keyword">if</span> t == x:</span><br><span class="line">                    res += <span class="number">1</span>  </span><br><span class="line">            dfs(cur.left, t, prefix)</span><br><span class="line">            dfs(cur.right, t, prefix)</span><br><span class="line">        </span><br><span class="line">        dfs(root, sum, [])</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>这样做在搜索到一个节点的时候需要$O(h)$的时间开销，可以利用hash表+前缀和来处理，这样每个节点就是$O(1)$了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pathSum</span><span class="params">(self, root, sum)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :type sum: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        count = defaultdict(int)</span><br><span class="line">        count[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(cur, kk)</span>:</span></span><br><span class="line">            <span class="keyword">nonlocal</span> res, count</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cur:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="comment"># print(kk)</span></span><br><span class="line">            <span class="comment"># print(cur.val)</span></span><br><span class="line">            kk += cur.val</span><br><span class="line">            res += count[kk - sum]</span><br><span class="line">            count[kk] += <span class="number">1</span></span><br><span class="line">            dfs(cur.left, kk)</span><br><span class="line">            dfs(cur.right, kk)</span><br><span class="line">            count[kk] -= <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        dfs(root, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4 id="9-二叉树中的最大路径和"><a href="#9-二叉树中的最大路径和" class="headerlink" title="9. 二叉树中的最大路径和"></a>9. 二叉树中的最大路径和</h4><blockquote>
<p>给定一个非空二叉树，返回其最大路径和。</p>
<p>本题中，路径被定义为一条从树中任意节点出发，沿父节点-子节点连接，达到任意节点的序列。该路径至少包含一个节点，且不一定经过根节点。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxPathSum</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span></span><br><span class="line">        max_ = -float(<span class="string">'inf'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(cur)</span>:</span></span><br><span class="line">            <span class="keyword">nonlocal</span> max_</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cur:</span><br><span class="line">                <span class="keyword">return</span> -float(<span class="string">'inf'</span>)</span><br><span class="line">            <span class="comment"># without parent</span></span><br><span class="line">            l, r = dfs(cur.left), dfs(cur.right)</span><br><span class="line">            without_parent = max(cur.val + l + r, l, r)</span><br><span class="line">            max_ = max(max_, without_parent)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># with parent</span></span><br><span class="line">            res = max(cur.val + l, cur.val + r, cur.val)</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> max(dfs(root), max_)</span><br></pre></td></tr></table></figure>
<p>dfs，对任意节点有两种情况：</p>
<p>包含当前节点的选择：每次递归返回当前节点下包含当前节点的最大路径和。</p>
<p>如果不包含当前节点：那么直接用一个全局变量保存并更新就好了。</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DFS</tag>
        <tag>Backtrack</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:coin-change-2</title>
    <url>/2019/03/18/leetcode/coin-change-2/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>给定若干硬币金额和一个总金额，返回能凑出这个总金额的凑法。</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>状态：前<script type="math/tex">i</script>个金额的硬币是否凑出总金额j记录在<script type="math/tex">dp[i][j]</script>。</p>
<p>初始值：<script type="math/tex">dp[:][0] = 1, dp[0][:] = 0</script>，凑出总金额0只有1种。</p>
<p>递推：已知<script type="math/tex">dp[<i],dp[i][<j]</script>，如果可以加入第<script type="math/tex">i</script>个元素那么由 [加入 + 不加入]的总凑法计算，否则就等于<script type="math/tex">dp[i-1][j]</script>（不加入的凑法）。</p>
<p>返回：<script type="math/tex">dp[n][amount]</script>。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><p>时间复杂度和空间开销都是<script type="math/tex">O(N\cdot Amount)</script>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change</span><span class="params">(self, amount: int, coins: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="comment"># 前i个硬币总金额为amount的拼法</span></span><br><span class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(amount + <span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(coins) + <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(coins) + <span class="number">1</span>):  <span class="comment"># 拼出总金额为0 = 1</span></span><br><span class="line">            dp[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(coins) + <span class="number">1</span>):  <span class="comment"># 前i个硬币</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, amount + <span class="number">1</span>):</span><br><span class="line">                <span class="comment"># 如果第i个硬币可以加进去</span></span><br><span class="line">                <span class="keyword">if</span> j - coins[i - <span class="number">1</span>] &gt;= <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># 不加进去的凑法 + 加进去的凑发</span></span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j] + dp[i][j - coins[i - <span class="number">1</span>]]</span><br><span class="line">                <span class="keyword">elif</span> j - coins[i - <span class="number">1</span>] &lt; <span class="number">0</span>:</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j]</span><br><span class="line">        <span class="keyword">return</span> dp[len(coins)][amount]</span><br></pre></td></tr></table></figure>
<p>由于当前的<script type="math/tex">dp[i][j]</script>只和<script type="math/tex">dp[i-1:i+1][:]</script>有关，所以可以对空间开销进行优化，优化为<script type="math/tex">O(Amount)</script>的开销。</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode: combination</title>
    <url>/2018/12/25/leetcode/combination/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>给定两个整数 <em>n</em> 和 <em>k</em>，返回 1 … <em>n</em> 中所有可能的 <em>k</em> 个数的组合。</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>用递归的DFS搜索，终止条件是临时数组长度足够，当前所有选择是指针j之后的所有数字中选择一个继续递归，递归完成后回溯，取消该数字的选择。</p>
<p>如果当前临时数组把后续所有数字都加上都无法满足条件则跳出当前搜索。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><p>每次记录答案复杂度为<script type="math/tex">O(k)</script>，这种情况下递归本身的时间复杂度为<script type="math/tex">O((\begin{array}{cols}n\\k\end{array}))</script>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combine</span><span class="params">(self, n, k)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :type k: int</span></span><br><span class="line"><span class="string">        :rtype: List[List[int]]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(res, i)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> len(res) == k:  <span class="comment"># 获得一组答案</span></span><br><span class="line">                ans.append(res.copy())            </span><br><span class="line">                <span class="keyword">return</span> </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, n + <span class="number">1</span>):  <span class="comment"># 遍历所有选择</span></span><br><span class="line">                <span class="keyword">if</span> len(res) + (n - i) &lt; k:  <span class="comment"># 剪枝</span></span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">                res.append(j)</span><br><span class="line">                print(res)</span><br><span class="line">                add(res, j)  <span class="comment"># DFS</span></span><br><span class="line">                res.pop(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        add([], <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DFS</tag>
        <tag>Backtrack</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode: house robber III</title>
    <url>/2019/06/22/leetcode/house-robber-iii/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>所有房子排列成一个二叉树的结构。</p>
<p>如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。</p>
<p>计算在不触动警报的情况下，小偷一晚能够盗取的最高金额。</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>用递归比较选择和不选择当前节点的最大收益，并把结果缓存起来。</p>
<h3 id="三、实现"><a href="#三、实现" class="headerlink" title="三、实现"></a>三、实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line">mem = &#123;&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rob</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">in</span> mem:</span><br><span class="line">            <span class="keyword">return</span> mem[root]</span><br><span class="line">        res = root.val</span><br><span class="line">        </span><br><span class="line">        left_go = self.rob(root.left.left) + self.rob(root.left.right) <span class="keyword">if</span> root.left <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        right_go = self.rob(root.right.left) + self.rob(root.right.right) <span class="keyword">if</span> root.right <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        left = self.rob(root.left) <span class="keyword">if</span> root.left <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        right = self.rob(root.right) <span class="keyword">if</span> root.right <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        total = max(res + left_go + right_go, left + right)</span><br><span class="line">        mem[root] = total</span><br><span class="line">        <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DFS</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode: DP</title>
    <url>/2019/06/24/leetcode/DP/</url>
    <content><![CDATA[<h2 id="最大化网格幸福感"><a href="#最大化网格幸福感" class="headerlink" title="最大化网格幸福感"></a>最大化网格幸福感</h2><blockquote>
<p>给你四个整数 m、n、introvertsCount 和 extrovertsCount 。有一个 m x n 网格，和两种类型的人：内向的人和外向的人。总共有 introvertsCount 个内向的人和 extrovertsCount 个外向的人。</p>
<p>请你决定网格中应当居住多少人，并为每个人分配一个网格单元。 注意，不必 让所有人都生活在网格中。</p>
</blockquote>
<h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>每个点都有三种状态：不住人、住内向的人或者住外向的人。</p>
<p>1.状态压缩DP，由于矩阵大小n比较小，用长度为n的三进制数字记录每一行的状态，一共有<script type="math/tex">3^n</script>种状态，预先计算出行内和行间不同状态间的得分变化。再进行回溯就可以，回溯的单位是行，每次选择的范围是<script type="math/tex">3^n</script>。因此时间复杂度为<script type="math/tex">O(3^{2n}\cdot m \cdot n \cdot ic\cdot ec)</script>。</p>
<p>2.插头DP（轮廓线DP），记录上一行当前元素到这一行j - 1个元素一共n个元素的状态，可以在<script type="math/tex">O(1)</script>时间算出行内和行间得分。进行记忆化搜索就可以，搜索的单位是每个点，每次选择的范围是3，时间复杂度是<script type="math/tex">O(3^n\cdot ic\cdot ec\cdot m \cdot n)</script>。</p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>第一种：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMaxGridHappiness_matrix_dp</span><span class="params">(self, m: int, n: int, ic: int, ec: int)</span> -&gt; int:</span></span><br><span class="line">    NO, IC, EC = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">line_extra</span><span class="params">(cur, prev)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> cur == NO <span class="keyword">or</span> prev == NO:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> cur == IC <span class="keyword">and</span> prev == IC:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-60</span></span><br><span class="line">        <span class="keyword">if</span> cur == EC <span class="keyword">and</span> prev == EC:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">40</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当前行状态数量，每个格子是3种，所以是3 ** 6</span></span><br><span class="line">    state_n = <span class="number">3</span> ** n</span><br><span class="line">    <span class="comment"># 用来方便的计算下面的1和2</span></span><br><span class="line">    mask = [[<span class="number">0</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> range(state_n)]</span><br><span class="line">    <span class="comment"># 1.mask的状态对应的ic和ec数量</span></span><br><span class="line">    ic_count = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(state_n)]</span><br><span class="line">    ec_count = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(state_n)]</span><br><span class="line">    <span class="comment"># 2.mask的状态对应的行内和行间得分</span></span><br><span class="line">    line_score = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(state_n)]</span><br><span class="line">    outline_score = [[<span class="number">0</span>] * state_n <span class="keyword">for</span> _ <span class="keyword">in</span> range(state_n)]</span><br><span class="line">    <span class="comment"># dp table</span></span><br><span class="line">    dp = [[[[<span class="number">-1</span>] * (ec + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(ic + <span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(m)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(state_n)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预先计算, O(3 ** n * n)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(state_n):</span><br><span class="line">        cur = i</span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> cur != <span class="number">0</span> <span class="keyword">and</span> j &lt;= n:</span><br><span class="line">            mask[i][j] = cur % <span class="number">3</span></span><br><span class="line">            cur //= <span class="number">3</span></span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 计算ic和ec数量以及行内得分</span></span><br><span class="line">            <span class="keyword">if</span> mask[i][j] != NO:</span><br><span class="line">                <span class="keyword">if</span> mask[i][j] == IC:</span><br><span class="line">                    ic_count[i] += <span class="number">1</span></span><br><span class="line">                    line_score[i] += <span class="number">120</span></span><br><span class="line">                <span class="keyword">elif</span> mask[i][j] == EC:</span><br><span class="line">                    ec_count[i] += <span class="number">1</span></span><br><span class="line">                    line_score[i] += <span class="number">40</span></span><br><span class="line">                <span class="keyword">if</span> j &gt; <span class="number">0</span>: <span class="comment"># 计算行内相邻得分</span></span><br><span class="line">                    line_score[i] += line_extra(mask[i][j], mask[i][j - <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 计算行间得分，O(3 ** n * 3 ** n * n)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(state_n):</span><br><span class="line">        <span class="keyword">for</span> i2 <span class="keyword">in</span> range(i, state_n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                outline_score[i][i2] += line_extra(mask[i][j], mask[i2][j])</span><br><span class="line">            outline_score[i2][i] = outline_score[i][i2]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># O(3 ** n * 3 ** n * m * ic * ec)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(last_mask, row, ic, ec)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> row == m <span class="keyword">or</span> ic + ec == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> dp[last_mask][row][ic][ec] != <span class="number">-1</span>:</span><br><span class="line">            <span class="keyword">return</span> dp[last_mask][row][ic][ec]</span><br><span class="line">        <span class="comment"># 做选择</span></span><br><span class="line">        best = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> range(state_n):</span><br><span class="line">            <span class="comment"># 剪枝</span></span><br><span class="line">            <span class="keyword">if</span> ic_count[state] &gt; ic <span class="keyword">and</span> ec_count[state] &gt; ec:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> ic_count[state] &gt; ic <span class="keyword">or</span> ec_count[state] &gt; ec:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            score = line_score[state] + outline_score[state][last_mask]</span><br><span class="line">            best = max(best, score + dfs(state, row + <span class="number">1</span>, ic - ic_count[state], ec - ec_count[state]))</span><br><span class="line">        dp[last_mask][row][ic][ec] = best</span><br><span class="line">        <span class="keyword">return</span> best</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dfs(<span class="number">0</span>, <span class="number">0</span>, ic, ec)</span><br></pre></td></tr></table></figure>
<p>第二种：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMaxGridHappiness</span><span class="params">(self, m: int, n: int, ic: int, ec: int)</span> -&gt; int:</span></span><br><span class="line">    <span class="comment"># 插头DP（轮廓线DP）</span></span><br><span class="line">    first = <span class="keyword">lambda</span> x: x // (<span class="number">3</span> ** (n - <span class="number">1</span>))</span><br><span class="line">    last = <span class="keyword">lambda</span> x: x % <span class="number">3</span></span><br><span class="line">    x = <span class="keyword">lambda</span> pos: pos / n </span><br><span class="line">    y = <span class="keyword">lambda</span> pos: pos % n</span><br><span class="line">    NO, IC, EC = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预先计算</span></span><br><span class="line">    state_n = <span class="number">3</span> ** n</span><br><span class="line">    elements = &#123;&#125;</span><br><span class="line">    n_states = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(state_n):</span><br><span class="line">        f, l = first(i), last(i)</span><br><span class="line">        elements[i] = (f, l)</span><br><span class="line">        remain = i % (<span class="number">3</span> ** (n - <span class="number">1</span>))</span><br><span class="line">        n_states[i] = (remain * <span class="number">3</span> + NO, remain * <span class="number">3</span> + IC, remain * <span class="number">3</span> + EC)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在位置pos出，选择为choice，轮廓线状态为state的得分，O(1)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cur_score</span><span class="params">(pos, state, choice)</span>:</span></span><br><span class="line">        c_x, c_y = x(pos), y(pos)</span><br><span class="line">        p_1, p_2 = elements[state]</span><br><span class="line">        res = line_extra(choice, p_1)  <span class="comment"># 与上方邻居的额外得分</span></span><br><span class="line">        <span class="keyword">if</span> c_y &gt; <span class="number">0</span>:</span><br><span class="line">            res += line_extra(choice, p_2)  <span class="comment"># 与左侧邻居之间的额外得分</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新轮廓线状态，O(1)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_state</span><span class="params">(state, choice)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> n_states[state][choice]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">line_extra</span><span class="params">(cur, prev)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> cur == NO <span class="keyword">or</span> prev == NO:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> cur == IC <span class="keyword">and</span> prev == IC:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-60</span></span><br><span class="line">        <span class="keyword">if</span> cur == EC <span class="keyword">and</span> prev == EC:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">40</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># O(3 ** n * m * ic * ec)</span></span><br><span class="line"><span class="meta">    @lru_cache(None)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(pos, prev_states, ic, ec)</span>:</span></span><br><span class="line">        <span class="comment"># 到达边界，后续score为0</span></span><br><span class="line">        <span class="keyword">if</span> pos == m * n <span class="keyword">or</span> ic + ec == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 做选择</span></span><br><span class="line">        <span class="comment"># 在当前位置啥也不放</span></span><br><span class="line">        state = next_state(prev_states, NO)</span><br><span class="line">        best = <span class="number">0</span> + dfs(pos + <span class="number">1</span>, state, ic, ec)</span><br><span class="line">        <span class="comment"># 放一个内向的</span></span><br><span class="line">        <span class="keyword">if</span> ic &gt; <span class="number">0</span>:</span><br><span class="line">            state = next_state(prev_states, IC)</span><br><span class="line">            score = <span class="number">120</span> + cur_score(pos, prev_states, IC)</span><br><span class="line">            best = max(best, score + dfs(pos + <span class="number">1</span>, state, ic - <span class="number">1</span>, ec))</span><br><span class="line">        <span class="comment"># 放一个外向的</span></span><br><span class="line">        <span class="keyword">if</span> ec &gt; <span class="number">0</span>:</span><br><span class="line">            state = next_state(prev_states, EC)</span><br><span class="line">            score = <span class="number">40</span> + cur_score(pos, prev_states, EC)</span><br><span class="line">            best = max(best, score + dfs(pos + <span class="number">1</span>, state, ic, ec - <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> best</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dfs(<span class="number">0</span>, <span class="number">0</span>, ic, ec)</span><br></pre></td></tr></table></figure>
<h2 id="k个节点的覆盖集合最小代价"><a href="#k个节点的覆盖集合最小代价" class="headerlink" title="k个节点的覆盖集合最小代价"></a>k个节点的覆盖集合最小代价</h2><blockquote>
<p>二叉树上找到一个k个节点组成的集合，使得每一条边都与该集合中至少一个节点相连接。</p>
<p>如果存在这样的集合，返回最小的代价，否则返回null。</p>
</blockquote>
<h3 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h3><p>对二叉树中某个点i来说，要保证一定有边与它相连接，那么有两种状态：该点在集合中or该点不在集合中。</p>
<ul>
<li>i未被选择且以i为根节点的子树满足要求时的最低cost和对应的节点weight，用dp[i][0]表示；</li>
<li>i被选择且以i为根节点的子树满足要求时的最低cost和对应的节点weight，用dp[i][1]表示。</li>
</ul>
<h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><p>树形DP实现。时间复杂度<script type="math/tex">O(|V| * k + |V| * log |V| + k) → O(|V| * (k + log|V|))</script>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> Tree.BTree <span class="keyword">import</span> Tree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">left_c</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">right_c</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x + <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vertex_cover</span><span class="params">(self, tree: List, k: int)</span>:</span></span><br><span class="line">        n = len(tree)</span><br><span class="line">        <span class="keyword">if</span> n &lt; k:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        dp = [[[<span class="number">0</span>, []] <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># O(|V| * k)，需要对每个节点遍历一次，由于需要进行列表的extend操作O(len(list2))开销，所以上限开销为←。</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">DP</span><span class="params">(curr: int)</span>:</span></span><br><span class="line">            <span class="comment"># is_leaf = all([tree[child] == '#' for child in [left_c(curr), right_c(curr)]])</span></span><br><span class="line">            <span class="comment"># 如果选当前节点或者不选当前节点的初始开销总和和weights</span></span><br><span class="line">            dp[curr][<span class="number">1</span>] = [tree[curr], [tree[curr]]]</span><br><span class="line">            dp[curr][<span class="number">0</span>] = [<span class="number">0</span>, []]</span><br><span class="line">            <span class="keyword">for</span> child <span class="keyword">in</span> [left_c(curr), right_c(curr)]:</span><br><span class="line">                <span class="keyword">if</span> child &lt; n <span class="keyword">and</span> tree[child] != <span class="string">'#'</span>:  <span class="comment"># 不为空节点</span></span><br><span class="line">                    DP(child)</span><br><span class="line">                    <span class="keyword">if</span> dp[child][<span class="number">1</span>][<span class="number">0</span>] &lt; dp[child][<span class="number">0</span>][<span class="number">0</span>]:</span><br><span class="line">                        dp[curr][<span class="number">1</span>][<span class="number">0</span>] += dp[child][<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">                        dp[curr][<span class="number">1</span>][<span class="number">1</span>] += dp[child][<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        dp[curr][<span class="number">1</span>][<span class="number">0</span>] += dp[child][<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">                        dp[curr][<span class="number">1</span>][<span class="number">1</span>] += dp[child][<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">                    dp[curr][<span class="number">0</span>][<span class="number">0</span>] += dp[child][<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">                    dp[curr][<span class="number">0</span>][<span class="number">1</span>] += dp[child][<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        DP(<span class="number">0</span>)  <span class="comment"># 从root开始</span></span><br><span class="line">        cost, weights = dp[<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">if</span> dp[<span class="number">0</span>][<span class="number">0</span>] &lt; dp[<span class="number">0</span>][<span class="number">1</span>] <span class="keyword">else</span> dp[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> len(weights) &gt; k:  <span class="comment"># 说明最小覆盖节点数要大于k，无法满足要求，返回None</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> len(weights) == k:  <span class="comment"># 如果恰好满足要求，直接返回最小开销和</span></span><br><span class="line">            <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">        <span class="comment"># O(|V|)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">traverse</span><span class="params">(curr: int)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> curr == <span class="string">'#'</span>:</span><br><span class="line">                <span class="keyword">return</span> []</span><br><span class="line">            res = [tree[curr]]</span><br><span class="line">            left = traverse(left_c(curr)) <span class="keyword">if</span> left_c(curr) &lt; n <span class="keyword">and</span> tree[left_c(curr)] != <span class="string">'#'</span> <span class="keyword">else</span> []</span><br><span class="line">            right = traverse(right_c(curr)) <span class="keyword">if</span> right_c(curr) &lt; n <span class="keyword">and</span> tree[right_c(curr)] != <span class="string">'#'</span> <span class="keyword">else</span> []</span><br><span class="line">            <span class="keyword">return</span> res + left + right</span><br><span class="line"></span><br><span class="line">        all_weights = traverse(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> len(all_weights) &lt; k:  <span class="comment"># 这里是由于存储的方式的问题，所以要判断两次，这次判断的是|V|是否小于k</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="comment"># O(|V| * k)</span></span><br><span class="line">        print(weights)</span><br><span class="line">        <span class="keyword">for</span> weight <span class="keyword">in</span> weights:</span><br><span class="line">            all_weights.remove(weight)  <span class="comment"># O(|V|)</span></span><br><span class="line">        <span class="comment"># O(|V| * log|V|)</span></span><br><span class="line">        all_weights.sort()</span><br><span class="line">        <span class="comment"># O(k)</span></span><br><span class="line">        <span class="keyword">while</span> len(weights) &lt; k:</span><br><span class="line">            weights.append(all_weights.pop(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sum(weights)</span><br></pre></td></tr></table></figure>
<h2 id="Edit-Distance"><a href="#Edit-Distance" class="headerlink" title="Edit Distance"></a>Edit Distance</h2><p>给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。</p>
<p>你可以对一个单词进行如下三种操作：</p>
<p>插入一个字符<br>删除一个字符<br>替换一个字符</p>
<p>即NLP中计算编辑距离。</p>
<h3 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h3><p>使用递归式的自顶向下的动态规划，并增加重叠子问题的缓存。</p>
<p>自底向上的动态规划，1）状态数组存储的是<script type="math/tex">word1[:i]</script>和<script type="math/tex">word2[:j]</script>的最小编辑距离，2）起始状态就是当其中一个数组为空，编辑距离是另一个数组的长度，3）计算当前时刻的状态值如果当前时刻字符不相等就选择增删改中最小的（进行了这些操作之后肯定会变成已知的一种状态）。</p>
<p>状态列表可以优化空间到O(m + n)。</p>
<h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><p>递归的方式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance</span><span class="params">(self, word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        mem = &#123;&#125;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(i, j)</span>:</span></span><br><span class="line">            key = <span class="string">'&#123;&#125;-&#123;&#125;'</span>.format(i, j)</span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="number">0</span>:   <span class="keyword">return</span> j + <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> j &lt; <span class="number">0</span>:   <span class="keyword">return</span> i + <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> mem:</span><br><span class="line">                <span class="keyword">return</span> mem[key]</span><br><span class="line">            <span class="keyword">if</span> word1[i] == word2[j]:</span><br><span class="line">                res = helper(i - <span class="number">1</span>, j - <span class="number">1</span>)</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res = min(</span><br><span class="line">                    helper(i - <span class="number">1</span>, j - <span class="number">1</span>) + <span class="number">1</span>,  <span class="comment"># 替换</span></span><br><span class="line">                    helper(i, j - <span class="number">1</span>) + <span class="number">1</span>, <span class="comment"># 插入word1的单词</span></span><br><span class="line">                    helper(i - <span class="number">1</span>, j) + <span class="number">1</span> <span class="comment"># 删除word1的单词</span></span><br><span class="line">                )</span><br><span class="line">            mem[key] = res</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        <span class="keyword">return</span> helper(len(word1) - <span class="number">1</span>, len(word2) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>改写的迭代求解。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance</span><span class="params">(self, word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        m, n = len(word1), len(word2)</span><br><span class="line"></span><br><span class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n + <span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(m + <span class="number">1</span>)] <span class="comment"># (m + 1) * (n + 1)，多一个模拟i - 1</span></span><br><span class="line">        dp[<span class="number">0</span>] = [j <span class="keyword">for</span> j <span class="keyword">in</span> range(n + <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m + <span class="number">1</span>):</span><br><span class="line">            dp[i][<span class="number">0</span>] = i</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> word1[i - <span class="number">1</span>] == word2[j - <span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = min(</span><br><span class="line">                        dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>,</span><br><span class="line">                        dp[i][j - <span class="number">1</span>] + <span class="number">1</span>,</span><br><span class="line">                        dp[i - <span class="number">1</span>][j] + <span class="number">1</span></span><br><span class="line">                    )</span><br><span class="line">        <span class="keyword">return</span> dp[m][n]</span><br></pre></td></tr></table></figure></p>
<p>两种解法时间复杂度都是O(mn)，空间复杂度第二种是O(mn)但是可以优化。</p>
<h2 id="连通两组点的最小成本"><a href="#连通两组点的最小成本" class="headerlink" title="连通两组点的最小成本"></a>连通两组点的最小成本</h2><blockquote>
<p>给你两组点，其中第一组中有 size1 个点，第二组中有 size2 个点，且 size1 &gt;= size2 。</p>
<p>任意两点间的连接成本 cost 由大小为 size1 x size2 矩阵给出，其中 cost[i][j] 是第一组中的点 i 和第二组中的点 j 的连接成本。如果两个组中的每个点都与另一组中的一个或多个点连接，则称这两组点是连通的。换言之，第一组中的每个点必须至少与第二组中的一个点连接，且第二组中的每个点必须至少与第一组中的一个点连接。</p>
<p>返回连通两组点所需的最小成本。</p>
<ul>
<li><code>1 &lt;= size1, size2 &lt;= 12</code></li>
</ul>
</blockquote>
<h3 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h3><p>首先，在搜索完整路径的情况下，某一个group中两个节点连接的顺序不影响最终的结果。一次选择随机一个节点进行连接就行。</p>
<p>直接DP的话会超时，所以把<strong>状态进行压缩</strong>用int的每一位来表示（长度不长），两个state每一位分别表示两个group中对应的节点是否已经连接，用位操作进行状态的判断和更新。</p>
<p>自顶向下的记忆化搜索。</p>
<h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">connectTwoGroups</span><span class="params">(self, cost: List[List[int]])</span> -&gt; int:</span></span><br><span class="line">        s1, s2 = len(cost), len(cost[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 因为从group1开始选择，所以先记录group2中每个节点cost最小的选择</span></span><br><span class="line">        min_group_2 = [min(cost[i][j] <span class="keyword">for</span> i <span class="keyword">in</span> range(s1)) <span class="keyword">for</span> j <span class="keyword">in</span> range(s2)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># DP[i, j]返回的是还有i和j代表状态的节点没有连接时的cost</span></span><br><span class="line">        <span class="comment"># 自顶向下，DP[0, 0] = 0，记忆化搜索</span></span><br><span class="line">        <span class="comment"># 因为每个组都必须要连接到另一个组里面的节点，所以先从节点多的组group1开始</span></span><br><span class="line">        <span class="comment"># 每一次选择没有连接的一个点，并遍历所有可能的连接方式和记录ans</span></span><br><span class="line"><span class="meta">        @lru_cache(None)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dp</span><span class="params">(state1, state2)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> state1 == <span class="number">0</span> <span class="keyword">and</span> state2 == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            ans = float(<span class="string">'inf'</span>)</span><br><span class="line">            <span class="keyword">if</span> state1 != <span class="number">0</span>:  <span class="comment"># s1里面还有没有连接的点</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(s1):</span><br><span class="line">                    <span class="keyword">if</span> state1 &amp; (<span class="number">1</span> &lt;&lt; i): <span class="comment"># 第i个点还没有连接，对它向下递归遍历并跳出</span></span><br><span class="line">                        <span class="keyword">for</span> j <span class="keyword">in</span> range(s2):</span><br><span class="line">                            new_s1, new_s2 = state1 &amp; ~(<span class="number">1</span> &lt;&lt; i), state2  &amp; ~(<span class="number">1</span> &lt;&lt; j)</span><br><span class="line">                            <span class="comment"># 更新开销为所有选择下的最小开销</span></span><br><span class="line">                            ans = min(ans, cost[i][j] + dp(new_s1, new_s2))</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># group1已经选完了，group2里的节点直接选择预先记录的最小cost边就行</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(s2):</span><br><span class="line">                    <span class="keyword">if</span> state2 &amp; (<span class="number">1</span> &lt;&lt; j):</span><br><span class="line">                        new_s1, new_s2 = <span class="number">0</span>, state2 &amp; ~(<span class="number">1</span> &lt;&lt; j)</span><br><span class="line">                        ans = min(ans, min_group_2[j] + dp(new_s1, new_s2))</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">return</span> ans</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">        state1, state2 = (<span class="number">1</span> &lt;&lt; s1) - <span class="number">1</span>, (<span class="number">1</span> &lt;&lt; s2) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> dp(state1, state2)</span><br></pre></td></tr></table></figure>
<h2 id="树中距离之和"><a href="#树中距离之和" class="headerlink" title="树中距离之和"></a>树中距离之和</h2><h3 id="思路-4"><a href="#思路-4" class="headerlink" title="思路"></a>思路</h3><p>首先进行第一次搜索，找到所有节点的子图节点到该节点的距离和以及包含的子图节点个数。</p>
<p>然后第二次DFS搜索，自顶向下用父节点的n_dis和子节点包含的子图节点数量更新距离和信息。</p>
<p>题目默认0可以作为根节点，但是因为是无向图，所以遍历的时候需要加上双向边信息并用set记录已遍历的节点。</p>
<h3 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sumOfDistancesInTree</span><span class="params">(self, N: int, edges: List[List[int]])</span> -&gt; List[int]:</span></span><br><span class="line">        edge = defaultdict(list)</span><br><span class="line">        <span class="comment"># p = [-1 for _ in range(N)]</span></span><br><span class="line">        n_node = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)]</span><br><span class="line">        n_dist = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)]</span><br><span class="line">        <span class="keyword">for</span> s, e <span class="keyword">in</span> edges:</span><br><span class="line">            edge[s].append(e)</span><br><span class="line">            edge[e].append(s)  <span class="comment"># 默认0为根节点，但是无向图要记录双向信息</span></span><br><span class="line">            <span class="comment"># p[e] = s</span></span><br><span class="line"></span><br><span class="line">        root = <span class="number">0</span></span><br><span class="line">        visited = set()</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(cur)</span>:</span></span><br><span class="line">            visited.add(cur)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> edge[cur]:</span><br><span class="line">                n_node[cur] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">            n_node_cur, n_dist_cur = <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> each <span class="keyword">in</span> edge[cur]:</span><br><span class="line">                <span class="keyword">if</span> each <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                    n_node_c, n_dist_c = dfs(each)</span><br><span class="line">                    n_node_cur += n_node_c</span><br><span class="line">                    n_dist_cur += n_dist_c</span><br><span class="line">            n_dist_cur += n_node_cur - <span class="number">1</span></span><br><span class="line">            n_node[cur] = n_node_cur</span><br><span class="line">            n_dist[cur] = n_dist_cur</span><br><span class="line">            <span class="keyword">return</span> n_node_cur, n_dist_cur</span><br><span class="line"></span><br><span class="line">        dfs(root)  <span class="comment"># 第一次遍历，记录每个节点所在子图节点到该节点的距离</span></span><br><span class="line">        <span class="comment"># print(f'&#123;n_node&#125;\n&#123;n_dist&#125;')</span></span><br><span class="line"></span><br><span class="line">        res = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)]</span><br><span class="line">        visited = set()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二次遍历，从顶向下，用父节点的n_dis和子节点包含的子图节点数量更新信息</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">calcu_dis</span><span class="params">(cur, parent)</span>:</span></span><br><span class="line">            visited.add(cur)</span><br><span class="line">            n_dist_cur = parent - n_node[cur] + (N - n_node[cur]) <span class="keyword">if</span> cur != root <span class="keyword">else</span> parent</span><br><span class="line">            res[cur] = n_dist_cur</span><br><span class="line">            <span class="keyword">for</span> each <span class="keyword">in</span> edge[cur]:</span><br><span class="line">                <span class="keyword">if</span> each <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                    calcu_dis(each, n_dist_cur)</span><br><span class="line"></span><br><span class="line">        calcu_dis(root, n_dist[root])</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="交错字符串"><a href="#交错字符串" class="headerlink" title="交错字符串"></a>交错字符串</h2><blockquote>
<p>给定三个字符串 <code>s1</code>、<code>s2</code>、<code>s3</code>，请你帮忙验证 <code>s3</code> 是否是由 <code>s1</code> 和 <code>s2</code> <strong>交错</strong> 组成的。</p>
</blockquote>
<h3 id="思路-5"><a href="#思路-5" class="headerlink" title="思路"></a>思路</h3><p>dp[i][j] 表示s1[:i]和s2[:j]可以交错为s3[:i + j - 1]。</p>
<p>可以用滚动数组优化空间复杂度。</p>
<h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isInterleave</span><span class="params">(self, s1: str, s2: str, s3: str)</span> -&gt; bool:</span></span><br><span class="line">        n, m, x = len(s1), len(s2), len(s3)</span><br><span class="line">        <span class="keyword">if</span> n + m != x:  <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="comment"># dp[i][j] 表示s1[:i]和s2[:j]可以交错为s3[:i + j - 1]，空间O(m * n)</span></span><br><span class="line">        dp = [[<span class="literal">True</span>] * (m + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n + <span class="number">1</span>)]</span><br><span class="line">        <span class="comment"># 边缘条件，O(m + n)</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> nn <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            dp[nn][<span class="number">0</span>] = dp[nn - <span class="number">1</span>][<span class="number">0</span>] <span class="keyword">and</span> (s1[nn - <span class="number">1</span>] == s3[nn - <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">for</span> mm <span class="keyword">in</span> range(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">0</span>][mm] = dp[<span class="number">0</span>][mm - <span class="number">1</span>] <span class="keyword">and</span> (s2[mm - <span class="number">1</span>] == s3[mm - <span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># O(n * m)</span></span><br><span class="line">        <span class="keyword">for</span> nn <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> mm <span class="keyword">in</span> range(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">                dp[nn][mm] = dp[nn - <span class="number">1</span>][mm] <span class="keyword">and</span> (s1[nn - <span class="number">1</span>] == s3[nn - <span class="number">1</span> + mm]) <span class="keyword">or</span> dp[nn][mm - <span class="number">1</span>] <span class="keyword">and</span> (s2[mm - <span class="number">1</span>] == s3[nn + mm - <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> dp[n][m]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DP</tag>
        <tag>树形DP</tag>
        <tag>插头DP</tag>
        <tag>轮廓线DP</tag>
        <tag>记忆化搜索</tag>
        <tag>状态压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Add Binary</title>
    <url>/2015/09/14/leetcode/leetcode-Add-Binary/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Given two binary strings, return their sum (also a binary string).</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>给两个二进制数的字符串，以二进制字符串形式返回他们的和。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.先转换为十进制，再转换为二进制字符串返回，使用了Python的BIF：<code>map()</code>和<code>reduce()</code></li>
</ul>
<p>尽管map和reduce的时间开销无法获知，但通过使用Python的BIF（<strong>built-in functions</strong>内置函数）所获得的性能通常要高于自己直接编写Python代码</p>
<p>时间开销:$O(n)$,空间开销:$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addBinary</span><span class="params">(a, b)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type a: str</span></span><br><span class="line"><span class="string">        :type b: str</span></span><br><span class="line"><span class="string">        :rtype: str</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = Solution.b_int(a)</span><br><span class="line">        y = Solution.b_int(b)</span><br><span class="line">        <span class="keyword">return</span> str(bin(x + y))[<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">b_int</span><span class="params">(a)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> reduce(<span class="keyword">lambda</span> x, y: (x &lt;&lt; <span class="number">1</span>) + y, list(map(int, a)))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Best Time to Buy and Sell Stock</title>
    <url>/2015/09/23/leetcode/leetcode-Best-Time-to-Buy-and-Sell-Stock/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Say you have an array for which the ith element is the price of a given stock on day i.</p>
<p>Design an algorithm to find the maximum profit. You may complete as many transactions as you like (ie, buy one and sell one share of the stock multiple times). However, you may not engage in multiple transactions at the same time (ie, you must sell the stock before you buy again).</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>给出代表股票每天价格的数组，在最多只能进行一次交易的情况下（买卖一次股票），求最大收益。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.动态规划，找出状态方程$f(n) = max(n - min(a[:n - 1]), f(n - 1))$，并存储状态量$min(a[n])$即可</li>
</ul>
<p>时间开销:O(n),空间开销:O(1),结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> len(prices) &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">MIN, res = min(prices[<span class="number">0</span>], prices[<span class="number">1</span>]), max(<span class="number">0</span>, prices[<span class="number">1</span>] - prices[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(prices)):</span><br><span class="line">    res = max(res, prices[i] - MIN)</span><br><span class="line">    MIN = min(MIN, prices[i])</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Bulls and Cows</title>
    <url>/2015/11/16/leetcode/leetcode-Bulls-and-Cows/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>You are playing the following <strong>Bulls and Cows</strong> game with your friend: You write down a number and ask your friend to guess what the number is. Each time your friend makes a guess, you provide a hint that indicates how many digits in said guess match your secret number exactly in both digit and position (called “bulls”) and how many digits match the secret number but locate in the wrong position (called “cows”). Your friend will use successive guesses and hints to eventually derive the secret number.</p>
<p>For example:</p>
<p>Secret number:  “1807”<br>Friend’s guess: “7810”<br>Hint: <code>1</code> bull and <code>3</code> cows. (The bull is <code>8</code>, the cows are <code>0</code>, <code>1</code> and <code>7</code>.)</p>
<p>Write a function to return a hint according to the secret number and friend’s guess, use <code>A</code> to indicate the bulls and <code>B</code> to indicate the cows. In the above example, your function should return “<code>1A3B</code>“.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>函数应当返回位置和数字都猜对的数量（<code>Bulls</code>），以及只猜对数字没猜对位置的数量（<code>Cows</code>）。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.首先使用map函数比较位置和数字都正确的数量，然后使用Hash表对正确数字串进行hash，比较两个串中相同数字出现的次数再减去bull就是cow的数量</li>
</ul>
<p>时间开销:$O(n)$,空间开销:$O(n)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cow = <span class="number">0</span></span><br><span class="line">bull = sum(map(operator.eq, secret, guess))</span><br><span class="line"></span><br><span class="line">num_dict = &#123;str(num): [] <span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">10</span>)&#125;</span><br><span class="line"><span class="keyword">for</span> inx, num <span class="keyword">in</span> enumerate(secret):</span><br><span class="line">    num_dict[num].append(inx)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> inx, num <span class="keyword">in</span> enumerate(guess):</span><br><span class="line">    <span class="keyword">if</span> num_dict[num]:</span><br><span class="line">        cow += <span class="number">1</span></span><br><span class="line">        num_dict[num].pop()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="string">'%dA%dB'</span> % (bull, cow - bull)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>Hash Table</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Count Primes</title>
    <url>/2015/09/06/leetcode/leetcode-Count-Primes/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Count the number of prime numbers less than a non-negative number, <strong>n</strong>.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>找到小于n的素数的个数。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.对于每个小于n的数，判断是否是素数。判断n是否素数的方法为分别除以2-[n-1]范围内的数看是否存在可以整除的数，判断范围可缩小为2-[n/2]</li>
</ul>
<p>时间开销:$O(n^2)$,空间开销:$O(1)$,结果:<code>TLE</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isPrime</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,n // <span class="number">2</span> + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> n % i == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countPrimes</span><span class="params">(n)</span>:</span></span><br><span class="line">    res = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> Solution.isPrime(i):</span><br><span class="line">            res += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<ul>
<li>2.改进<code>IsPrime(n)</code>，若数n对2-[sqrt(n) + 1]的数均无法整除，那么n为素数，这样该函数时间复杂度优化为$O(\sqrt n)$</li>
</ul>
<p>时间开销:$O(n^{1.5})$,空间开销:$O(1)$,结果:<code>TLE</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isPrime</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    R = int(sqrt(n) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,R):</span><br><span class="line">        <span class="keyword">if</span> n % i == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<ul>
<li>3.埃拉托斯特尼筛法</li>
</ul>
<p>时间开销:$O(n \log {\log n})$,空间开销:$O(n)$,结果:<code>AC</code></p>
<p>过程如下图，对于输入n，对于从2开始的每一个素数p，将$p^p,p^2+p,p^2+2p,…,p^2+kp \in (2,n)$过滤出来，直到对于下一个素数p：$p^2 &gt;= n$：</p>
<p><img src="https://leetcode.com/static/images/solutions/Sieve_of_Eratosthenes_animation.gif" alt="Sieve of Eratosthenes"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">primeList = [<span class="literal">True</span>] * n</span><br><span class="line">primeList[<span class="number">0</span>],primeList[<span class="number">1</span>] = <span class="literal">False</span>,<span class="literal">False</span></span><br><span class="line">i = <span class="number">2</span></span><br><span class="line">MAX = n - <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> i * i &lt;= MAX:</span><br><span class="line">    <span class="keyword">if</span> primeList[i]:</span><br><span class="line">        delete = i * i</span><br><span class="line">        <span class="keyword">while</span> delete &lt;= MAX:</span><br><span class="line">            primeList[delete] = <span class="literal">False</span></span><br><span class="line">            delete += i</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> primeList.count(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>题解中思路来自:<br><a href="https://leetcode.com/problems/count-primes/" target="_blank" rel="noopener">Leetcode-Hint</a></p>
<p><a href="http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes" target="_blank" rel="noopener">埃拉托斯特尼筛法</a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Compare Version Numbers</title>
    <url>/2015/09/09/leetcode/leetcode-Compare-Version-Numbers/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Compare two version numbers version1 and version2.<br>If version1 &gt; version2 return 1, if version1 &lt; version2 return -1, otherwise return 0.</p>
<p>You may assume that the version strings are non-empty and contain only digits and the . character.<br>The . character does not represent a decimal point and is used to separate number sequences.<br>For instance, <code>2.5</code> is not “two and a half” or “half way to version three”, it is the fifth second-level revision of the second first-level revision.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>判断两个版本号<code>String</code>的大小，例如1.0.1小于1.1.0，输入不为空且只含有数字和<code>.</code>符号。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.分割字符串并比较即可</li>
</ul>
<p>时间开销:$O(n)$,空间开销:$O(n)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v1,v2 = version1.split(<span class="string">'.'</span>),version2.split(<span class="string">'.'</span>)</span><br><span class="line">v1 += [<span class="number">0</span>] * (len(v2) - len(v1))</span><br><span class="line">v2 += [<span class="number">0</span>] * (len(v1) - len(v2))</span><br><span class="line">length = len(v1)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">    <span class="keyword">if</span> int(v1[i]) == int(v2[i]):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">elif</span> int(v1[i]) &lt; int(v2[i]):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<ul>
<li>2.简洁版，使用Python内置的_<em>builtin_</em>中的map函数简化代码，Python3中的map函数是惰性求值因此需要手动用返回的map对象来构造一个list</li>
</ul>
<p>时间开销:$O(n)$,空间开销:$O(n)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v1, v2 = list(map(int,version1.split(<span class="string">'.'</span>))), list(map(int,version2.split(<span class="string">'.'</span>)))</span><br><span class="line">v1 += [<span class="number">0</span>] * (len(v2) - len(v1))</span><br><span class="line">v2 += [<span class="number">0</span>] * (len(v1) - len(v2))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> v1 &gt; v2 <span class="keyword">else</span> (<span class="number">-1</span> <span class="keyword">if</span> v1 &lt; v2 <span class="keyword">else</span> <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>题解中思路2来自:<a href="https://leetcode.com/discuss/46993/5-line-python-solution" target="_blank" rel="noopener">Leetcode-Discuss</a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Find Minimum in Rotated Sorted Array II</title>
    <url>/2015/10/21/leetcode/leetcode-Find-Minimum-in-Rotated-Sorted-Array-II/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Suppose a sorted array is rotated at some pivot unknown to you beforehand.</p>
<p>(i.e., <code>0 1 2 4 5 6 7</code> might become <code>4 5 6 7 0 1 2</code>).</p>
<p>Find the minimum element.</p>
<p>The array may contain duplicates.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>一个排序后的数组，在某个点被旋转（该点后的<code>n</code>个元素按原有顺序插入到数组前<code>n</code>位）。在该数组中找出最小元素并返回。该数组可能存在重复元素。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.二分查找；由于可能出现重复元素，因此当出现中间元素和右端元素相同时，无法判断最小值位于哪个子区间，因此最坏时间开销为$O(n)$</li>
</ul>
<p>时间开销:最坏$O(n)$,空间开销:$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMin</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">return</span> self.findMinInternal(nums, <span class="number">0</span>, len(nums) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMinInternal</span><span class="params">(self, nums, l, r)</span>:</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            m = (l + r) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[m] &gt; nums[r]:</span><br><span class="line">                l = m + <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> nums[m] == nums[r]:</span><br><span class="line">                <span class="keyword">return</span> min(self.findMinInternal(nums, l, m), self.findMinInternal(nums, m + <span class="number">1</span>, r))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                r = m</span><br><span class="line">        <span class="keyword">return</span> nums[l]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>Binary Search</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:H-Index II</title>
    <url>/2015/09/04/leetcode/leetcode-H-Index-II/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p><strong>Follow up</strong> for H-Index: What if the <code>citations</code> array is sorted in ascending order? Could you optimize your algorithm?</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>若引用次数数组已经增序排列，如何优化算法？时间开销要求$O(\log n)$</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.二分查找即可</li>
</ul>
<p>时间开销:$O(\log n)$,空间开销:$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L, R ,length = <span class="number">0</span>, len(citations) - <span class="number">1</span>,len(citations)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> L &lt;= R:</span><br><span class="line">    mid = L + R &gt;&gt; <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> length - mid &gt; citations[mid]:</span><br><span class="line">        L = mid + <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> length - mid &lt; citations[mid]:</span><br><span class="line">        R = mid - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> citations[mid]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> length - L</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Game of Life</title>
    <url>/2015/10/04/leetcode/leetcode-Game-of-Life/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>According to the Wikipedia’s article: “<strong>The Game of Life</strong>, also known simply as <strong>Life</strong>, is a cellular automaton devised by the British mathematician John Horton Conway in 1970.”</p>
<p>Given a board with m by n cells, each cell has an initial state live (1) or dead (0). Each cell interacts with its <code>eight neighbors</code> (horizontal, vertical, diagonal) using the following four rules (taken from the above Wikipedia article):</p>
<ul>
<li>1.Any live cell with fewer than two live neighbors dies, as if caused by under-population.</li>
<li>2.Any live cell with two or three live neighbors lives on to the next generation.</li>
<li>3.Any live cell with more than three live neighbors dies, as if by over-population..</li>
<li>4.Any dead cell with exactly three live neighbors becomes a live cell, as if by reproduction.</li>
</ul>
<p>Write a function to compute the next state (after one update) of the board given its current state.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>给予一个<code>m</code>*<code>n</code>的数组，其中每个元素代表一个细胞，元素为1则细胞为活细胞，否则为死细胞。根据当前所有八个邻居细胞的状态对每个细胞下一状态进行更新，更新规则为以下四条：</p>
<ul>
<li>1.活细胞周围活着的邻居细胞少于2个则该细胞由于”人口过少”死去；</li>
<li>2.活细胞周围邻居细胞为2或者3个则该细胞继续存活；</li>
<li>3.活细胞周围活着的邻居细胞多于3个则该细胞由于”人口过多”死去；</li>
<li>4.死细胞周围活着的邻居细胞恰好为3个则该细胞变为活细胞。</li>
</ul>
<p>在原有数组上修改数据。所有更新必须同时进行：不能更新一部分细胞的状态然后使用它们的新状态更新其他细胞的状态。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.使用额外的一个数组来保存原有状态</li>
</ul>
<p>时间开销:$O(mn)$,空间开销:$O(mn)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gameOfLife</span><span class="params">(board)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type board: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: void Do not return anything, modify board in-place instead.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">find_live_nei</span><span class="params">(board, i, j)</span>:</span></span><br><span class="line">            res = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> ((i - <span class="number">1</span>, j - <span class="number">1</span>), (i - <span class="number">1</span>, j), (i - <span class="number">1</span>, j + <span class="number">1</span>), (i, j - <span class="number">1</span>), (i, j + <span class="number">1</span>), (i + <span class="number">1</span>, j - <span class="number">1</span>), (i + <span class="number">1</span>, j),</span><br><span class="line">                         (i + <span class="number">1</span>, j + <span class="number">1</span>)):</span><br><span class="line">                <span class="keyword">if</span> x &gt;= <span class="number">0</span> <span class="keyword">and</span> x &lt; len(board) <span class="keyword">and</span> y &gt;= <span class="number">0</span> <span class="keyword">and</span> y &lt; len(board[<span class="number">0</span>]) <span class="keyword">and</span> board[x][y] % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">                    res += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">        m_board = copy.deepcopy(board)</span><br><span class="line">        X, Y = len(board), len(board[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(X):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(Y):</span><br><span class="line">                live_num = find_live_nei(m_board, i, j)</span><br><span class="line">                <span class="keyword">if</span> board[i][j] == <span class="number">0</span>:</span><br><span class="line">                    board[i][j] = <span class="number">1</span> <span class="keyword">if</span> live_num == <span class="number">3</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">                <span class="keyword">elif</span> live_num &gt; <span class="number">3</span> <span class="keyword">or</span> live_num &lt; <span class="number">2</span>:</span><br><span class="line">                    board[i][j] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<ul>
<li>2.使用额外的一位来同时保存原有的状态和新状态，然后利用移位操作来清除原有状态</li>
</ul>
<p>时间开销:$O(mn)$,空间开销:$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gameOfLife_2</span><span class="params">(board)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">find_live_nei</span><span class="params">(board, i, j)</span>:</span></span><br><span class="line">            res = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> ((i - <span class="number">1</span>, j - <span class="number">1</span>), (i - <span class="number">1</span>, j), (i - <span class="number">1</span>, j + <span class="number">1</span>), (i, j - <span class="number">1</span>), (i, j + <span class="number">1</span>), (i + <span class="number">1</span>, j - <span class="number">1</span>), (i + <span class="number">1</span>, j),</span><br><span class="line">                         (i + <span class="number">1</span>, j + <span class="number">1</span>)):</span><br><span class="line">                <span class="keyword">if</span> x &gt;= <span class="number">0</span> <span class="keyword">and</span> x &lt; len(board) <span class="keyword">and</span> y &gt;= <span class="number">0</span> <span class="keyword">and</span> y &lt; len(board[<span class="number">0</span>]) <span class="keyword">and</span> board[x][y] % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">                    res += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">        X, Y = len(board), len(board[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(X):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(Y):</span><br><span class="line">                live_num = find_live_nei(board, i, j)</span><br><span class="line">                <span class="keyword">if</span> board[i][j] == <span class="number">0</span> <span class="keyword">and</span> live_num == <span class="number">3</span> <span class="keyword">or</span> board[i][j] == <span class="number">1</span> <span class="keyword">and</span> live_num <span class="keyword">in</span> (<span class="number">2</span>,<span class="number">3</span>):</span><br><span class="line">                    board[i][j] |= <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(X):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(Y):</span><br><span class="line">                board[i][j] &gt;&gt;= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>题解中思路2来自:<a href="https://leetcode.com/discuss/62058/python-solution-o-mn-time-o-1-space" target="_blank" rel="noopener">Leetcode:Discuss</a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:House Robber</title>
    <url>/2015/09/09/leetcode/leetcode-House-Robber/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security system connected and <strong>it will automatically contact the police if two adjacent houses were broken into on the same night</strong>.</p>
<p>Given a list of non-negative integers representing the amount of money of each house, determine the maximum amount of money you can rob tonight <strong>without alerting the police</strong>.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>数组中每个数字代表每间房屋获得的金钱数，计算抢劫能够获得的最大金钱数量，相邻的屋子（<strong>数字</strong>）只能有一个被计算在内。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.动态规划，状态转化方程为$f(i) = \max (f(i - 1),f(i - 2) + num_i)$</li>
</ul>
<p>时间开销:$O(n)$,空间开销:$O(n)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">length = len(nums)</span><br><span class="line"><span class="keyword">if</span> length == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">res = [<span class="number">0</span>] * (length + <span class="number">1</span>)</span><br><span class="line">res[<span class="number">1</span>] = nums[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, length + <span class="number">1</span>):</span><br><span class="line">    res[i] = max(res[i - <span class="number">1</span>], res[i - <span class="number">2</span>] + nums[i - <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> res[length]</span><br></pre></td></tr></table></figure>
<ul>
<li>2.观察可知，$f(i) = \max (\sum<em>{k = 1, k *= 2}^i {num_k}, \sum</em>{k = 2,k *= 2}^i {num_k})$，可以将空间开销减少到$O(1)$</li>
</ul>
<p>时间开销:$O(n)$,空间开销:$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">length, odd, even = len(nums), <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2</span>:</span><br><span class="line">        odd = max(odd + nums[i], even)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        even = max(even + nums[i], odd)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> max(odd, even)</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>题解中思路2来自:<a href="http://bookshadow.com/weblog/2015/04/01/leetcode-house-robber/" target="_blank" rel="noopener">书影</a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Implement strStr()</title>
    <url>/2015/09/15/leetcode/leetcode-Implement-strStr/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Implement strStr().</p>
<p>Returns the <em>index</em> of the first occurrence of needle in haystack, or <em>-1</em> if needle is not part of haystack.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>实现strstr字符串匹配函数，若子串无法匹配则返回-1。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.暴力匹配法，时间开销中n为母串的长度，m为子串的长度，指针有回溯，此题暴力匹配即可解决</li>
</ul>
<p>时间开销:$O(nm)$,空间开销:$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len_s, len_l = len(needle), len(haystack)</span><br><span class="line">i, j = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> j &lt; len_s <span class="keyword">and</span> i &lt; len_l:</span><br><span class="line">    <span class="keyword">if</span> haystack[i] == needle[j]:</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        i = i - j + <span class="number">1</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> i - j <span class="keyword">if</span> j == len_s <span class="keyword">else</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>2.KMP算法</p>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm" target="_blank" rel="noopener">KMP</a>算法是一种扫描指针无需回溯的字符串匹配算法；于1977年由<strong>Donald Knuth、Vaughan Pratt、James H. Morris</strong>三人联合发表</p>
</li>
<li><p>对于暴力匹配法，其扫描指针也就是上面的变量<code>i</code>存在着不必要的回溯</p>
</li>
<li><p>通过计算子串当前比较位之前的字符串的<code>最长前缀后缀公共元素长度</code>，也就是该字符串最前面n位和最后面n位相同，可以获取子串的下一个需要比较的位置，扫描指针也无需回溯</p>
</li>
<li><p>计算出子串每一位的<code>最长前缀后缀公共元素最大长度</code>即Next数组，计算方式是通过递推式计算；子串当前位置若与母串不相等，则子串向右移动j-Next[j]位</p>
</li>
<li><p>进一步分析，由于当前的p[j]不匹配时，下一个p[next[j]]若等于p[j]也必定无法匹配，因此这种情况下应设置next[j] = next[next[j]]，这一步对Next数组进行了一些优化</p>
</li>
<li><p>时间复杂度：由于扫描指针无需回溯，因此搜索匹配的时间复杂度是O(n)，算上计算Next数组的时间复杂度O(m)，KMP算法总的时间复杂度是O(m+n)</p>
</li>
</ul>
</li>
</ul>
<p>时间开销:$O(n+m)$,空间开销:$O(m)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strstr_KMP</span><span class="params">(haystack, needle)</span>:</span></span><br><span class="line">    len_s, len_l = len(needle), len(haystack)</span><br><span class="line">    i, j = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    next = Solution.genNext([k <span class="keyword">for</span> k <span class="keyword">in</span> needle])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> j &lt; len_s <span class="keyword">and</span> i &lt; len_l:</span><br><span class="line">        <span class="keyword">if</span> haystack[i] == needle[j]:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> next[j] &gt;= <span class="number">0</span>:</span><br><span class="line">            j = next[j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> i - j <span class="keyword">if</span> j == len_s <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genNext</span><span class="params">(needle)</span>:</span></span><br><span class="line">    next = [<span class="number">0</span>, ] * len(needle)</span><br><span class="line">    <span class="keyword">if</span> len(needle):</span><br><span class="line">        next[<span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(needle)):</span><br><span class="line">        <span class="keyword">if</span> needle[next[i - <span class="number">1</span>]] == needle[i - <span class="number">1</span>]:</span><br><span class="line">            next[i] = next[i - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tmp = next[i - <span class="number">1</span>]</span><br><span class="line">            <span class="keyword">while</span> next[tmp] != <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">if</span> needle[next[tmp]] == needle[i - <span class="number">1</span>]:</span><br><span class="line">                    next[i] = next[tmp] + <span class="number">1</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    tmp = next[tmp]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(next)):</span><br><span class="line">        <span class="keyword">if</span> needle[i] == needle[next[i]]:</span><br><span class="line">            next[i] = next[next[i]]</span><br><span class="line">    <span class="keyword">return</span> next</span><br></pre></td></tr></table></figure>
<ul>
<li><p>3.<a href="https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm" target="_blank" rel="noopener">BM算法</a></p>
</li>
<li><p>4.<a href="http://dsqiu.iteye.com/blog/1700312" target="_blank" rel="noopener">Sunday算法</a></p>
</li>
</ul>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>KMP算法详细介绍： <a href="http://blog.csdn.net/v_july_v/article/details/7041827" target="_blank" rel="noopener">July的博客</a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>KMP</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:H-index</title>
    <url>/2015/09/04/leetcode/leetcode-H-index/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Given an array of citations (each citation is a non-negative integer) of a researcher, write a function to compute the researcher’s h-index.</p>
<p>According to the <em>definition of h-index on Wikipedia</em>: “A scientist has index h if h of his/her N papers have <strong>at least</strong> h citations each, and the other N − h papers have <strong>no more than</strong> h citations each.”</p>
<p>For example, given <code>citations = [3, 0, 6, 1, 5]</code>, which means the researcher has <code>5</code> papers in total and each of them had received <code>3, 0, 6, 1, 5</code> citations respectively. Since the researcher has <code>3</code> papers with <strong>at least</strong> 3 citations each and the remaining two with <strong>no more than</strong> <code>3</code> citations each, his h-index is <code>3</code>.</p>
<p><strong>Note</strong>: If there are several possible values for <code>h</code>, the maximum one is taken as the h-index.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p><code>H-index</code>被用来计算一个科研工作者工作的影响力大小，其计算方式为如果一个科研人员发表的<code>h</code>篇论文都被引用至少<code>h</code>次以上且并没有<code>h + 1</code>篇论文被引用<code>h + 1</code>次，那么他/她的<code>h-index</code>为<code>h</code>。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.使用hash表存储每个引用次数的文章数量，遍历hash表获取结果</li>
</ul>
<p>时间开销:$O(n^2)$,空间开销$O(m)$[m为引用次数的规模],结果:<code>TLE</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cList = defaultdict()</span><br><span class="line"><span class="keyword">for</span> paper <span class="keyword">in</span> range(len(citations)):</span><br><span class="line">    citeNum = citations[paper]</span><br><span class="line">    cList[citeNum] = <span class="number">1</span> <span class="keyword">if</span> cList.get(citeNum) <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> cList[citeNum] + <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> cList.keys():</span><br><span class="line">        <span class="keyword">if</span> key &lt; citeNum:</span><br><span class="line">            cList[key] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">res = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> cList.keys():</span><br><span class="line">    <span class="keyword">if</span> min(key, cList[key]) &gt; res:</span><br><span class="line">        res = min(key, cList[key])</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<ul>
<li>2.先进行排序，再查找，可在一遍内找到结果</li>
</ul>
<p>时间开销:$O(n\log n)$,空间开销$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">citations.sort()</span><br><span class="line">res = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> paper <span class="keyword">in</span> range(len(citations)):</span><br><span class="line">    h = min(len(citations) - paper,citations[paper])</span><br><span class="line">    <span class="keyword">if</span> h &gt; res:</span><br><span class="line">        res = h</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<ul>
<li>3.使用额外的N+1数组统计引用次数在0~N篇的论文个数，而后遍历</li>
</ul>
<p>时间开销:$O(n)$,空间开销$O(n)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">length = len(citations)</span><br><span class="line">cList = [<span class="number">0</span>] * (length + <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> citeNum <span class="keyword">in</span> citations:</span><br><span class="line">    <span class="keyword">if</span> citeNum &gt; length:</span><br><span class="line">        cList[length] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        cList[citeNum] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">res = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(length,<span class="number">0</span>,<span class="number">-1</span>):</span><br><span class="line">    <span class="keyword">if</span> res + cList[i] &gt;= i:</span><br><span class="line">        <span class="keyword">return</span> i</span><br><span class="line">    res += cList[i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>题解中思路3来自:<a href="http://bookshadow.com/weblog/293/" target="_blank" rel="noopener">书影</a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>Hash Table</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Maximum Product Subarray</title>
    <url>/2015/10/23/leetcode/leetcode-Maximum-Product-Subarray/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Find the contiguous subarray within an array (containing at least one number) which has the largest product.</p>
<p>For example, given the array <code>[2,3,-2,4]</code>,<br>the contiguous subarray <code>[2,3]</code> has the largest product = <code>6</code>.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>找出一个数组（至少含有一个元素）的子数组，这个子数组元素相乘具有最大的乘积。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.动态规划</li>
</ul>
<p>时间开销:$O(n)$,空间开销:$O(1)$,结果:<code>AC</code></p>
<p>通过计算前$i$位数组最大乘积$f<em>i$的递推公式求解。注意，需要保留前$i-1$位数组的<code>最大</code>乘积$f</em>{i-1}$和<code>最小</code>乘积$g<em>{i-1}$，因为若第$i$个数是负数，那么$g</em>{i-1} * nums_i$可能是前$i$位数组的最大乘积。得到$f_i$和$g_i$的递推公式后即可使用动态规划解决。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MAX_im1, MIN_im1, res = nums[<span class="number">0</span>], nums[<span class="number">0</span>], nums[<span class="number">0</span>]</span><br><span class="line">MAX_i, MIN_i = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(nums)):</span><br><span class="line">    MAX_i = max(max(MAX_im1 * nums[i], MIN_im1 * nums[i]), nums[i])</span><br><span class="line">    MIN_i = min(min(MAX_im1 * nums[i], MIN_im1 * nums[i]), nums[i])</span><br><span class="line">    res = max(MAX_i, res)</span><br><span class="line">    MAX_im1, MIN_im1 = MAX_i, MIN_i</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Isomorphic Strings</title>
    <url>/2015/09/06/leetcode/leetcode-Isomorphic-Strings/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Given two strings <strong>s</strong> and <strong>t</strong>, determine if they are isomorphic.</p>
<p>Two strings are isomorphic if the characters in <strong>s</strong> can be replaced to get <strong>t</strong>.</p>
<p>All occurrences of a character must be replaced with another character while preserving the order of characters. No two characters may map to the same character but a character may map to itself.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>判断两个字符串是否是同构的。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.使用两个<strong>Hash</strong>表来存放每个字母的出现位置，并比较排序后的<strong>Hash</strong>表值集合，需要两遍</li>
</ul>
<p>时间开销:O(n),空间开销:O(n),结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = dict()</span><br><span class="line">B = dict()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)):</span><br><span class="line">    <span class="keyword">if</span> s[i] <span class="keyword">in</span> A:</span><br><span class="line">        A[s[i]] += [i]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        A[s[i]] = [i]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(t)):</span><br><span class="line">    <span class="keyword">if</span> t[i] <span class="keyword">in</span> B:</span><br><span class="line">        B[t[i]] += [i]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        B[t[i]] = [i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> sorted(A.values()) == sorted(B.values())</span><br></pre></td></tr></table></figure>
<ul>
<li>2.使用两个<strong>hash</strong>表来记录两个字符串之间的字符映射关系，当表中的映射关系出现矛盾时，说明不同构</li>
</ul>
<p>时间开销:O(n),空间开销:O(n),结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A, B = dict(), dict()</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(len(s)):</span><br><span class="line">    tx, sx = A.get(t[x]), B.get(s[x])</span><br><span class="line">    <span class="keyword">if</span> tx <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> sx <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        A[t[x]], B[s[x]] = s[x], t[x]</span><br><span class="line">    <span class="keyword">elif</span> sx != t[x] <span class="keyword">or</span> tx != s[x]:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Median of Two Sorted Arrays</title>
    <url>/2016/04/02/leetcode/leetcode-Median-of-Two-Sorted-Arrays/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>There are two sorted arrays <strong>nums1</strong> and <strong>nums2</strong> of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be $O(log (m+n))$.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>给定两个长度为m和n的排序数组，求他们的中位数，要求时间开销$O(log(m+n))$。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.由于数组已经排序，可以用找第k个数的思路，通过每次取两数组各自的中位数并和k进行比较，从而截取其中某个数组的一段，使用递归实现。注意若总长度为偶数，这里要求返回的中位数为两中位数的平均值。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findMedianSortedArrays</span><span class="params">(self, nums1, nums2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :type nums1: List[int]</span></span><br><span class="line"><span class="string">    :type nums2: List[int]</span></span><br><span class="line"><span class="string">    :rtype: float</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m, n = len(nums1), len(nums2)</span><br><span class="line">    l = m + n</span><br><span class="line">    <span class="keyword">if</span> l % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> self.kth(nums1, nums2, l // <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> (self.kth(nums1, nums2, l // <span class="number">2</span>) + self.kth(nums1, nums2, l // <span class="number">2</span> - <span class="number">1</span>)) / <span class="number">2.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kth</span><span class="params">(self, nums1, nums2, k)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums1:</span><br><span class="line">        <span class="keyword">return</span> nums2[k]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums2:</span><br><span class="line">        <span class="keyword">return</span> nums1[k]</span><br><span class="line"></span><br><span class="line">    a, b = len(nums1) // <span class="number">2</span>, len(nums2) // <span class="number">2</span></span><br><span class="line">    ma, mb = nums1[a], nums2[b]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> a + b &lt; k:</span><br><span class="line">        <span class="keyword">if</span> ma &gt; mb:</span><br><span class="line">            <span class="keyword">return</span> self.kth(nums1, nums2[b + <span class="number">1</span>:], k - b - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.kth(nums1[a + <span class="number">1</span>:], nums2, k - a - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> ma &gt; mb:</span><br><span class="line">            <span class="keyword">return</span> self.kth(nums1[:a], nums2, k)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.kth(nums1, nums2[:b], k)</span><br></pre></td></tr></table></figure>
<ul>
<li>2.上面的方法有一个问题，就是由于Python数组的切片方法是线性时间开销所以时间开销并不满足要求，将使用切片改进为使用参数传入起始和结束索引。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_without_slice</span><span class="params">(self, nums1, s1, e1, nums2, s2, e2, k)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums1 <span class="keyword">or</span> e1 - s1 &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> nums2[k + s2]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums2 <span class="keyword">or</span> e2 - s2 &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> nums1[k + s1]</span><br><span class="line"></span><br><span class="line">    a, b = (e1 + s1) // <span class="number">2</span>, (e2 + s2) // <span class="number">2</span></span><br><span class="line">    ma, mb = nums1[a], nums2[b]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (a - s1) + (b - s2) &lt; k:</span><br><span class="line">        <span class="keyword">if</span> ma &gt; mb:</span><br><span class="line">            <span class="keyword">return</span> self.find_without_slice(nums1, s1, e1, nums2, b + <span class="number">1</span>, e2, k - (b - s2) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.find_without_slice(nums1, a + <span class="number">1</span>, e1, nums2, s2, e2, k - (a - s1) - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> ma &gt; mb:</span><br><span class="line">            <span class="keyword">return</span> self.find_without_slice(nums1, s1, a - <span class="number">1</span>, nums2, s2, e2, k)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.find_without_slice(nums1, s1, e1, nums2, s2, b - <span class="number">1</span>, k)</span><br></pre></td></tr></table></figure>
<p>时间开销:$O(log(m+n))$,空间开销:$O(1)$,结果:<code>AC</code></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Palindrome Number</title>
    <url>/2015/09/05/leetcode/leetcode-Palindrome-Number/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Determine whether an integer is a palindrome. Do this without extra space.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>确定一个整数是否是回文，不允许使用额外空间，这里额外空间应当理解为空间开销最多是O(1)。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.首先想到的是转换为字符串处理，但是这样会使用额外的空间，这里的n是数字的位数</li>
</ul>
<p>时间开销:O(n),空间开销:O(n),结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">st = str(x)</span><br><span class="line">length = len(st)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(length // <span class="number">2</span>):</span><br><span class="line">    <span class="keyword">if</span> st[i] != st[<span class="number">-1</span> - i]:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>类似思路Discuss中的简洁版Python代码，使用了内置的list-reverse：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">return</span> str(x)[::<span class="number">-1</span>] == str(x)</span><br></pre></td></tr></table></figure>
<ul>
<li>2.从低位加到高位，再比较和原数字的大小</li>
</ul>
<p>时间开销:O(n),空间开销:O(1),结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">res,tmp = <span class="number">0</span>,x</span><br><span class="line"><span class="keyword">while</span> tmp != <span class="number">0</span>:</span><br><span class="line">    res = res * <span class="number">10</span> + tmp % <span class="number">10</span></span><br><span class="line">    tmp //= <span class="number">10</span></span><br><span class="line"><span class="keyword">return</span> res == x</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>题解中思路1简明代码来自<a href="https://leetcode.com/discuss/49294/one-line-python-solution" target="_blank" rel="noopener">Leetcode-Discuss</a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Remove Duplicates from Sorted Array</title>
    <url>/2015/09/10/leetcode/leetcode-Remove-Duplicates-from-Sorted-Array/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Given a sorted array, remove the duplicates in place such that each element appear only once and return the new length.</p>
<p>Do not allocate extra space for another array, you must do this in place with constant memory.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>删除已排序数组中重复的元素，并将不重复的n个元素移到数组前面，返回个数n，要求空间开销O(1)。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.Leetcode给的tag是快慢指针；一个慢指针cur用来指示不重复元素的个数，nums[cur]是当前被比较的元素；快指针i用来遍历数组，遍历到不同于当前元素的数字就向前移动慢指针，并把当前nums[i]复制到慢指针所在的位置。最后慢指针i+1就是不同元素数组的长度。</li>
</ul>
<p>时间开销:O(n),空间开销:O(1),结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> len(nums) &lt; <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span> len(nums)</span><br><span class="line">cur = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">    <span class="keyword">if</span> nums[i] == nums[cur]:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    cur += <span class="number">1</span></span><br><span class="line">    nums[cur] = nums[i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> cur + <span class="number">1</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Product of Array Except Self</title>
    <url>/2015/09/27/leetcode/leetcode-Product-of-Array-Except-Self/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Given an array of n integers where n &gt; 1, <code>nums</code>, return an array <code>output</code> such that <code>output[i]</code> is equal to the product of all the elements of <code>nums</code> except <code>nums[i]</code>.</p>
<p>Solve it <strong>without division</strong> and in O(n).</p>
<p>For example, given <code>[1,2,3,4]</code>, return <code>[24,12,8,6]</code>.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>不使用额外的空间，不用除法操作，在<code>O(n)</code>的时间开销下计算输出数组，其中第i位是输入数组除<code>nums[i]</code>外的成员相乘。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.遍历两遍数组，对于第i位输出，第一遍计算nums[0] - nums[i - 1]，第二遍计算nums[i + 1] - nums[-1]</li>
</ul>
<p>时间开销:O(n),空间开销:O(1),结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prev, res = <span class="number">1</span>, [<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(nums)):</span><br><span class="line">    prev *= nums[i - <span class="number">1</span>]</span><br><span class="line">    res.append(prev)</span><br><span class="line"></span><br><span class="line">prev = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums) - <span class="number">2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">    prev *= nums[i + <span class="number">1</span>]</span><br><span class="line">    res[i] *= prev</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Path Sum II</title>
    <url>/2015/10/13/leetcode/leetcode-Path-Sum-II/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Given a binary tree and a sum, find all root-to-leaf paths where each path’s sum equals the given sum. </p>
<p>For example:<br>Given the below binary tree and <code>sum = 22</code>,</p>
<pre><code>          5
         / \
        4   8
       /   / \
      11  13  4
     /  \    / \
    7    2  5   1
</code></pre><p>return </p>
<pre><code>[
    [5,4,11,2],
    [5,8,4,5]
]
</code></pre><h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>找出一颗二叉树中所有树根到树叶路径和为指定数字的路径，并输出每一条符合条件的路径。深度优先搜索。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.递归</li>
</ul>
<p>时间开销:$O(n)$,空间开销:$O(n)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pathSum_recursive</span><span class="params">(self, root, sum)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(root, cur_sum, nodeList)</span>:</span></span><br><span class="line">        val, childs = root.val, (root.left, root.right)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> any(childs) <span class="keyword">and</span> cur_sum == sum:</span><br><span class="line">            res.append(nodeList)</span><br><span class="line">        <span class="keyword">if</span> root.left:</span><br><span class="line">            dfs(root.left, cur_sum + root.left.val, nodeList + [root.left.val])</span><br><span class="line">        <span class="keyword">if</span> root.right:</span><br><span class="line">            dfs(root.right, cur_sum + root.right.val, nodeList + [root.right.val])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    res = []</span><br><span class="line">    dfs(root, root.val, [root.val])</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<ul>
<li>2.非递归，利用栈进行深度优先搜索，由于Python进行函数调用代价较大，非递归方式时间开销小一些</li>
</ul>
<p>时间开销:$O(n)$,空间开销:$O(n)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">    <span class="keyword">return</span> []</span><br><span class="line">res, mStack = [], [(root, [])]</span><br><span class="line"><span class="keyword">while</span> mStack:</span><br><span class="line">    cur, nodeList = mStack.pop()</span><br><span class="line">    cur_sum, childs = __builtins__.sum(nodeList), (cur.left, cur.right)</span><br><span class="line">    nodeList = nodeList + [cur.val]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> any(childs) <span class="keyword">and</span> cur_sum + cur.val == sum:</span><br><span class="line">        res.append(nodeList)</span><br><span class="line">    <span class="keyword">if</span> cur.right:</span><br><span class="line">        mStack.append((cur.right, nodeList))</span><br><span class="line">    <span class="keyword">if</span> cur.left:</span><br><span class="line">        mStack.append((cur.left, nodeList))</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>题解中思路1来自:<a href="https://leetcode.com/discuss/questions/oj/path-sum-ii" target="_blank" rel="noopener">Leetcode-Discuss</a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DFS</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Search in Rotated Sorted Array-(I and II)</title>
    <url>/2015/10/28/leetcode/leetcode-Search-in-Rotated-Sorted-Array-I-and-II/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>I.</p>
<p>Suppose a sorted array is rotated at some pivot unknown to you beforehand.</p>
<p>(i.e., <code>0 1 2 4 5 6 7</code> might become <code>4 5 6 7 0 1 2</code>).</p>
<p>You are given a target value to search. If found in the array return its index, otherwise return <code>-1</code>.</p>
<p>You may assume no duplicate exists in the array.</p>
<p>II.</p>
<p>What if duplicates are allowed?</p>
<p>Would this affect the run-time complexity? How and why?</p>
<p>Return <code>True</code> if target is found in the array ,otherwise return <code>False</code>.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>给一个旋转后的排序数组，该数组以某个点为界前后两段被颠倒次序。在该数组中查找元素。I中假设数组不含重复元素，II中包含重复元素。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.对第一题，由于不包含重复元素，使用二分法即可。</li>
</ul>
<p>时间开销:$O(log(n))$,空间开销:$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L, R = <span class="number">0</span>, len(nums) - <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> L &lt;= R:</span><br><span class="line">    m = (L + R) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> target == nums[m]:   <span class="keyword">return</span> m</span><br><span class="line">    <span class="keyword">if</span> nums[m] &gt; nums[R]:</span><br><span class="line">        <span class="keyword">if</span> nums[L] &lt;= target &lt;= nums[m]:</span><br><span class="line">            R = m - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            L = m + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> nums[m] &lt;= target &lt;= nums[R]:</span><br><span class="line">            L = m + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            R = m - <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<ul>
<li>2.第二题由于包含重复元素，因此二分法在最坏情况下时间开销变为$O(n)$。</li>
</ul>
<p>时间开销:$O(log(n))$,空间开销:$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L, R = <span class="number">0</span>, len(nums) - <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> L &lt;= R:</span><br><span class="line">    m = (L + R) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> target == nums[m]:   <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">while</span> R &gt; m <span class="keyword">and</span> nums[m] == nums[R]:  <span class="comment"># deal with the duplicate</span></span><br><span class="line">        R -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> nums[m] &gt; nums[R]:  <span class="comment"># the left part is sorted</span></span><br><span class="line">        <span class="keyword">if</span> nums[L] &lt;= target &lt; nums[m]:</span><br><span class="line">            R = m - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            L = m + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># the right part is sorted</span></span><br><span class="line">        <span class="keyword">if</span> nums[m] &lt; target &lt;= nums[R]:</span><br><span class="line">            L = m + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            R = m - <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>Binary Search</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Set Matrix Zeroes</title>
    <url>/2015/10/05/leetcode/leetcode-Set-Matrix-Zeroes/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Given a m x n matrix, if an element is 0, set its entire row and column to 0. Do it in place.</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>给一个m*n的矩阵，若矩阵中元素为0，则将其相同行和列的元素全部设置为0。在原有矩阵上修改数据。要求不要使用额外空间为佳。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.使用类似于<a href="https://leetcode.com/problems/game-of-life/" target="_blank" rel="noopener">Game of Life</a>中的解法，在数组元素位置上设置标志位，这里选取的标志是整数的最大值，缺点是时间开销略大</li>
</ul>
<p>时间开销:$O(mn(m+n))$,空间开销:$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_zero</span><span class="params">(matrix, x, y)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(matrix)):</span><br><span class="line">        matrix[i][y] = MAX <span class="keyword">if</span> matrix[i][y] != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(matrix[<span class="number">0</span>])):</span><br><span class="line">        matrix[x][j] = MAX <span class="keyword">if</span> matrix[x][j] != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">X, Y = len(matrix), len(matrix[<span class="number">0</span>])</span><br><span class="line">MAX = sys.maxsize</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(X):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(Y):</span><br><span class="line">        <span class="keyword">if</span> matrix[i][j] == <span class="number">0</span>:</span><br><span class="line">            all_zero(matrix, i, j)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(X):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(Y):</span><br><span class="line">        <span class="keyword">if</span> matrix[i][j] == MAX:</span><br><span class="line">            matrix[i][j] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<ul>
<li>2.使用数组的第一行和第一列存储清零标志，第一行和第一列本身是否清零由额外的两个标志位决定，时间开销更小</li>
</ul>
<p>时间开销:$O(mn)$,空间开销:$O(1)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, Y = len(matrix), len(matrix[<span class="number">0</span>])</span><br><span class="line">row_zero, column_zero = <span class="literal">False</span>, <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(X):</span><br><span class="line">    <span class="keyword">if</span> matrix[i][<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        row_zero = <span class="literal">True</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(Y):</span><br><span class="line">    <span class="keyword">if</span> matrix[<span class="number">0</span>][j] == <span class="number">0</span>:</span><br><span class="line">        column_zero = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(X):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(Y):</span><br><span class="line">        <span class="keyword">if</span> matrix[i][j] == <span class="number">0</span>:</span><br><span class="line">            matrix[<span class="number">0</span>][j], matrix[i][<span class="number">0</span>] = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, X):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, Y):</span><br><span class="line">        <span class="keyword">if</span> matrix[i][<span class="number">0</span>] == <span class="number">0</span> <span class="keyword">or</span> matrix[<span class="number">0</span>][j] == <span class="number">0</span>:</span><br><span class="line">            matrix[i][j] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> row_zero:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X):</span><br><span class="line">        matrix[i][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="keyword">if</span> column_zero:</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(Y):</span><br><span class="line">        matrix[<span class="number">0</span>][j] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>题解中思路2来自:<a href="http://chenpindian.com/2014/12/18/post-4/" target="_blank" rel="noopener">一只幻想飞翔的猪</a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:Symmetric Tree</title>
    <url>/2015/09/18/leetcode/leetcode-Symmetric-Tree/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>Given a binary tree, check whether it is a mirror of itself (ie, symmetric around its center).</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>判断二叉树是否是对称的</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><ul>
<li>1.递归方法，比较左子树的右子树和右子树的左子树，以及左子树的左子树和右子树的右子树</li>
</ul>
<p>由于使用了递归，设定n为同高度满二叉树的节点数量，按照函数调用次数计算，空间开销为$O(2^ {\lg n - 1})$，时间开销为$O(2^ {\lg n - 1})$</p>
<p>时间开销:$O(2^ {\lg n - 1})$,空间开销:$O(2^ {\lg n - 1})$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isSymmetric_recursive</span><span class="params">(self, root)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> Solution.isSyme(root.left, root.right)</span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isSyme</span><span class="params">(l, r)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> bool(l) != bool(r):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="keyword">not</span> l <span class="keyword">and</span> <span class="keyword">not</span> r:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">elif</span> l.val != r.val:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> Solution.isSyme(l.right, r.left) <span class="keyword">and</span> Solution.isSyme(l.left, r.right)</span><br></pre></td></tr></table></figure>
<ul>
<li>2.迭代方法，每层进行比较，由于开辟了栈进行存放，因此空间开销为O(n)</li>
</ul>
<p>时间开销:$O(n)$,空间开销:$O(n)$,结果:<code>AC</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">mStack = [(root.left, root.right)]</span><br><span class="line"><span class="keyword">while</span> mStack:</span><br><span class="line">    l, r = mStack.pop()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> l <span class="keyword">and</span> <span class="keyword">not</span> r:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">if</span> bool(l) != bool(r) <span class="keyword">or</span> l.val != r.val:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    mStack.append((l.left, r.right))</span><br><span class="line">    mStack.append((l.right, r.left))</span><br><span class="line"><span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>题解中思路2来自:<a href="https://leetcode.com/discuss/51603/python-short-recursive-solution" target="_blank" rel="noopener">Leetcode-Discuss</a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DFS</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode:partition-equal-subset-sum</title>
    <url>/2019/01/18/leetcode/partition-equal-subset-sum/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>给定一个<strong>只包含正整数</strong>的<strong>非空</strong>数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>状态：前i个元素是否能找到一个子集其和等于sum/2。前<script type="math/tex">i</script>个是否能够等于<script type="math/tex">j</script>记录在<script type="math/tex">dp[i][j]</script>。</p>
<p>初始值：<script type="math/tex">dp[:][0] = 0</script></p>
<p>递推：已知<script type="math/tex">dp[<i],dp[i][<j]</script>，如果可以加入第<script type="math/tex">i</script>个元素那么由 [加入or不加入]决定，如果加入当前元素不合法那么就等于<script type="math/tex">dp[i-1][j]</script>。</p>
<p>返回：<script type="math/tex">dp[n][sum/2]</script>。</p>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><p>时间复杂度和空间开销都是<script type="math/tex">O(N\cdot S)</script>，S是sum/2。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canPartition</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        target = sum(nums)</span><br><span class="line">        n = len(nums)</span><br><span class="line">        <span class="keyword">if</span> target % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        target = target // <span class="number">2</span></span><br><span class="line">        <span class="comment"># 前n个数字能否组成和为target</span></span><br><span class="line">        dp = [[<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(target + <span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n + <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>):</span><br><span class="line">            dp[i][<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):  <span class="comment"># 前n个数</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, target + <span class="number">1</span>):  <span class="comment"># 和为1到target</span></span><br><span class="line">                <span class="keyword">if</span> j - nums[i - <span class="number">1</span>] &gt;= <span class="number">0</span>:  <span class="comment"># 可以选择第i个数字</span></span><br><span class="line">                    dp[i][j] = (</span><br><span class="line">                        dp[i - <span class="number">1</span>][j] <span class="keyword">or</span> dp[i - <span class="number">1</span>][j - nums[i - <span class="number">1</span>]]</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j]</span><br><span class="line">        <span class="keyword">return</span> dp[n][target]</span><br></pre></td></tr></table></figure>
<p>由于当前的<script type="math/tex">dp[i][j]</script>只和<script type="math/tex">dp[i-1][:]</script>有关，所以可以对空间开销进行优化，优化为<script type="math/tex">O(S)</script>的开销。</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode: lfu-cache</title>
    <url>/2019/03/25/leetcode/lfu-cache/</url>
    <content><![CDATA[<h3 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h3><p>为 <a href="https://baike.baidu.com/item/缓存算法" target="_blank" rel="noopener">最不经常使用（LFU）</a>缓存算法设计并实现数据结构。它应该支持以下操作：<code>get</code> 和 <code>put</code>。</p>
<ul>
<li>get(key) - 如果键存在于缓存中，则获取键的值（总是正数），否则返回 -1。</li>
<li>put(key, value) - 如果键已存在，则变更其值；如果键不存在，请插入键值对。当缓存达到其容量时，则应该在插入新项之前，使最不经常使用的项无效。在此问题中，当存在平局（即两个或更多个键具有相同使用频率）时，应该去除最久未使用的键。</li>
</ul>
<p>「项的使用次数」就是自插入该项以来对其调用 get 和 put 函数的次数之和。使用次数会在对应项被移除后置为 0 。<br>在 O(1) 时间复杂度内执行两项操作？</p>
<h3 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h3><p>对get操作来说，只需要使用key-&gt;value的映射即可在O(1)时间查找。<br>对put操作来说：</p>
<ul>
<li>1.因为要根据频率删除元素，所以要更新频率，就需要记录key-&gt;freq的映射。</li>
<li>2.因为需要O(1)删除节点，所以需要按照freq排序，即记录freq -&gt; key_list，还需要记录并维护min_freq这个数值，从而直接找到使用频率最低的key的集合。</li>
<li>3.因为题目要求在多个key使用频率相同时按照插入顺序删除最早插入的，但这个key_list不能以O(1)插入和删除，我们这里使用了OrderedDict来存储，其实也可以用Queue是一样的。</li>
</ul>
<h3 id="三、思路"><a href="#三、思路" class="headerlink" title="三、思路"></a>三、思路</h3><p>时间开销：</p>
<ul>
<li><p>更新频率，包含 OrderedDict/Queue删除元素、字典取值、OrderedDict/Queue插入元素，均为O(1)。</p>
</li>
<li><p>get，涉及到更新频率和字典取值，O(1)。</p>
</li>
<li>put，涉及到更新频率、字典取值、字典插入、字典/OrderedDict/Queue删除，O(1)。</li>
</ul>
<p>空间开销，额外存储需要O(3n)也就是O(n)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict, defaultdict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LFUCache</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, capacity: int)</span>:</span></span><br><span class="line">        self.key2value = &#123;&#125;</span><br><span class="line">        self.key2freq = defaultdict(int)</span><br><span class="line">        self.freq2keylist = defaultdict(OrderedDict)</span><br><span class="line">        self.min_freq = <span class="number">0</span></span><br><span class="line">        self.cap = capacity</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_freq</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="comment"># 更新频率</span></span><br><span class="line">        freq = self.key2freq[key]</span><br><span class="line">        self.key2freq[key] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> freq != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">del</span> self.freq2keylist[freq][key]</span><br><span class="line">        self.freq2keylist[freq + <span class="number">1</span>][key] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># 是否更新min_freq</span></span><br><span class="line">        <span class="keyword">if</span> freq == self.min_freq <span class="keyword">and</span> <span class="keyword">not</span> self.freq2keylist[freq]:</span><br><span class="line">            self.min_freq += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, key: int)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self.key2value <span class="keyword">and</span> self.cap &gt; <span class="number">0</span>:</span><br><span class="line">            self.update_freq(key)</span><br><span class="line">            <span class="comment"># print('get &#123;&#125;'.format(key))</span></span><br><span class="line">            <span class="comment"># print(self)</span></span><br><span class="line">            <span class="keyword">return</span> self.key2value[key]</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.key2value)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'cache: &#123;&#125;\nfreq_list: &#123;&#125;\nmin_freq:&#123;&#125;'</span>.format(self.key2value,</span><br><span class="line">        self.freq2keylist, self.min_freq)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span><span class="params">(self, key: int, value: int)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="comment"># 已存在</span></span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self.key2value:</span><br><span class="line">            self.key2value[key] = value</span><br><span class="line">            self.update_freq(key)</span><br><span class="line">        <span class="comment"># 不存在</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> len(self) &gt;= self.cap <span class="keyword">and</span> self.key2value:  <span class="comment"># 删除一个key</span></span><br><span class="line">                min_freq = self.min_freq</span><br><span class="line">                del_key, _ = self.freq2keylist[min_freq].popitem(last=<span class="literal">False</span>)   <span class="comment"># O(1)</span></span><br><span class="line">                <span class="keyword">del</span> self.key2freq[del_key]</span><br><span class="line">                <span class="keyword">del</span> self.key2value[del_key]</span><br><span class="line">            <span class="comment"># 插入数据并更新频率</span></span><br><span class="line">            self.key2value[key] = value</span><br><span class="line">            self.update_freq(key)</span><br><span class="line">            self.min_freq = <span class="number">1</span></span><br><span class="line">        <span class="comment"># print('Inject &#123;&#125;:&#123;&#125;'.format(key, value))</span></span><br><span class="line">        <span class="comment"># print(self)</span></span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="comment"># Your LFUCache object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"># obj = LFUCache(capacity)</span></span><br><span class="line"><span class="comment"># param_1 = obj.get(key)</span></span><br><span class="line"><span class="comment"># obj.put(key,value)</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>Hash Table</tag>
        <tag>Data Structure</tag>
        <tag>Linkedlist</tag>
      </tags>
  </entry>
  <entry>
    <title>树</title>
    <url>/2015/09/20/leetcode/%E6%A0%91/</url>
    <content><![CDATA[<h3 id="一、Flatten-Binary-Tree-to-Linked-List"><a href="#一、Flatten-Binary-Tree-to-Linked-List" class="headerlink" title="一、Flatten Binary Tree to Linked List"></a>一、Flatten Binary Tree to Linked List</h3><blockquote>
<p>Given a binary tree, flatten it to a linked list in-place.</p>
</blockquote>
<p>给一个二叉树，按照前序遍历序将它扁平化为一个链表。</p>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><ul>
<li>1.使用列表存储前序遍历的结果而后遍历列表</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten</span><span class="params">(self, root)</span>:</span></span><br><span class="line">    self.traversal(root)</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> self.mlist:</span><br><span class="line">        node.left = <span class="literal">None</span></span><br><span class="line">        node.right = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.mlist) - <span class="number">1</span>):</span><br><span class="line">        self.mlist[i].right = self.mlist[i + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">traversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    self.mlist.append(root)</span><br><span class="line">    self.traversal(root.left)</span><br><span class="line">    self.traversal(root.right)</span><br></pre></td></tr></table></figure>
<ul>
<li>2.使用栈进行深度优先搜索，右子树进栈，左子树进行处理</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">mStack, cur = [], root</span><br><span class="line"><span class="keyword">while</span> cur:</span><br><span class="line">    <span class="keyword">if</span> cur.right:</span><br><span class="line">        mStack.append(cur.right)</span><br><span class="line">    <span class="keyword">if</span> cur.left:</span><br><span class="line">        cur.right = cur.left</span><br><span class="line">        cur.left = <span class="literal">None</span></span><br><span class="line">        cur = cur.right</span><br><span class="line">    <span class="keyword">elif</span> mStack:</span><br><span class="line">        cur.right = mStack.pop()</span><br><span class="line">        cur = cur.right</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<ul>
<li>3.使用递归进行深度优先搜索，先处理左子树，再处理右子树</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_recursive</span><span class="params">(self, root)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :type root: TreeNode</span></span><br><span class="line"><span class="string">    :rtype: void Do not return anything, modify root in-place instead.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.flatten_in(root)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_in</span><span class="params">(self, root)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    start, end = root, root</span><br><span class="line">    lstart, lend = self.flatten_in(root.left)</span><br><span class="line">    pright = root.right</span><br><span class="line">    <span class="keyword">if</span> lstart:</span><br><span class="line">        end.right = lstart</span><br><span class="line">        end = lend</span><br><span class="line"></span><br><span class="line">    rstart, rend = self.flatten_in(pright)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> rstart:</span><br><span class="line">        end.right = rstart</span><br><span class="line">        end = rend</span><br><span class="line"></span><br><span class="line">    start.left, end.right = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> start, end</span><br></pre></td></tr></table></figure>
<h3 id="二、通过指令创建有序数组"><a href="#二、通过指令创建有序数组" class="headerlink" title="二、通过指令创建有序数组"></a>二、通过指令创建有序数组</h3><blockquote>
<p>给你一个整数数组 instructions ，你需要根据 instructions 中的元素创建一个有序数组。一开始你有一个空的数组 nums ，你需要 从左到右 遍历 instructions 中的元素，将它们依次插入 nums 数组中。每一次插入操作的 代价 是以下两者的 较小值 ：</p>
<p>nums 中 严格小于  instructions[i] 的数字数目。<br>nums 中 严格大于  instructions[i] 的数字数目。</p>
</blockquote>
<h4 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h4><p>1.通过二分查找、二分插入和mapping来找每个指令的cost。$O(n^2)$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createSortedArray</span><span class="params">(self, a: List[int])</span> -&gt; int:</span></span><br><span class="line">        freqs = defaultdict(int)</span><br><span class="line">        nums = []</span><br><span class="line">        cost = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> a:</span><br><span class="line">            mid = bisect.bisect_right(nums, x)</span><br><span class="line">            cost += (min(mid - freqs[x], len(nums) - mid) % (<span class="number">1e9</span> + <span class="number">7</span>))</span><br><span class="line">            <span class="comment"># O(n)</span></span><br><span class="line">            bisect.insort_right(nums, x)</span><br><span class="line">            freqs[x] += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> int(cost % (<span class="number">1e9</span> + <span class="number">7</span>))</span><br></pre></td></tr></table></figure>
<p>2.通过树状数组，将每个元素的值作为索引，更新时更新的变化值为1，这样前缀和就是小于当前数字的个数，时间开销可以下降到$O(n\cdot\log n)$。这个思路还可以用离散化进一步优化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lowbit</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x &amp; -x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArrayTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nums: List, merge=lambda x, y: x + y, length=None)</span>:</span></span><br><span class="line">        <span class="string">"""O(n logn)，还可以优化为O(n)"""</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        self.data = [<span class="number">0</span>] * (length + <span class="number">1</span>) <span class="keyword">if</span> length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> [<span class="number">0</span>] * (n + <span class="number">1</span>)</span><br><span class="line">        self.merge = merge</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            self.update(i, nums[i - <span class="number">1</span>] - <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, idx, delta)</span>:</span></span><br><span class="line">        <span class="string">"""单点更新, O(log n)，自下而上，更新后续的父节点，这里的value不是新的值而是变化"""</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt; idx &lt; len(self)</span><br><span class="line">        <span class="keyword">while</span> idx &lt; len(self):</span><br><span class="line">            self.data[idx] = self.merge(self.data[idx], delta)</span><br><span class="line">            idx += lowbit(idx)  <span class="comment"># 更新到父节点的索引</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">query</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="string">"""查询[1, i]的前缀信息，O(log n)"""</span></span><br><span class="line">        <span class="comment"># assert 0 &lt; i &lt; len(self)</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &gt; <span class="number">0</span>:</span><br><span class="line">            res = self.merge(res, self.data[i])</span><br><span class="line">            i -= lowbit(i)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createSortedArray</span><span class="params">(self, a: List[int])</span> -&gt; int:</span></span><br><span class="line">        max_ = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">            max_ = max(max_, i)</span><br><span class="line">        at = ArrayTree([], length=max_)</span><br><span class="line">        cost = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        freqs = defaultdict(int)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">            <span class="comment"># O(logn)</span></span><br><span class="line">            left = at.query(i - <span class="number">1</span>)</span><br><span class="line">            cost += min(left, total - left - freqs[i])</span><br><span class="line">            <span class="comment"># O(logn)</span></span><br><span class="line">            at.update(i, <span class="number">1</span>)</span><br><span class="line">            freqs[i] += <span class="number">1</span></span><br><span class="line">            total += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> int(cost % (<span class="number">1e9</span> + <span class="number">7</span>))</span><br></pre></td></tr></table></figure>
<p>离散化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sorted(list(set(nums)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discretization</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="string">"""离散化，返回原有数据到[1, len]范围内的映射"""</span></span><br><span class="line">    res = &#123;&#125;</span><br><span class="line">    n_nums = sort(nums)</span><br><span class="line">    <span class="keyword">for</span> i, v <span class="keyword">in</span> enumerate(n_nums):</span><br><span class="line">        res[v] = i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res, len(n_nums)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>Tree</tag>
        <tag>DFS</tag>
        <tag>树状数组</tag>
      </tags>
  </entry>
  <entry>
    <title>LBFGS</title>
    <url>/2015/07/31/machine-learning/LBFGS/</url>
    <content><![CDATA[<h2 id="DFP、BFGS和L-BFGS是三种重要的拟牛顿法"><a href="#DFP、BFGS和L-BFGS是三种重要的拟牛顿法" class="headerlink" title="DFP、BFGS和L-BFGS是三种重要的拟牛顿法"></a>DFP、BFGS和L-BFGS是三种重要的拟牛顿法</h2><h3 id="一、牛顿法"><a href="#一、牛顿法" class="headerlink" title="一、牛顿法"></a>一、牛顿法</h3><p>牛顿法同梯度下降法一样是一种逼近解法，用来求损失（或极大似然）函数的最小（最大）值，它利用了函数的二阶导数矩阵$Hession$矩阵，因此收敛速度比只利用了一阶导数的梯度下降快。</p>
<p>对函数$f(x)$在$X_k$点进行二阶泰勒展开，</p>
<script type="math/tex; mode=display">\varphi(x) = f(X_k) + \bigtriangledown f(X_k)(X-X_k) + \frac12(X-X_k)^T \bigtriangledown^2 f(X_k)(X-X_k)</script><p>其中$\bigtriangledown f$为$f$的梯度向量，$\bigtriangledown^2 f$为$f$的$Hession$矩阵</p>
<blockquote>
<p>由于要求$f(x)$的极值，因此要找出$\bigtriangledown \varphi (x) = 0$的点，即：</p>
</blockquote>
<script type="math/tex; mode=display">g_k + H_k(X-X_k) = 0</script><p>可解出：</p>
<script type="math/tex; mode=display">X_{k+1} = X_k - H_k^{-1}g_k \qquad k = 0,1,2,.....</script><p>其中的$d_k = - H_k^{-1}g_k$为搜索方向，称为牛顿方向</p>
<p>牛顿法的问题是每次迭代需要求二阶导矩阵（Hession Matrix）及它的逆矩阵，计算量太大</p>
<h3 id="二、拟牛顿法"><a href="#二、拟牛顿法" class="headerlink" title="二、拟牛顿法"></a>二、拟牛顿法</h3><p>拟牛顿法就是利用近似的方法来求海森矩阵及它的逆矩阵来减少运算量：</p>
<script type="math/tex; mode=display">B_{k+1} \sim H_{k+1}</script><script type="math/tex; mode=display">D_{k+1} \sim H_{k+1}^{-1}</script><h3 id="三、DFP算法"><a href="#三、DFP算法" class="headerlink" title="三、DFP算法"></a>三、DFP算法</h3><p>DFP算法以三个发明人的名字首字母命名，是最早的拟牛顿法</p>
<h3 id="四、BFGS算法"><a href="#四、BFGS算法" class="headerlink" title="四、BFGS算法"></a>四、BFGS算法</h3><p>BFGS算法以其四个发明人的名字首字母命名，相比于DFP算法其性能更好，而且其局部收敛性在数学上得到了证明</p>
<h3 id="五、L-BFGS算法"><a href="#五、L-BFGS算法" class="headerlink" title="五、L-BFGS算法"></a>五、L-BFGS算法</h3><p>即Limited-memory BFGS算法，对BFGS算法进行了近似，不再存储完整的矩阵$D_k$，而是存储最新的m个向量序列$S_i,Y_j$，每次需要$D_k$时，利用向量序列乘法进行计算，这样存储代价就由原来的$O(N^2)$降低到了$O(mN)$</p>
<h1 id=""><a href="#" class="headerlink" title="#"></a>#</h1><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a href="http://blog.csdn.net/itplus/article/details/21897715" target="_blank" rel="noopener">牛顿法与拟牛顿法学习笔记</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode: 图</title>
    <url>/2020/05/25/leetcode/%E5%9B%BE/</url>
    <content><![CDATA[<h3 id="一、最小生成树"><a href="#一、最小生成树" class="headerlink" title="一、最小生成树"></a>一、最小生成树</h3><blockquote>
<p>给你一个points 数组，表示 2D 平面上的一些点，其中 points[i] = [xi, yi] 。</p>
<p>连接点 [xi, yi] 和点 [xj, yj] 的费用为它们之间的 曼哈顿距离 ：|xi - xj| + |yi - yj| ，其中 |val| 表示 val 的绝对值。</p>
<p>请你返回将所有点连接的最小总费用。只有任意两点之间 有且仅有 一条简单路径时，才认为所有点都已连接。</p>
</blockquote>
<p>Prim：从点的视角出发，优点就是不用一次计算所有的边的权重。某个节点出发，把它放到队列里面。</p>
<p>然后循环以下操作直到所有节点都已经访问：从队列中弹出距离最小的节点，如果还没访问就设置为访问，加入总代价。然后计算当前节点和所有没访问的节点之间的权重，放到优先级队列里面。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> PriorityQueue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minCostConnectPoints</span><span class="params">(self, points: List[List[int]])</span> -&gt; int:</span></span><br><span class="line">        calcu_distance = <span class="keyword">lambda</span> x, y: abs(points[x][<span class="number">0</span>] - points[y][<span class="number">0</span>]) + abs(points[x][<span class="number">1</span>] - points[y][<span class="number">1</span>])   </span><br><span class="line">        n = len(points)</span><br><span class="line">        pq = PriorityQueue()  <span class="comment"># 利用优先级队列以logN时间开销存储</span></span><br><span class="line">        un_visit = set([i <span class="keyword">for</span> i <span class="keyword">in</span> range(n)])</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        cur = (<span class="number">0</span>, <span class="number">0</span>)  <span class="comment"># distance, node index</span></span><br><span class="line">        pq.put(cur)</span><br><span class="line">        <span class="keyword">while</span> un_visit:</span><br><span class="line">            dist, cur = pq.get()</span><br><span class="line">            <span class="comment"># print('&#123;&#125;'.format(un_visit))</span></span><br><span class="line">            <span class="keyword">if</span> cur <span class="keyword">not</span> <span class="keyword">in</span> un_visit:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            un_visit.remove(cur)</span><br><span class="line">            res += dist</span><br><span class="line">            <span class="keyword">for</span> each <span class="keyword">in</span> un_visit:</span><br><span class="line">                dist = calcu_distance(cur, each)</span><br><span class="line">                pq.put((dist, each))</span><br><span class="line">            <span class="comment"># print('&#123;&#125;'.format(list(pq.queue)))</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>kruskal算法 + 并查集实现：从边的视角出发。初始化并查集。</p>
<p>1.计算出所有边的权重并排序。</p>
<p>2.在边的集合中按从小到大循环：</p>
<p>如果加入当前边会形成环（也就是两个节点已经有通路了，并查集），那么就跳过。</p>
<p>否则就加入当前边（加入总代价，并连通两个节点）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minCostConnectPoints</span><span class="params">(self, points: List[List[int]])</span> -&gt; int:</span></span><br><span class="line">        n = len(points)</span><br><span class="line">        uf = &#123;i: i <span class="keyword">for</span> i <span class="keyword">in</span> range(n)&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(x)</span>:</span></span><br><span class="line">            <span class="keyword">while</span> x != uf[x]:</span><br><span class="line">                x = uf[x]</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line">            </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">connect</span><span class="params">(p, q)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> find(p) == find(q)</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(p, q)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> connect(p, q):</span><br><span class="line">                uf[find(p)] = find(q)</span><br><span class="line">        </span><br><span class="line">        edges = defaultdict(int)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, n):</span><br><span class="line">                edges[(i, j)] = abs(points[i][<span class="number">0</span>] - points[j][<span class="number">0</span>]) + abs(points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>])</span><br><span class="line">        edges = sorted(edges.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>]) <span class="comment"># 排序后的边</span></span><br><span class="line">        <span class="comment"># print(edges)</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">       </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(edges)):</span><br><span class="line">            (x, y), dist = edges[i]</span><br><span class="line">            <span class="comment"># print('x: &#123;&#125;, y: &#123;&#125;'.format(find(x), find(y)))</span></span><br><span class="line">            <span class="keyword">if</span> find(x) != find(y):  <span class="comment"># 如果还没有连通</span></span><br><span class="line">                total += <span class="number">1</span></span><br><span class="line">                res += dist</span><br><span class="line">                <span class="comment"># print('&#123;&#125;, &#123;&#125;, &#123;&#125;'.format((x, y), res, total))</span></span><br><span class="line">                <span class="keyword">if</span> total == n - <span class="number">1</span>: <span class="comment"># 所有节点都已经连通</span></span><br><span class="line">                    <span class="keyword">return</span> res</span><br><span class="line">                union(x, y)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="二、判断二分图"><a href="#二、判断二分图" class="headerlink" title="二、判断二分图"></a>二、判断二分图</h3><blockquote>
<p>给定一个无向图graph，当这个图为二分图时返回true。</p>
<p>如果我们能将一个图的节点集合分割成两个独立的子集A和B，并使图中的每一条边的两个节点一个来自A集合，一个来自B集合，我们就将这个图称为二分图。</p>
</blockquote>
<p>染色，对于每个边连接的节点，他们一定属于A和B这两个不同集合，BFS或者DFS搜索，如果出现颜色矛盾说明不能构成二分图。</p>
<p>对于不连通的子图，可以作为两个图看待，这个子图里面随机从一个点开始进行染色就行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"><span class="string">"""BFS、并查集"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isBipartite</span><span class="params">(self, graph: List[List[int]])</span> -&gt; bool:</span></span><br><span class="line">        n = len(graph)</span><br><span class="line">        parent = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(node)</span>:</span></span><br><span class="line">            <span class="keyword">while</span> parent[node] != node:</span><br><span class="line">                node = parent[node]</span><br><span class="line">            <span class="keyword">return</span> node</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(node1, node2)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> find(node1) != find(node2):</span><br><span class="line">                parent[find(node1)] = find(node2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, each <span class="keyword">in</span> enumerate(graph):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> each:</span><br><span class="line">                union(i, j)</span><br><span class="line">        <span class="comment"># 所有子图中，每个子图找一个随机节点</span></span><br><span class="line">        roots = set()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            cur = find(i)</span><br><span class="line">            <span class="keyword">if</span> cur <span class="keyword">not</span> <span class="keyword">in</span> roots:</span><br><span class="line">                roots.add(cur)</span><br><span class="line">        visited = defaultdict(bool)  <span class="comment"># 要把每个节点都遍历</span></span><br><span class="line">        undo, green, red = <span class="number">0</span>, <span class="number">1</span>, <span class="number">-1</span></span><br><span class="line">        color = defaultdict(int)  <span class="comment"># 0.未着色，1.蓝色，-1.红色</span></span><br><span class="line">        cur_color = <span class="number">1</span></span><br><span class="line">        q = Queue()</span><br><span class="line">        <span class="comment"># print(roots)</span></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> roots:</span><br><span class="line">            q.put(each)</span><br><span class="line">            visited[each] = <span class="literal">True</span></span><br><span class="line">            color[each] = red</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> q.empty():</span><br><span class="line">            cur = q.get()</span><br><span class="line">            cur_color = color[cur]</span><br><span class="line">            wanted_color = red <span class="keyword">if</span> cur_color == green <span class="keyword">else</span> green</span><br><span class="line">            <span class="keyword">for</span> nei <span class="keyword">in</span> graph[cur]:</span><br><span class="line">                <span class="keyword">if</span> color[nei] == undo:</span><br><span class="line">                    color[nei] = wanted_color</span><br><span class="line">                <span class="keyword">elif</span> color[nei] != wanted_color:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                color[nei] = wanted_color</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> visited[nei]:</span><br><span class="line">                    visited[nei] = <span class="literal">True</span></span><br><span class="line">                    q.put(nei)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h3 id="三、图连通"><a href="#三、图连通" class="headerlink" title="三、图连通"></a>三、图连通</h3><blockquote>
<p>有 n 座城市，编号从 1 到 n 。编号为 x 和 y 的两座城市直接连通的前提是： x 和 y 的公因数中，至少有一个 严格大于 某个阈值 threshold 。更正式地说，如果存在整数 z ，且满足以下所有条件，则编号 x 和 y 的城市之间有一条道路：</p>
<p>x % z == 0<br>y % z == 0<br>z &gt; threshold<br>给你两个整数 n 和 threshold ，以及一个待查询数组，请你判断每个查询 queries[i] = [ai, bi] 指向的城市 ai 和 bi 是否连通（即，它们之间是否存在一条路径）。</p>
<p>返回数组 answer ，其中answer.length == queries.length 。如果第 i 个查询中指向的城市 ai 和 bi 连通，则 answer[i] 为 true ；如果不连通，则 answer[i] 为 false 。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UF</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.uf = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">        self.count_= n</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        ux, uy = self.find(x), self.find(y)</span><br><span class="line">        <span class="keyword">if</span> ux == uy:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="comment"># 规模小的优先合并</span></span><br><span class="line">        <span class="keyword">if</span> self.uf[ux] &lt; self.uf[uy]:</span><br><span class="line">           self.uf[ux] += self.uf[uy]</span><br><span class="line">           self.uf[uy] = ux</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">           self.uf[uy] += self.uf[ux]</span><br><span class="line">           self.uf[ux] = uy</span><br><span class="line">        self.count_ -= <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        r = x</span><br><span class="line">        <span class="keyword">while</span> self.uf[x] &gt;= <span class="number">0</span>:</span><br><span class="line">            x = self.uf[x]</span><br><span class="line">        <span class="comment"># 路径压缩</span></span><br><span class="line">        <span class="keyword">while</span> r != x:</span><br><span class="line">            self.uf[r],r = x,self.uf[r]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">areConnected</span><span class="params">(self, n: int, threshold: int, queries: List[List[int]])</span> -&gt; List[bool]:</span></span><br><span class="line">        uf = UF(n + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 通过连接大于阈值的因子与所有可能的节点来判断连通，时间复杂度O(n·logn)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(threshold + <span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span> * i, n + <span class="number">1</span>, i):</span><br><span class="line">                uf.union(i, j)</span><br><span class="line"></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> a, b <span class="keyword">in</span> queries:</span><br><span class="line">            ans.append(<span class="literal">True</span> <span class="keyword">if</span> uf.find(a) == uf.find(b) <span class="keyword">else</span> <span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h3 id="四、最长连续序列"><a href="#四、最长连续序列" class="headerlink" title="四、最长连续序列"></a>四、最长连续序列</h3><blockquote>
<p>给定一个未排序的整数数组 nums ，找出数字连续的最长序列（不要求序列元素在原数组中连续）的长度。</p>
<p>进阶：你可以设计并实现时间复杂度为 O(n) 的解决方案吗？</p>
</blockquote>
<p>转换为图连通问题，用并查集处理相邻数字代表的节点，时间复杂度O(n)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnionFind</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.uf = [<span class="number">-1</span>] * n</span><br><span class="line">        self.count_ = n</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        r = x</span><br><span class="line">        <span class="keyword">while</span> self.uf[x] &gt;= <span class="number">0</span>:</span><br><span class="line">            x = self.uf[x]</span><br><span class="line">        <span class="comment"># x为根节点</span></span><br><span class="line">        <span class="comment"># 路径压缩，摊平</span></span><br><span class="line">        <span class="keyword">while</span> r != x:</span><br><span class="line">            self.uf[r], r = x, self.uf[r]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        ux, uy = self.find(x), self.find(y)</span><br><span class="line">        <span class="keyword">if</span> ux == uy:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="comment"># 按秩合并，把规模大的合并到规模小的群里，群的元素数量是-uf[root]</span></span><br><span class="line">        <span class="keyword">if</span> self.uf[ux] &lt; self.uf[uy]:</span><br><span class="line">            self.uf[ux] += self.uf[uy]</span><br><span class="line">            self.uf[uy] = ux</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.uf[uy] += self.uf[ux]</span><br><span class="line">            self.uf[ux] = uy</span><br><span class="line">        self.count_ -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(self)</span>:</span>  <span class="comment"># 连通子图的数量</span></span><br><span class="line">        <span class="keyword">return</span> self.count_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestConsecutive</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        uf = UnionFind(len(nums))</span><br><span class="line">        value2idx = &#123;&#125;</span><br><span class="line">        <span class="comment"># O(n)</span></span><br><span class="line">        <span class="keyword">for</span> i, a <span class="keyword">in</span> enumerate(nums):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">not</span> <span class="keyword">in</span> value2idx:</span><br><span class="line">                value2idx[a] = i</span><br><span class="line">        </span><br><span class="line">        visited = set()</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># O(n)</span></span><br><span class="line">        <span class="keyword">for</span> i, a <span class="keyword">in</span> enumerate(nums):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">in</span> visited:  <span class="comment"># 排除已经处理的中间值</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            visited.add(a)</span><br><span class="line">            <span class="keyword">if</span> a + <span class="number">1</span> <span class="keyword">in</span> value2idx <span class="keyword">and</span> a + <span class="number">1</span> <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                uf.union(i, value2idx[a + <span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> a - <span class="number">1</span> <span class="keyword">in</span> value2idx <span class="keyword">and</span> a - <span class="number">1</span> <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                uf.union(i, value2idx[a - <span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        max_ = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            max_ = max(max_, -uf.uf[uf.find(i)])</span><br><span class="line">        <span class="keyword">return</span> max_</span><br></pre></td></tr></table></figure>
<h3 id="五、二分图最大匹配"><a href="#五、二分图最大匹配" class="headerlink" title="五、二分图最大匹配"></a>五、二分图最大匹配</h3><blockquote>
<p>你有一块棋盘，棋盘上有一些格子已经坏掉了。你还有无穷块大小为1 * 2的多米诺骨牌，你想把这些骨牌不重叠地覆盖在完好的格子上，请找出你最多能在棋盘上放多少块骨牌？这些骨牌可以横着或者竖着放。</p>
</blockquote>
<p>因为是1×2的棋子，所以一个棋子必须在横向或者纵向占两个格子，把棋盘相邻的格子看作点，并连接起来，则问题转换为一个二分图上的最大匹配问题。</p>
<p>这一题也可以用轮廓线DP来做（如代码），不过速度要慢很多。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">domino</span><span class="params">(self, n: int, m: int, broken: List[List[int]])</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""匈牙利算法，二分图最大匹配"""</span></span><br><span class="line">        edges = defaultdict(set)</span><br><span class="line">        broken = set([i * m + j <span class="keyword">for</span> i, j <span class="keyword">in</span> broken])</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">neighbor</span><span class="params">(x, y)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> x &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">yield</span> (x - <span class="number">1</span>) * m + y</span><br><span class="line">            <span class="keyword">if</span> y &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">yield</span> x * m + y - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> x + <span class="number">1</span> &lt; n:</span><br><span class="line">                <span class="keyword">yield</span> (x + <span class="number">1</span>) * m + y</span><br><span class="line">            <span class="keyword">if</span> y + <span class="number">1</span> &lt; m:</span><br><span class="line">                <span class="keyword">yield</span> x * m + y + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 建一个二分图, O(m * n)</span></span><br><span class="line">        left_nodes = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">                    point = i * m + j</span><br><span class="line">                    <span class="keyword">if</span> point <span class="keyword">in</span> broken:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="keyword">if</span> (i + j) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">                        left_nodes.append(point)</span><br><span class="line">                    <span class="keyword">for</span> each <span class="keyword">in</span> neighbor(i, j):</span><br><span class="line">                        <span class="keyword">if</span> each <span class="keyword">not</span> <span class="keyword">in</span> broken:</span><br><span class="line">                            edges[point].add(each)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 匈牙利算法求最大匹配数量，O(|V| * |E|) = O(m * n * m * n)</span></span><br><span class="line">        match_l = defaultdict(<span class="keyword">lambda</span>: <span class="literal">None</span>)</span><br><span class="line">        match_r = defaultdict(<span class="keyword">lambda</span>: <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(p, visited)</span>:</span></span><br><span class="line">            <span class="keyword">nonlocal</span> match_l, match_r</span><br><span class="line">            visited.add(p)</span><br><span class="line">            <span class="keyword">for</span> neighbor_p <span class="keyword">in</span> edges[p]:</span><br><span class="line">                <span class="comment"># 直接找到了一个连接非匹配点的非匹配边</span></span><br><span class="line">                <span class="keyword">if</span> match_r[neighbor_p] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    match_l[p] = neighbor_p</span><br><span class="line">                    match_r[neighbor_p] = p</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> neighbor_p <span class="keyword">in</span> edges[p]:</span><br><span class="line">                nxt = match_r[neighbor_p]</span><br><span class="line">                <span class="keyword">if</span> nxt <span class="keyword">in</span> visited:  <span class="comment"># 增广路不要涉及重复节点</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="comment"># 递归寻找下一组  左 -&gt; 右，如果右边是非匹配节点，那么递归的逆转增广路选择</span></span><br><span class="line">                <span class="keyword">if</span> dfs(nxt, visited):  </span><br><span class="line">                    match_l[p] = neighbor_p</span><br><span class="line">                    match_r[neighbor_p] = p</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> left_nodes:</span><br><span class="line">            <span class="keyword">if</span> dfs(node, set()):</span><br><span class="line">                res += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">domino_dp</span><span class="params">(self, n: int, m: int, broken: List[List[int]])</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""插头DP"""</span></span><br><span class="line">        valid = defaultdict(bool)</span><br><span class="line">        NO, HENG, SHU = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">        <span class="comment"># 记录坏掉的地板，这里不能放东西</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> broken:</span><br><span class="line">            valid[x * m + y] = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 最后增加一行坏掉的地板，表示最后一行只能横着放</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> (m * n, (m + <span class="number">1</span>) * n):</span><br><span class="line">            valid[i] = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 0代表不放，1代表横着放，2代表竖着放，用m位三进制表示轮廓线的状态</span></span><br><span class="line">        <span class="comment"># [3 ^ m, 3]</span></span><br><span class="line">        states_transition = [[<span class="literal">None</span>] * <span class="number">3</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span> ** m)]</span><br><span class="line">        elements = &#123;&#125;</span><br><span class="line">        first = <span class="keyword">lambda</span> s: s // (<span class="number">3</span> ** (m - <span class="number">1</span>))  <span class="comment"># 轮廓线第一个元素的状态，在当前元素上方</span></span><br><span class="line">        second = <span class="keyword">lambda</span> s: s % (<span class="number">3</span> ** (m - <span class="number">1</span>)) // (<span class="number">3</span> ** (m - <span class="number">2</span>)) </span><br><span class="line">        last = <span class="keyword">lambda</span> x: x % <span class="number">3</span>  <span class="comment"># 轮廓线最后一个元素的状态，在当前元素左边（如果x &gt; 0）</span></span><br><span class="line">        row = <span class="keyword">lambda</span> p: p // m</span><br><span class="line">        col = <span class="keyword">lambda</span> p: p % m</span><br><span class="line">        <span class="comment"># 预先计算轮廓线状态转移</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span> ** m):</span><br><span class="line">            f, l = first(i), last(i)</span><br><span class="line">            sec = second(i)</span><br><span class="line">            elements[i] = (f, sec, l)</span><br><span class="line">            remain = i % (<span class="number">3</span> ** (m - <span class="number">1</span>))</span><br><span class="line">            states_transition[i] = [</span><br><span class="line">                remain * <span class="number">3</span> + NO,</span><br><span class="line">                remain * <span class="number">3</span> + HENG,</span><br><span class="line">                remain * <span class="number">3</span> + SHU</span><br><span class="line">            ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">        @lru_cache(None)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(pos, state)</span>:</span></span><br><span class="line">            x, y = row(pos), col(pos)</span><br><span class="line">            <span class="comment"># 终止条件</span></span><br><span class="line">            <span class="keyword">if</span> pos == m * n:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            first, second, last = elements[state]</span><br><span class="line">            <span class="comment"># 当前位置不能放置</span></span><br><span class="line">            new_state = states_transition[state][NO]</span><br><span class="line">            ans = dfs(pos + <span class="number">1</span>, new_state)</span><br><span class="line">            <span class="keyword">if</span> first == SHU <span class="keyword">or</span> (y &gt; <span class="number">0</span> <span class="keyword">and</span> last == HENG) <span class="keyword">or</span> valid[pos]:</span><br><span class="line">                ...</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 可以横着放</span></span><br><span class="line">                <span class="keyword">if</span> y + <span class="number">1</span> &lt; m <span class="keyword">and</span> <span class="keyword">not</span> valid[pos + <span class="number">1</span>] <span class="keyword">and</span> second != SHU:</span><br><span class="line">                    new_state = states_transition[state][HENG]</span><br><span class="line">                    ans = max(ans, <span class="number">1</span> + dfs(pos + <span class="number">1</span>, new_state))</span><br><span class="line">                <span class="comment"># 可以竖着放</span></span><br><span class="line">                <span class="keyword">if</span> x + <span class="number">1</span> &lt; n <span class="keyword">and</span> <span class="keyword">not</span> valid[pos + m]:</span><br><span class="line">                    new_state = states_transition[state][SHU]</span><br><span class="line">                    ans = max(ans, <span class="number">1</span> + dfs(pos + <span class="number">1</span>, new_state))</span><br><span class="line">            <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dfs(<span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>BFS</tag>
        <tag>Graph</tag>
        <tag>并查集</tag>
        <tag>最小生成树</tag>
        <tag>Greedy</tag>
      </tags>
  </entry>
  <entry>
    <title>BP,RNN 和 LSTM暨《Supervised Sequence Labelling with Recurrent Neural Networks-2012》阅读笔记</title>
    <url>/2016/11/23/machine-learning/BP-RNN-%E5%92%8C-LSTM%E6%9A%A8%E3%80%8ASupervised-Sequence-Labelling-with-Recurrent-Neural-Networks-2012%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="一、BackPropagation"><a href="#一、BackPropagation" class="headerlink" title="一、BackPropagation"></a>一、BackPropagation</h2><ul>
<li>$w_{jk}^l$：表示第$l-1$层第k个神经元到第$l$层第j个神经元的连接权重；</li>
<li>$b_j^l$：表示第$l$层第j个神经元的偏置；</li>
<li>$z_j^l$：表示第$l$层第j个神经元的带权输入；</li>
<li>$a_j^l$：表示第$l$层第j个神经元的激活值；</li>
<li>$\sigma$：表示一个激活函数(sigmoid,relu,tanh)；</li>
</ul>
<script type="math/tex; mode=display">z_j^l = \sum_kw_{jk}^la_k^{l-1} + b_j^l</script><script type="math/tex; mode=display">a_j^l = \sigma(\sum_kw_{jk}^la_k^{l-1} + b_j^l)=\sigma(z_j^l)</script><p>向量化上面的公式：</p>
<script type="math/tex; mode=display">a^l = \theta(w^la^{l-1} + b_l) = \theta(z^l) ​</script><ul>
<li>L(W,b,x,y)：表示损失函数，其中y是正确值，x是输入，下面是二次方损失函数的计算；</li>
</ul>
<script type="math/tex; mode=display">L(W,b,x,y) = \frac{1}{2N} \sum_N||y-a^l||^2</script><ul>
<li>阿达马（Hadamard）乘积，表示两个同维度向量a,b按元素相乘的结果向量，用$a\odot b$表示。</li>
<li>反向传播BP计算的目标：$\frac{\partial L}{\partial w_{jk}^l}$和$\frac{\partial L}{\partial b_j^l}$，也就是L对所有参数的偏导数。</li>
<li>$\delta^l_j$：表示第$l$层第j个单元的残差。也就是$\frac{\partial L}{\partial z^l_j}$。</li>
<li>$\delta_j^L = \frac{\partial L}{\partial a_j^L}\sigma’(z_j^L)$：表示顶层（输出层，假设是第L层）的残差，也就是损失函数对输出层的带权输入的偏导数。这个公式也可以进行向量化，为以下的形式：</li>
</ul>
<script type="math/tex; mode=display">\delta^L = \bigtriangledown_aL \odot \sigma'(z^L)</script><ul>
<li>当使用二次损失函数的时候，$\delta^l = (a^L - y) \odot \sigma’(z^L)$。</li>
<li>使用上一层的残差表示当前层的残差：</li>
</ul>
<script type="math/tex; mode=display">\delta^l = ((w^{l+1})^T\delta^{l+1})\odot \sigma'(z^l)</script><ul>
<li>最后，利用链式求导求出来的结果：</li>
</ul>
<script type="math/tex; mode=display">\frac{\partial L}{\partial w_{jk}^l} =  \frac{\partial L}{\partial z_{j}^l}\frac{\partial z_j^l}{\partial w_{jk}^l} = \delta^l_j a_k^{l-1}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial b^l_j} =  \frac{\partial L}{\partial z_{j}^l}\frac{\partial z_j^l}{\partial b_{j}^l} = \delta^l_j</script><p>向量化以上的结果，得到：</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial b} = \delta</script><script type="math/tex; mode=display">\frac{\partial L}{\partial w} = a_{in}\delta_{out}</script><h2 id="二、RNN"><a href="#二、RNN" class="headerlink" title="二、RNN"></a>二、RNN</h2><ul>
<li>RNN能够从之前的输入中映射出输出，具有一定程度的记忆能力，能够从之前的输入中存储网络的内部状态。</li>
<li>有足够多隐藏单元的RNN能够模拟任何Sequence-to-Sequence的映射。</li>
</ul>
<h3 id="1-RNN的forward-pass"><a href="#1-RNN的forward-pass" class="headerlink" title="1.RNN的forward pass"></a>1.RNN的forward pass</h3><p>注：下面这段使用的符号和上面的符号定义又有些不同，没办法，下面这篇论文就是这么用的。</p>
<p>Graves A. Supervised Sequence Labelling with Recurrent Neural Networks[M]. Springer Berlin Heidelberg, 2012.</p>
<ul>
<li>一个I个输入单元、H个隐藏单元和K个输出单元的RNN，输入序列是$x$，长度为$T$,$x_i^t$是输入单元$i$在$t$时刻的输入。</li>
<li>对每一个隐藏单元h，其t时刻的带权输入值$a_h^t$如下所示，也就是本文第一章中的$z^t_h$，(⊙o⊙)…好乱：</li>
</ul>
<script type="math/tex; mode=display">a_h^t = \sum_{i=1}^IW_{ih}x_i^t + \sum_{h'=1}^HW_{h'h}b_{h'}^{t-1}</script><ul>
<li>也就是，其带权输入值不仅与这一时刻的输入层单元值相关，还与上一时刻所有隐藏单元的激活值$b_{h’}^{t-1}$相关，而激活值计算如下所示，其中$\theta_h$是激活函数（tanh,sigmoid,….），这里是一样的：</li>
</ul>
<script type="math/tex; mode=display">b_h^t = \theta_h(a_h^t)</script><ul>
<li>整个网络从t=1开始迭代，$b_h^0$可以取全0，不过2006年之后也有人提出，使用非0来初始化能够提高RNN的性能和稳定性。</li>
<li>第k个输出单元t时刻的值按照下面的公式计算，就是每个隐藏单元激活值的连接，这里并没有引入偏置向量，不过可以认为是第0个单元，不影响：</li>
</ul>
<script type="math/tex; mode=display">a_k^t = \sum_{h=1}^H w_{hk}b_h^t</script><h3 id="2-RNN的Output层和损失函数"><a href="#2-RNN的Output层和损失函数" class="headerlink" title="2.RNN的Output层和损失函数"></a>2.RNN的Output层和损失函数</h3><p>按照实际情况选择RNN输出层的激活函数(sigmoid,softmax)和损失函数(cross_entropy)。</p>
<p>RNN应用在时间序列分析。</p>
<h3 id="3-RNN的backward-pass"><a href="#3-RNN的backward-pass" class="headerlink" title="3.RNN的backward pass"></a>3.RNN的backward pass</h3><p>现在计算出cross_entropy损失函数L关于输出y的偏导数，下一步就是计算L关于权重的偏导数。</p>
<p>已经提出的两种算法，可以有效的计算RNN的权重偏导数</p>
<ul>
<li>real time recurrent learning(RTRL,1987)</li>
<li>backpropagation through time(BPTT,1990)</li>
</ul>
<p>这里对BPTT进行介绍，因为它的思想更简单，而且计算效率更高。</p>
<p>BPTT也是通过链式法则的应用进行权值训练，但是对于递归神经网络，一个隐藏层单元激活值的影响不仅仅通过通往输出层的连接，还可以通过对下一时刻(timestep)隐藏层的影响。所以，残差$\delta^t_h$的计算也应该包含这两条线路，如下所示：</p>
<script type="math/tex; mode=display">\delta^t_h  = \theta'(a^t_h)(\sum_{k=1}^K\delta^t_kw_{hk} + \sum_{h'=1}^H \delta^{t+1}_{h'}w_{hh'})</script><p>这里的残差计算方式和第一章相同，$\delta^t_j = \frac{\partial L}{\partial a^t_j}$。其中a是带权输入。</p>
<p>这样从T开始计算，算到1，就可以得出$\frac{\partial L}{\partial w_{ij}}$，注意在这里整个网络在所有时刻使用的是同一权重。</p>
<h3 id="4-双向RNN"><a href="#4-双向RNN" class="headerlink" title="4.双向RNN"></a>4.双向RNN</h3><p>BRNN，1997、1999年提出。能够使用输入序列当前时刻后面的内容作为上下文。使用两个隐藏层，一个从前往后一个从后向前，两个隐藏层互不连接。</p>
<ul>
<li>前向过程</li>
</ul>
<p>从1到T，前向隐藏层的前向过程，存储每个t的激活值。</p>
<p>从T到1，后向隐藏层的前向过程，存储每个t的激活值。</p>
<p>对于所有t，使用前两步计算的激活值，进行输出层的前向过程。</p>
<ul>
<li>后向过程</li>
</ul>
<p>对所有t，计算输出层的$\delta$值。</p>
<p>对T到1，利用$\delta$进行前向隐藏层的后向过程。</p>
<p>对1到T，利用$\delta$进行后向隐藏层的前向过程。</p>
<p>BRNN可以应用于像蛋白质结构预测（2001）、时间序列任务、自动听写等应用中。</p>
<h3 id="5-序列雅克比矩阵（sequential-Jacobian原谅我不知道它怎么翻译）"><a href="#5-序列雅克比矩阵（sequential-Jacobian原谅我不知道它怎么翻译）" class="headerlink" title="5.序列雅克比矩阵（sequential Jacobian原谅我不知道它怎么翻译）"></a>5.序列雅克比矩阵（sequential Jacobian原谅我不知道它怎么翻译）</h3><h3 id="6-训练神经网络的一些tricks"><a href="#6-训练神经网络的一些tricks" class="headerlink" title="6.训练神经网络的一些tricks"></a>6.训练神经网络的一些tricks</h3><ul>
<li>梯度下降算法：批量vs在线，梯度下降、动量momentum<ul>
<li>一些批量梯度下降算法：RPROP、quickprop、conjugate gradients、L-BFGS</li>
<li>在线学习算法：stochastic meta-descent</li>
</ul>
</li>
<li>泛化方式<ul>
<li>early stopping</li>
<li>输入增加高斯噪声</li>
<li>权重增加高斯噪声</li>
</ul>
</li>
<li>输入的表示，输入的标准化（把输入向量化为均值0，标准差1）很重要。</li>
<li>权重的初始化，小而随机</li>
</ul>
<h2 id="三、LSTM"><a href="#三、LSTM" class="headerlink" title="三、LSTM"></a>三、LSTM</h2><h3 id="1-网络结构"><a href="#1-网络结构" class="headerlink" title="1.网络结构"></a>1.网络结构</h3><ul>
<li>Memory Block(Memory Cell)<ul>
<li>输入：$x_t$</li>
<li>input gate输入门：$i_t$</li>
<li>output gate输出门：$O_t$</li>
<li>forget gate遗忘门：$f_t$</li>
<li>细胞状态：$C_t$</li>
<li>单元激活值：$h_t$</li>
</ul>
</li>
</ul>
<p>盗个图过来吧，文字还是有点描述不清楚，以后别记不得了。</p>
<p><img src="http://img.blog.csdn.net/20150928231034836" alt="这里写图片描述"></p>
<script type="math/tex; mode=display">f_t = \sigma(W_f\cdot [h_{t-1},x_t] + b_f)</script><script type="math/tex; mode=display">i_t = \sigma(W_i\cdot [h_{t-1},x_t] + b_i)</script><script type="math/tex; mode=display">\tilde{C}_t = tanh(W_C\cdot [h_{t-1},x_t] + b_C)</script><script type="math/tex; mode=display">{C}_t = f_t\cdot C_{t-1}+i_t\cdot \tilde{C}_t</script><script type="math/tex; mode=display">O_t = \sigma(W_o\cdot [h_{t-1},x_t] + b_o)</script><script type="math/tex; mode=display">h_t=tanh(C_t)\cdot O_t</script><h3 id="2-LSTM的变种和进一步研究"><a href="#2-LSTM的变种和进一步研究" class="headerlink" title="2.LSTM的变种和进一步研究"></a>2.LSTM的变种和进一步研究</h3><ul>
<li>2000年，增加了窥视孔连接Peephole，所有或者部分公式中的$[h<em>{t-1},x_t]$替换为$[C</em>{t-1},h_{t-1},x_t]$。</li>
<li>配对了遗忘门和输入门，即$i_t=1-f_t$。</li>
<li>2014年，Gated Recurrent Unit（GRU），合并了细胞状态和单元激活值，合并输入门和遗忘门为更新门，模型更加简单。</li>
<li>Depth Gated RNNs, Clockwork RNNs等</li>
<li>注意力机制、Grid LSTM等</li>
</ul>
<h3 id="3-预处理"><a href="#3-预处理" class="headerlink" title="3.预处理"></a>3.预处理</h3><p>预处理能够通过采样减少输入序列的长度，从而使得LSTM能够更好的进行处理，上下文信息更少。</p>
<h3 id="4-双向LSTM"><a href="#4-双向LSTM" class="headerlink" title="4.双向LSTM"></a>4.双向LSTM</h3><h3 id="5-LSTM的正向（激活）和反向（BPTT）过程"><a href="#5-LSTM的正向（激活）和反向（BPTT）过程" class="headerlink" title="5.LSTM的正向（激活）和反向（BPTT）过程"></a>5.LSTM的正向（激活）和反向（BPTT）过程</h3><p>下面的过程都是带peephole的LSTM。输入公式还是不如手写方便，还是手写吧。。。</p>
<ul>
<li>正向过程：</li>
</ul>
<p><img src="/images/Fh4TB2PmQXMqaADGnb40YJ9d2HiT.jpg" alt="LSTM正向过程"></p>
<ul>
<li>反向过程：</li>
</ul>
<p><img src="/images/FjcVrhalhfm_0tt6jHDzzckG-h9X.jpg" alt="forward pass"></p>
<h3 id="6-参考"><a href="#6-参考" class="headerlink" title="6.参考"></a>6.参考</h3><p><a href="http://blog.csdn.net/ycheng_sjtu/article/details/48792467" target="_blank" rel="noopener">http://blog.csdn.net/ycheng_sjtu/article/details/48792467</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>RNN</tag>
        <tag>BP</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>OGD和正则化</title>
    <url>/2015/07/26/machine-learning/OGD%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    <content><![CDATA[<h3 id="一、OGD（Online-Gradient-Descent）"><a href="#一、OGD（Online-Gradient-Descent）" class="headerlink" title="一、OGD（Online Gradient Descent）"></a>一、OGD（Online Gradient Descent）</h3><p>批量梯度下降法（Batch Gradient Descent）也就是相对于这里的在线梯度下降来说的离线梯度下降，它每次迭代使用所有数据计算出目标函数的梯度方向，但是代价较高</p>
<p>在线梯度下降，当前迭代只根据当前的数据来计算梯度，有一定偏差，迭代次数可能增加，但代价较小</p>
<p>随机梯度下降（SGD），随机选择一定量的数据样本在每次迭代中确定梯度，相对于BGD代价较小</p>
<h3 id="二、正则化"><a href="#二、正则化" class="headerlink" title="二、正则化"></a>二、正则化</h3><ul>
<li>1.规则化符合奥卡姆剃刀的原则：选择能解释已知数据的最简单的模型，规则化用来保证模型的简单化（不会过分拟合）</li>
<li>2.从贝叶斯估计的角度来看，规则化项对应于模型的先验概率</li>
<li>3.规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)</li>
</ul>
<script type="math/tex; mode=display">w^* = arg\ min_w \sum_i{L(y_i,f(x_i;w)) + λ\Omega(w)}</script><p>公式第一项是为了拟合训练数据，不同的模型使用不同的Loss函数；第二项是为了规则化，有以下几种{0范数(norm)，1范数，2范数，迹范数，Frobenius范数，核范数}等</p>
<p>0范数也就是L0正则，1范数就是L1正则，他们都可以使得特征变得稀疏，但L0计算代价太大，因此通常使用L1正则</p>
<p>2范数也就是L2正则，是特征向量各元素的平方和开根号，它的作用是放置过拟合</p>
<p>L2正则可以防止参数估计的过拟合，但是选择合适的λ比较困难，需要交叉验证。如果有个特征与输出结果不相关，则L2会给一个特别小的值，但是不会为0。<br>L1正则会产生稀疏解，即不相关的的特征对应的权重为0，就相当于降低了维度。但是L1的求解复杂度要高于L2,并且L1更为流行。</p>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a href="https://mli7.wordpress.com/2011/04/08/online_learning_2/" target="_blank" rel="noopener">漫谈在线学习：在线梯度下降</a></p>
<p><a href="http://blog.csdn.net/zouxy09/article/details/24971995" target="_blank" rel="noopener">机器学习中的范数规则化之（一）L0、L1与L2范数</a></p>
<p><a href="g.cn">斯坦福机器学习公开课第七课——正则化</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《Windows10安装Theano和Keras并配置加速》</title>
    <url>/2016/11/15/machine-learning/%E3%80%8AWindows10%E5%AE%89%E8%A3%85Theano%E5%92%8CKeras%E5%B9%B6%E9%85%8D%E7%BD%AE%E5%8A%A0%E9%80%9F%E3%80%8B/</url>
    <content><![CDATA[<h2 id="Windows10安装Theano和Keras并配置加速"><a href="#Windows10安装Theano和Keras并配置加速" class="headerlink" title="Windows10安装Theano和Keras并配置加速"></a>Windows10安装Theano和Keras并配置加速</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><ul>
<li>1.Windows10-64位-14951.1000</li>
<li>2.Visual Studio2015-Community</li>
<li>3.NVIDIA-GTX960显卡</li>
</ul>
<h4 id="一、安装Python和Theano"><a href="#一、安装Python和Theano" class="headerlink" title="一、安装Python和Theano"></a>一、安装Python和Theano</h4><ul>
<li>1.下载并安装WinPython3.4.4-64bit（WinPython3.4.3.2-64bit中所带的numpy版本不到1.10，使用keras中会出现问题）。</li>
<li>2.从Github上Clone下Theano的工程，在dist目录下会有.egg的二进制安装包，在命令行输入<code>easy_install theano-0.9.0-py34-AMD64.egg</code>安装theano，而后通过<code>pip show theano</code>命令查看是否安装成功。</li>
<li>3.此时，命令行进入Python环境后，输入<code>import theano</code>如无错误，则表示theano安装成功。</li>
<li>4.可以使用<code>theano.test()</code>命令对theano进行测试。</li>
<li>5.此时，可以使用theano的CPU版本。</li>
</ul>
<h3 id="二、配置GPU加速"><a href="#二、配置GPU加速" class="headerlink" title="二、配置GPU加速"></a>二、配置GPU加速</h3><ul>
<li>1.下载并安装CUDAv8.0和CuDNNv5.1，直接都选择了最新版本，因为CUDA7.5只支持VS2013，而机器上安装的是VS2015，再装一个VS感觉坑会很大，所以直接装最新版本CUDA。</li>
<li>2.下载MinGW套件，提供GCC编译环境，需要把MinGW的相应目录加入环境变量。</li>
<li>3.配置环境变量，提供CL编译时的Classpath，添加如下两个环境变量，并根据你系统的实际情况稍作修改即可，可能需要Windows10相应的SDK。十分感谢<a href="http://www.ituring.com.cn/article/207389" target="_blank" rel="noopener">图灵社区的这篇博客</a>。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INCLUDE环境变量：C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include;C:\Program Files (x86)\Windows Kits\10\Include\10.0.14393.0\ucrt;C:\Program Files (x86)\Windows Kits\10\Lib\10.0.14393.0\um\arm64</span><br><span class="line">LIB环境变量：C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\lib;C:\Program Files (x86)\Windows Kits\10\Lib\10.0.14393.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\Lib\10.0.14393.0\um\x64</span><br></pre></td></tr></table></figure>
<ul>
<li>4.在C:\User[YourName]\目录下新建<strong>.theanorc</strong>文件，内容为如下所示。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="keyword">global</span>] </span><br><span class="line">device = gpu</span><br><span class="line">floatX = float32</span><br><span class="line"></span><br><span class="line">[blas]</span><br><span class="line">ldflags = -LC:\\OpenBLAS-v0<span class="number">.2</span><span class="number">.19</span>-Win64-int32\\bin -lopenblas</span><br><span class="line"></span><br><span class="line">[cuda]</span><br><span class="line">root = C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8<span class="number">.0</span></span><br><span class="line"></span><br><span class="line">[nvcc]</span><br><span class="line">compiler_bindir = C:\Program Files (x86)\Microsoft Visual Studio <span class="number">14.0</span>\VC\bin</span><br><span class="line">fastmath = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<ul>
<li>5.此时可以使用GPU版本的Theano。</li>
</ul>
<ul>
<li>7.把CuDNN下的目录解压复制到CUDA相应目录中，即可使用CuDNN加速。</li>
</ul>
<h4 id="三、安装Keras"><a href="#三、安装Keras" class="headerlink" title="三、安装Keras"></a>三、安装Keras</h4><ul>
<li>1.使用<code>pip install keras</code>即可安装。</li>
<li>2.WinPython自带了keras的最新版本，这里是1.1.1。</li>
</ul>
<h4 id="四、配置OPENBLAS加速"><a href="#四、配置OPENBLAS加速" class="headerlink" title="四、配置OPENBLAS加速"></a>四、配置OPENBLAS加速</h4><ul>
<li>1.下载OPENBLAS：<a href="http://ncu.dl.sourceforge.net/project/openblas/v0.2.19/OpenBLAS-v0.2.19-Win64-int32.zip" target="_blank" rel="noopener">链接</a>和Mingw64<em>dll.zip:</em><a href="http://ncu.dl.sourceforge.net/project/openblas/v0.2.14/mingw64_dll.zip" target="_blank" rel="noopener">链接</a></li>
</ul>
<ul>
<li>2.测试方法：<ul>
<li>1.使用Theano/misc/check_blas.py进行测试。</li>
<li>2.使用如下语句进行测试。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line">id(numpy.dot)==id(numpy.core.multiarray.dot) //<span class="literal">False</span>代表使用了OPENBLAS加速</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>1<a href="http://keras-cn.readthedocs.io/en/latest/getting_started/keras_windows/" target="_blank" rel="noopener">这篇Keras官方安装指南很有用</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title>《Greedy Layer-Wise Training of Deep Networks》</title>
    <url>/2016/11/15/machine-learning/%E3%80%8AGreedy-Layer-Wise-Training-of-Deep-Networks%E3%80%8B/</url>
    <content><![CDATA[<h2 id="Greedy-Layer-Wise-Training-of-Deep-Networks"><a href="#Greedy-Layer-Wise-Training-of-Deep-Networks" class="headerlink" title="Greedy Layer-Wise Training of Deep Networks"></a>Greedy Layer-Wise Training of Deep Networks</h2><p>本文对Hinton逐层贪婪非监督的参数初始化学习方法（2006）的探索，探究其原理、应用于连续型输入、应用于从输入结构难以窥探预测变量性质的监督学习中。</p>
<p>对于比较复杂、不断变化的函数，由于采用分段线性近似的方式拟合随着输入变量的增加、拟合的段数在指数级增长，因此需要其他的拟合（学习）方式。</p>
<p>如果这个函数的不同部分是相互相关的可以被用来相互预测，那么non-local的学习算法能够预测没有被训练集覆盖的部分，这种能力可以被用在人工智能方面。</p>
<p>多个非线性函数的组合这种表示方式能够有效的表达复杂函数。比如，d个输入的奇偶函数用高斯SVM需要$O(2^d)$个参数表示，而一个隐藏层的神经网络需要$O(d^2)$，多层网络需要$O(d)$，递归神经网络RDD只需要$O(1)$ 。而且，<strong>参数越多，需要的训练集数量就越大</strong>。</p>
<p>训练过程：</p>
<ul>
<li>greedy pre-training </li>
<li>unsupervised each layer</li>
<li>fine-tuning whole network</li>
</ul>
<p>一个$l$层的深度置信网络DBN：</p>
<ul>
<li>对于第i+1层和第i层之间的连接，权重矩阵$W^i$</li>
<li>对于第i+1层和第i层之间的连接，偏置向量$b^i$</li>
<li>对第i+1层和i层之间第j个单元，其值为$sigmoid(\sum<em>{k=1}^{n(i+1)} W^i</em>{kj}g_k^{i+1} + b_j^i)$</li>
<li>$l-1$层和$l$层（即最上面两层）之间的连接是受限玻尔兹曼机RBM</li>
<li>Gibbs马尔科夫链和对数似然梯度</li>
</ul>
<p>扩展DBN，使其能够处理连续值：</p>
<ul>
<li>Hinton的DBN模型只能处理二值输入，有一种作弊的方式来处理连续输入就是把输入归一化到[0,1]而后把值作为该输入取1的概率，但是这种方式只对像素灰度值的输入效果不错。</li>
<li>线性能量函数：概率密度函数是指数类型或者截断指数，<strong>差异对比更新过程</strong>不变（因为它们只依赖于能量函数对于参数的偏导数），只有<strong>采样过程</strong>需要根据条件概率密度函数改变。</li>
<li>指数能量函数，高斯单元（Gaussian units）</li>
<li>在隐藏层单元中使用连续值，使用高斯单元和隐藏层的指数单元也有一些缺点，通过高斯单元会产生纯粹的线性变换。</li>
<li>进行了实验，对比了二值输入、高斯输入的DBN以及逻辑回归，居然逻辑回归结果最好。。。</li>
</ul>
<p>进行了一些实验，说明了逐层预训练产生很好效果的原因：</p>
<ul>
<li>利用自编码器的交叉熵作为预训练的损失函数。</li>
<li>利用监督式的逐层预训练。</li>
<li>逐层预训练，但是改变为整个网络一起训练，这样停止预训练的阈值的形式更加简单。</li>
</ul>
<p>最后，对于输入的分布和y不太相关的监督学习。采用无监督预训练效果不好，此时需要采用无监督和监督式相结合的预训练方法。</p>
<p>论文的附录中，有很多算法的伪代码，可以参考。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>《Scaling Learning Algorithms towards AI》</title>
    <url>/2016/11/10/machine-learning/%E3%80%8AScaling-Learning-Algorithms-towards-AI%E3%80%8B/</url>
    <content><![CDATA[<h2 id="Scaling-Learning-Algorithms-towards-AI"><a href="#Scaling-Learning-Algorithms-towards-AI" class="headerlink" title="Scaling Learning Algorithms towards AI"></a>Scaling Learning Algorithms towards AI</h2><h4 id="1-Shallow-Arch-VS-Deep-Arch"><a href="#1-Shallow-Arch-VS-Deep-Arch" class="headerlink" title="1.Shallow Arch VS. Deep Arch"></a>1.Shallow Arch VS. Deep Arch</h4><ul>
<li><p>浅层结构分类？</p>
<ul>
<li>固定的预处理层 + 线性预测器（逻辑回归，感知机）</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">f(x) = \sum_{i=1}^k w_i\phi_i(x)</script><ul>
<li>模式匹配器 + 线性预测器（核方法，核就是template matcher）<ul>
<li>需要的计算次数多，local kernel functions</li>
<li>与之相比，局部的特征检测器（输出与未相连的输入并不相关）更优</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">f(x) = b + \sum_{i=1}^n \alpha_iK(x,x_i)</script><ul>
<li><p>简单的基础函数（线性组合、<strong>高斯径向基函数</strong>） + 线性预测器（只有一层隐藏层的神经网络、RBF、核函数是学习到的核方法、boosting算法）</p>
<ul>
<li>输出是参数化的非线性函数</li>
<li>损失函数的最小化是非凸问题。</li>
</ul>
</li>
<li><p>深层结构定义？</p>
<ul>
<li>多层参数化的非线性组件。</li>
<li>参数可训练。</li>
<li>层与层之间连接起来组成最终的函数。</li>
</ul>
</li>
<li><p>浅层结构损失函数往往是凸函数，深层结构损失函数基本都是非凸函数。</p>
</li>
<li><p>深层结构能够有效表示任意函数，而浅层结构不行（或者代价很大）。</p>
</li>
<li><p>对比的方式？</p>
<ul>
<li>通过考虑architecture的深度和宽度（每层元素数量）的trade-off，说明浅层结构的局限。<ul>
<li>即使是线性函数，增加层数也能利用任务的内在特征。</li>
<li>深层结构能够简洁表示的函数，不一定能用浅层结构简洁的表示。</li>
</ul>
</li>
<li>说明局部化和先验知识限制了浅层结构对knowledge的有效表示。</li>
</ul>
</li>
<li><p>深层结构的缺点？</p>
<ul>
<li>非凸优化：从很少的先验知识中学习到的复杂函数，一般只能用非凸函数的形式表示。所以这不是缺点。</li>
<li>训练时间：说明最近提出的突破性的训练深度网络的方法。所以这也不足以成为缺点了。<ul>
<li>Lecun使用梯度下降训练卷积网络，1998年。</li>
<li>Bengio 等人使用逐层贪婪初始化+梯度下降训练DBN，2007年。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2-Generalized-method-VS-Task-specific-method"><a href="#2-Generalized-method-VS-Task-specific-method" class="headerlink" title="2.Generalized method VS. Task-specific method"></a>2.Generalized method VS. Task-specific method</h4><ul>
<li>所有实际的学习算法都存在某些预先假设。</li>
<li>因此，不存在一个绝对的Generalized方法，只能寻找对某一任务很合适的方法。</li>
<li>比如，高级动物和人类具有的感知、控制、预测、逻辑、计划、语言理解等能力。</li>
<li>短期目标是具有人工智能的Agent。</li>
<li>关键在于，模型如何有效的捕获和表示required knowledge。</li>
<li><p>使用下面三个标准判定模型捕获和表示knowledge的有效程度。</p>
<ul>
<li>训练数据量（label数据量）</li>
<li>达到某一性能所需计算资源量</li>
<li>所需先验知识数量</li>
</ul>
</li>
<li>非参数化方法（solution的复杂度会增加的方法）：例如核方法、经典k-近邻、混合模型、多层神经网络等。<ul>
<li>会出现curse of dimensionality问题。</li>
<li>局部性和smooth的权衡，高斯核函数若$\sigma$越大，局部性越弱，但是函数约smooth，模型不容易表示比较复杂的函数。</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">K_\sigma(u,v) = e^{-\frac{1}{\sigma^2}||u-v||^2}</script><ul>
<li>1996，Wolpert，No-Free-Lunch定理，没有一个学习算法是绝对泛化的，一定存在一个数据集，该学习算法在测试集和训练集表现都很差（有限的VC维）。</li>
<li><p>因此，在所有函数的集合中，需要找到一个子集<code>AI-set</code>，其中的元素可能是我们的目标函数。这个子集可能很小，但是很容易找出。</p>
<ul>
<li>因为LeCun and Denker在1992年提出的理论。</li>
<li>视网膜和大脑中区域在胚胎发生时连接起来。如果每一个视神经是等概率的进行排列，那么上百万个连接的数量过大，基因中的比特数量都不够编码正确的连接方式，胚胎发育的时间也不够计算这些连接的。</li>
<li>其中一些（视神经与大脑）的连接和其他的比较起来一定是更简单、更容易计算的。其实也正是这样，连接模式能够使用很简单的语言进行描述，比如生物学中使用神经生长因子和注意力权重就可以描述出一个连接模式的拓扑结构。</li>
<li>所以同理，我们能够根据相对较少的信息，来确定AI-set集合。</li>
</ul>
</li>
<li>如后文所说，尽量寻找更加泛化的学习方法，才是应该做的事情。</li>
</ul>
<h4 id="3-Learning-Models"><a href="#3-Learning-Models" class="headerlink" title="3.Learning Models"></a>3.Learning Models</h4><p>先验知识（Prior Knowledge）通过下面的方式来植入学习模型中：</p>
<ul>
<li>数据的表示：预处理、特征抽取等。</li>
<li>Architecture：模型能够表示的函数的集合，以及参数设定。</li>
<li>损失函数loss function和正则化算子regularizer：表示了函数集合中函数如何被比较和评价，以及除去训练集影响的前提下对函数的偏好（先验Prior或者正则化惩罚项regularizer）。</li>
</ul>
<p>机器学习研究的策略分类[Hinton,2007]：</p>
<ul>
<li>失败主义（悲观型）。把函数集合限定在很小的范围内，靠人力精心设计的task-specified的预处理、参数和正则化项使模型能够work。虽然这种研究被称为失败，但是精心设计的预处理等过程，推动了机器学习的应用。还有一些方法，把中间特征用latent variable表示，而latent variable和observed variable之间的关系是硬编码在一起的。这种方法相当于把先验知识以图模型的结构确定下来并提供给模型。大部分对图模型的研究，人工设计核函数的核方法以及人工设计依赖结构和语义的图模型，都属于这种方法。属于短期看起来对特定任务效果很好，但其实<strong>没用</strong>的。</li>
<li>保守型。有些认为使用类似高斯核的泛化模型，就可以模拟出任何函数了，再加上正则化保证不要过拟合，一切就ok了。但其实不是这样的，核方法只能有效的表示函数集合中的一小部分。而且虽然有效的数据预处理加上核方法对某些应用看似有用，但它并不是通往true AI的正确道路。</li>
<li>乐观型。太过乐观，希望在尽量不加入先验知识的前提下，能够表示最大的AI-set函数集合。</li>
</ul>
<p>关于先验的几个概念：</p>
<ul>
<li>smooth prior：导数经常改变方向的函数最难学习，而smooth prior由于使用的是比较平滑的函数，因此无法单纯依靠smooth prior学习复杂函数。</li>
<li>strong prior：强先验，对应更小的函数学习范围。</li>
<li>broad prior：更宽泛的先验，对应更大的函数学习范围。</li>
</ul>
<p>陈述了观点：复杂函数是可能通过broad prior学习的，挑战在于学习算法的设计，如何把数据表示出来，而且表现出数据中蕴含的规律。并列举以下的例子和实验：</p>
<ul>
<li>Lecun1989、1998年关于视觉模式识别的研究。</li>
<li>这个复杂函数应该由多层的简单函数构成，<strong>不同层级的函数就是不同级别的抽象</strong>。低级别的抽象识别出相对简单的部分，高级别的抽象组合这些简单的部分，就像人类的学习过程一样。</li>
</ul>
<p>陈述了另一个观点，即在学习了一些任务后，在学习完全不同的新任务的时候，能够仅仅使用很少的时间和数据就能抓住其本质，就像人类一样。这个观点与现在的迁移学习很相似。</p>
<p>最后，陈述了学习模型的几个要素：</p>
<ul>
<li>灵活的定义先验知识的方式。</li>
<li>能够应用在深层结构的学习算法。</li>
<li>能够处理数以百万计参数的参数化学习算法。</li>
<li>学习算法训练速度要够快。</li>
<li>学习算法要能支持多任务学习和半监督式学习。</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式数据库BigTable</title>
    <url>/2015/12/19/machine-learning/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93BigTable/</url>
    <content><![CDATA[<p>Bigtable于2006年在<code>OSDI</code>（Symposium on Operating System Design and Implementation，操作系统方面的顶级会议）发表论文公开其设计。</p>
<p><strong>一、使用场景和解决的问题</strong></p>
<p><code>Bigtable</code>是一个分布式的结构化数据存储系统，具有以下特点：</p>
<ul>
<li>1.被设计分布式的存储和处理海量数据</li>
<li>2.应用在Google超过60种需求迥异的不同产品中</li>
<li>3.达到了适用性广泛、可扩展、高性能、高可用性的要求</li>
</ul>
<p><strong>二、数据模型</strong></p>
<p><img src="/images/Fo6mHI_s7JwrjftLQrOP3hDVIBl1.jpg" alt=""></p>
<p>Bigtable是一个稀疏的、分布式的、持久化的、多维的排序映射，具有行键、列键和时间戳这三个维度的键，键值对记录可以表示为<code>(row:string,column:string,time:int64) -&gt; string</code>。</p>
<p><code>行键</code>可以是任意字节串，通常有10-100字节。行的读写都是原子性的。Bigtable按照行键的字典序存储数据。Bigtable的表会根据行键自动划分为片（tablet），片是负载均衡的单元。最初表都只有一个片，但随着表不断增大，片会自动分裂，片的大小控制在100-200MB。</p>
<p><code>列</code>或者<code>列族</code>是第二级索引，每行拥有的列是不受限制的，可以随时增加减少。为了方便管理，列被分为多个列族（column family，是访问控制的单元），一个列族里的列一般存储相同类型的数据。一行的列族很少变化（改变的代价也较大），但是列族里的列可以随意添加删除。列键按照family:qualifier格式命名的。</p>
<p><img src="/images/FjGXOsuXwBHGIg5aWGv-YcmX_sNL.jpg" alt=""></p>
<p>时间戳是第三级索引。Bigtable允许保存数据的多个版本，版本区分的依据就是时间戳。时间戳可以由Bigtable赋值，代表数据进入Bigtable的准确时间，也可以由客户端赋值。数据的不同版本按照时间戳降序存储，因此先读到的是最新版本的数据。</p>
<p><img src="/images/FhKk8hK-shetU49hsigUQopSwHer.jpg" alt=""></p>
<p>以网页信息表为例，使用<code>反转URL</code>作为一级索引（行键），而列族可以是该URL中具有的<code>外链</code>，列族中的每个列代表一个外链，时间戳代表了数据的不同版本，比如URL的具体内容存储了多个版本。</p>
<p>从逻辑数据模型的角度，<code>多维</code>是能够从多个维度快速对数据进行筛选和查找，<code>排序</code>是采用行键排序记录，<code>稀疏</code>是Bigtable相比较关系数据库提供半结构化和无结构数据，同一个表中不同行的列可能完全不同，<code>分布式</code>提供了可扩展性、节点容错性和高带宽。</p>
<p><strong>三、接口</strong></p>
<ul>
<li>1.建立、删除、修改表、列族或者列</li>
<li>2.修改集群、表和列族的元数据</li>
<li>3.写入、删除、修改记录（对单行记录的操作具有原子性）</li>
<li>4.查找记录或者记录集合</li>
</ul>
<p><strong>四、系统结构</strong></p>
<p>Bigtable包含一个（逻辑上的）Master服务器和多个tablet服务器，客户端通过链接库来使用Bigtable服务。集群中动态的添加或者删除tablet服务器来适应负载变化。</p>
<p><img src="/images/FqfntXj2sRIQc1opAPX_DhkIGuiT.jpg" alt=""></p>
<p><code>Master</code>分配tablet到tablet服务器中，检测集群中tablet 服务器的状态（加入和失效），负责tablet服务器中的负载均衡和垃圾回收。</p>
<p><code>tablet</code>服务器负责存储tablet的集合，提供对存储的tablet的读写服务并分割容量增长到阈值（100—200MB）的tablet。底层以<code>SSTable</code>文件格式存储，和log一起存储在GFS上。</p>
<p><code>chubby</code>负责选举出集群的Master、存储tablet的<code>root table</code>、元数据表及存取访问控制表。</p>
<p><code>集群管理系统</code>负责集群的监控和错误处理。</p>
<ul>
<li>1.tablet的位置信息</li>
</ul>
<p>使用三层的类似B+树的结构存储tablet的位置信息。其中root table和元数据表存储在chubby服务器中的文件里。</p>
<p><img src="/images/FqcWIqQfBql89KY7V8xyuawOX76o.jpg" alt=""></p>
<p><strong>五、设计点</strong></p>
<ul>
<li>1.增加并发访问支持的设计——单行事务处理</li>
</ul>
<p>Bigtable只支持单行范围的<code>事务处理</code>，也就是只有单行读写具有<a href="https://zh.wikipedia.org/wiki/ACID" target="_blank" rel="noopener">ACID</a>保证，并不支持跨行的事务处理，这限定了锁粒度，可以更好的避免死锁和支持高并发访问。</p>
<ul>
<li>2.提高可扩展性的设计——无共享式架构（shared-nothing architecture）</li>
</ul>
<p>常规的并行关系数据库（比如MySQL）采用共享存储数据进行扩展，导致可扩展性受限制。而Bigtable集群的tablet server之间无数据共享，单独负责存储在上面的tablet提供的服务，易于扩展。</p>
<ul>
<li>3.提供海量数据支撑的设计——基于列族的数据存储模型</li>
</ul>
<p>由于<code>SSTable</code>使用<code>基于列族的存储</code>方案，而数据查询操作通常只会对于少数列族进行，因此这种存储方式减少了I/O数量。而且提供了对SSTable压缩和缓存的技术，以tablet服务器的CPU时间换取集群的网络带宽事件，提高了读取速度。</p>
<ul>
<li>4.方便重构复杂系统的设计——稀疏数据模型</li>
</ul>
<p>由于采用稀疏的数据存储模型，所以已存在海量数据的复杂系统易于改变存储模式（schema），便于重构。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="http://www.cs.cmu.edu/~pavlo/courses/fall2013/static/papers/bigtable-osdi06.pdf" target="_blank" rel="noopener">Chang F, Dean J, Ghemawat S, et al. Bigtable: a distributed storage system for structured data[C]// Proceedings of the 7th symposium on Operating systems design and implementation. USENIX Association, 2006:205-218.</a></li>
</ul>
]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
      </tags>
  </entry>
  <entry>
    <title>堆栈式去噪自编码器</title>
    <url>/2015/11/05/machine-learning/%E5%A0%86%E6%A0%88%E5%BC%8F%E5%8E%BB%E5%99%AA%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</url>
    <content><![CDATA[<h3 id="一、堆栈式去噪自编码器（Stacked-Denoising-Autoencoder）"><a href="#一、堆栈式去噪自编码器（Stacked-Denoising-Autoencoder）" class="headerlink" title="一、堆栈式去噪自编码器（Stacked Denoising Autoencoder）"></a>一、堆栈式去噪自编码器（Stacked Denoising Autoencoder）</h3><p>通过把前一层的潜在表示作为当前层的输入，去噪自编码器可以堆栈式叠加成一个深度神经网络。</p>
<p>对堆栈式去噪自编码器的训练分为两步。首先进行无监督式预训练（<code>unsupervised pre-training</code>），每次训练一层，也就是一个最小化一个自编码器的重构误差。当前k层训练完成，我们使用第k层的潜在表示来训练第k+1层。</p>
<p>当预训练完成，网络进行监督式的微调（<code>supervised fine-tuning</code>）。我们最小化监督学习任务的预测误差。因此，在网络的最顶层增加一个逻辑回归层，再像训练一个多层感知机（<code>MLP</code>）一样训练整个网络。</p>
<h3 id="二、Theano实现"><a href="#二、Theano实现" class="headerlink" title="二、Theano实现"></a>二、Theano实现</h3><p><a href="http://deeplearning.net/tutorial/SdA.html#sda" target="_blank" rel="noopener">DeepLearning.net</a>教程中基于Theano的实现和在Mnist数据集下的测试结果。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="http://deeplearning.net/tutorial/SdA.html#sda" target="_blank" rel="noopener">DeepLearning.net-SdA部分</a></li>
<li><a href="http://www.cs.toronto.edu/~larocheh/publications/vincent10a.pdf" target="_blank" rel="noopener">Vincent P, Larochelle H, Lajoie I, et al. Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion[J]. Journal of Machine Learning Research, 2010, 11(6):3371-3408.</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自编码器</tag>
      </tags>
  </entry>
  <entry>
    <title>协同过滤算法</title>
    <url>/2015/10/20/machine-learning/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>协同过滤算法就是依据用户与用户之间的相似度或者物品与物品之间的相似度来进行推荐的算法，也就是依靠相似度来进行<code>User-Item</code>矩阵的填充。</p>
<h3 id="一、基于记忆的协同过滤算法"><a href="#一、基于记忆的协同过滤算法" class="headerlink" title="一、基于记忆的协同过滤算法"></a>一、基于记忆的协同过滤算法</h3><h4 id="1-分类"><a href="#1-分类" class="headerlink" title="1.分类"></a>1.分类</h4><p>基于记忆的协同过滤算法直接对整个<code>User-Item</code>评分矩阵进行计算，通过相似度计算寻找相似近邻来产生推荐结果。主要分为以下两种：</p>
<ul>
<li><p>(1) User-Based-CF：基于用户的协同过滤</p>
</li>
<li><p>(2) Item-Based-CF：基于物品的协同过滤</p>
</li>
</ul>
<h4 id="2-过程"><a href="#2-过程" class="headerlink" title="2.过程"></a>2.过程</h4><p>基于记忆的协同过滤算法的<code>过程</code>主要有以下几步：</p>
<ul>
<li><p>(1) 计算相似度</p>
</li>
<li><p>(2) 选择相似近邻</p>
</li>
<li><p>(3) 预测评分</p>
</li>
<li><p>(4) 推荐</p>
</li>
</ul>
<h4 id="3-优缺点"><a href="#3-优缺点" class="headerlink" title="3.优缺点"></a>3.优缺点</h4><p>基于记忆的协同过滤算法的优点有：</p>
<ul>
<li><p>(1) 推荐的理论解释较清晰，易于理解</p>
</li>
<li><p>(2) 理论上推荐精度更高</p>
</li>
</ul>
<p>缺点有：</p>
<ul>
<li><p>(1) 维度爆炸问题 超高维度空间寻找最近邻计算代价太大</p>
</li>
<li><p>(2) 评分矩阵过于稀疏导致最近邻准确度降低</p>
</li>
<li><p>(3) 冷启动问题 新用户无法获得精确的推荐值</p>
</li>
</ul>
<h3 id="二、基于模型的协同过滤算法"><a href="#二、基于模型的协同过滤算法" class="headerlink" title="二、基于模型的协同过滤算法"></a>二、基于模型的协同过滤算法</h3><p>基于模型的协同过滤算法会首先离线处理原始数据矩阵，得到抽象化的特征模型，从而减少高维稀疏矩阵的计算时间。</p>
<h4 id="1-分类与过程"><a href="#1-分类与过程" class="headerlink" title="1.分类与过程"></a>1.分类与过程</h4><ul>
<li><p>矩阵因子分解（降维）</p>
<ul>
<li><p>(1) SVD 矩阵奇异值分解</p>
<ul>
<li>过程：设<code>User-Item</code>矩阵为$D<em>{m*n}$，使用$D</em>{m<em>n} = U_{m</em>i} <em> S_{i</em>i} <em> V^T_{i</em>n}$即可得到，其中$i$为非0奇异值的个数，而$V^T_{i*n}$矩阵主要反映了物品信息，由此矩阵可以得出相似度进而进行推荐</li>
</ul>
</li>
<li><p>(2) 交替最小二乘矩阵分解</p>
<ul>
<li>过程：交替的固定用户向量和物品向量，不断迭代来更新用户特征向量和物品特征向量，从而计算出用户对物品的预测评分进行推荐</li>
</ul>
</li>
<li><p>(3) PCA 主成分分析</p>
</li>
</ul>
</li>
<li><p>概率模型</p>
<ul>
<li>贝叶斯网络</li>
</ul>
</li>
<li><p>聚类</p>
<ul>
<li><p>K-Means聚类</p>
</li>
<li><p>模糊K-Means聚类</p>
</li>
</ul>
</li>
<li><p>关联规则</p>
</li>
</ul>
<h4 id="2-优缺点"><a href="#2-优缺点" class="headerlink" title="2.优缺点"></a>2.优缺点</h4><p>基于模型的协同过滤算法的优点有：</p>
<ul>
<li>速度较快，由于建立的模型维度通常比原数据集小，因此速度较快</li>
</ul>
<p>缺点有：</p>
<ul>
<li>不够直观，通常基于模型的CF算法的理论解释要难于基于记忆的CF算法</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐算法</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-Basics Recap-软核</title>
    <url>/2019/11/17/ML-Basics/</url>
    <content><![CDATA[<p>软核机器学习，记录结论和性质，推导细节省略。</p>
<p>所有向量默认是列向量。</p>
<h3 id="零、Basics"><a href="#零、Basics" class="headerlink" title="零、Basics"></a>零、Basics</h3><p>Sum Rule: <script type="math/tex">p(x_1) = \int p(x_1, x_2)dx_2</script></p>
<p>Product Rule: <script type="math/tex">p(x_1, x_2) = p(x_1)\cdot p(x_2|x_1) =  p(x_2)\cdot p(x_1|x_2)</script></p>
<p>Chain Rule: <script type="math/tex">p(x_1, \cdots, x_p) = \prod_i^p p(x_i|x_{<i})</script></p>
<p>Bayesian Rule: <script type="math/tex">p(x_2|x_1) = \frac{p(x_1|x_2)p(x_2)}{p(x_1)}</script></p>
<p>观测数据$X = {\cdots, x_i, \cdots}$, 标签$Y = {\cdots, y_i, \cdots}$，隐变量$Z = {\cdots, z_i, \cdots}$</p>
<p>生成模型：能够以某种方式估计到观测数据的分布，比如计算联合分布或者边缘分布来求解参数。</p>
<p>判别模型：无法获取观测数据的分布，只能估计条件分布$p(y|x)$。</p>
<h3 id="一、线性模型"><a href="#一、线性模型" class="headerlink" title="一、线性模型"></a>一、线性模型</h3><h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><p>线性回归模型<script type="math/tex">w^Tx</script>使用最小二乘法等价于噪声<script type="math/tex">\epsilon\sim N(0, \sigma^2)</script>（<script type="math/tex">y</script>为一维）的极大似然估计。</p>
<p>正则化和MAP的类似性：如果权重服从先验<script type="math/tex">w\sim N(0, \sigma_0^2)</script>，那么利用MAP极大后验估计的结果类似于MLE增加L2正则项；而如果权重服从Laplace分布，那么类似于增加L1正则项。</p>
<h4 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h4><p>1.硬分类（直接得到类别）</p>
<p>感知机：线性回归的基础上增加不可导的<script type="math/tex">sign(\cdot)</script>激活函数，没有解析解，使用梯度下降求解。</p>
<p>线性判别分析：将样本投影到一条向量上，并使得类内聚合、类间分开。</p>
<p>2.软分类（得到分类概率）</p>
<blockquote>
<p>判别式</p>
</blockquote>
<p>logistic回归：线性回归增加<script type="math/tex">sigmoid(\cdot)</script>激活函数，MLE作为目标函数，使用梯度下降求解。</p>
<blockquote>
<p>生成式</p>
</blockquote>
<p>高斯判别分析：假设<script type="math/tex">x</script>从两个方差相同的正态分布中采样，<script type="math/tex">y</script>代表样本属于哪个正态分布。通过贝叶斯公式，最大化以下先验的联合概率求解：<script type="math/tex">argmax P(X, Y) = argmax P(X|Y)P(Y)</script>，包含四个参数。推断时利用条件概率<script type="math/tex">P(Y|X)</script>进行推断。</p>
<script type="math/tex; mode=display">
y\sim Bernoulli(\phi)\\\\
x|y=1 \sim \mathcal{N}(\mu_1, \Sigma) \\\\
x|y=2 \sim \mathcal{N}(\mu_2, \Sigma)</script><p>朴素贝叶斯：对于属性之间增加了独立性假设，使得后验分布<script type="math/tex">P(x|y)</script>能够按照维度分解。</p>
<h3 id="二、指数族分布"><a href="#二、指数族分布" class="headerlink" title="二、指数族分布"></a>二、指数族分布</h3><p>具有如下形式概率密度函数的分布：<script type="math/tex">P(x|\eta) = h(x)exp(\eta^T\phi (x) - A(\eta))</script>。其中<script type="math/tex">\eta</script>是参数向量，<script type="math/tex">x\in R^{p}</script>。</p>
<p>充分统计量：<script type="math/tex">\phi(x)</script>，能够表述样本的统计性质。</p>
<p>对数配分函数：<script type="math/tex">A(\eta)</script>，因为转换为<script type="math/tex">\frac{1}{z} h(x)\cdot exp(\eta^T \phi(x))</script>的形式的时候，<script type="math/tex">A(\eta) = log(z)</script>。</p>
<p>对数配分函数的性质：<script type="math/tex">A'(\eta) = E_{p(x|\eta)}[\phi(x)]</script>。</p>
<p>极大似然估计的性质：<script type="math/tex">argmax_\eta \sum_i p(x_i|\eta) = argmax_\eta\sum_i (\eta^T\phi(x) - A(\eta))</script>，令导数为0可得<script type="math/tex">A_{MLE}'(\eta) = \frac{1}{N}\sum_i\phi(x_i)</script>。因此MLE对指数族分布来说只需要保留样本的充分统计量即可优化。</p>
<p>从最大熵的角度：1.没有已知信息的时候熵最大出现在均匀分布下。2.在有一个样本集合的情况下，利用经验分布<script type="math/tex">\hat p(x)</script>和最大熵原理，最终得到的分布<script type="math/tex">p(x)</script>一定是指数族分布，而且能够得到其PDF的形式。</p>
<h4 id="一维高斯分布"><a href="#一维高斯分布" class="headerlink" title="一维高斯分布"></a>一维高斯分布</h4><p>对于具有参数<script type="math/tex">\theta = (\mu, \sigma^2)</script>的一维高斯分布，转换为指数族分布的形式后，指数族的参数<script type="math/tex">\eta</script>可以求出来：<script type="math/tex">\eta^T = (\eta_1, \eta_2) = (\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2})</script>。</p>
<p>充分统计量：<script type="math/tex">\phi(x) = (\begin{array}{} x\\x^2 \end{array})</script>。对数配分函数的形式略复杂。</p>
<h3 id="三、支持向量机（SVM）"><a href="#三、支持向量机（SVM）" class="headerlink" title="三、支持向量机（SVM）"></a>三、支持向量机（SVM）</h3><p>主要关注点：间隔，对偶，核技巧。</p>
<p>分类：硬间隔（hard-margin），软间隔，kernel SVM。</p>
<p>用来解决二分类问题，其模型和感知机相同$f(x;w) = sign(w^Tx + b)$，属于线性分类模型中的硬分类模型。</p>
<h4 id="1-最大间隔分类器"><a href="#1-最大间隔分类器" class="headerlink" title="1.最大间隔分类器"></a>1.最大间隔分类器</h4><p>对于属于两个类别的<script type="math/tex">\{x_i, y_i\}_{i=1}^N, x_i\in R^p, y_i\in \{-1, 1\}</script>的数据样本，求解<script type="math/tex">w,b</script>使得<script type="math/tex">y_i(w^Tx + b) > 0</script>，也就是要正确划分。并满足如下的最大间隔：</p>
<script type="math/tex; mode=display">
max\ margin(w, b) = max\ min_{(w, b, x_i)} distance(w, b, x_i)</script><script type="math/tex; mode=display">
= max\ min_{(w,b,x_i)} \frac{1}{||w||} |w^Tx + b|=max_{(w,b)}\ \frac{1}{||w||} min_{(x_i)}  y_i(w^Tx + b)</script><p>那么存在一个正数<script type="math/tex">\gamma</script>等于这些样本到分类超平面的最小距离，那么上面的间隔约束可以转化为<script type="math/tex">max_{(w,b)}\ \frac{1}{||w||}\cdot \gamma</script>，可以约束<script type="math/tex">\gamma</script>为1，这个约束变成了<script type="math/tex">min_{(w,b)} \ ||w||=min_{w,b} \frac{1}{2} w^Tw</script>。且满足<script type="math/tex">y_i(w^Tx + b) \geq 1</script>。</p>
<blockquote>
<p>$\gamma$为什么可以取值为1？1）$aw^Tx + ab = 0$与$w^Tx + b = 0$是同一个超平面，2）且同一个向量到这两个超平面的距离相同。</p>
<p>所以可以用$\gamma$来缩放$w,b$使得$\gamma=w^Tx = 1$。</p>
</blockquote>
<h4 id="2-优化问题的转化。"><a href="#2-优化问题的转化。" class="headerlink" title="2.优化问题的转化。"></a>2.优化问题的转化。</h4><p>拉格朗日乘子法，将上面的有约束优化问题转化为无约束优化问题（两者实际上可以证明是等价的）：</p>
<script type="math/tex; mode=display">
min_{\{w,b\}}\ max_{\lambda}\ \mathcal{L}(w,b,\lambda) = \frac{1}{2}w^Tw +  \sum_{i=1}^N\lambda_i (1-y_i(w^Tx_i+b)),其中\lambda_i \geq 0,\lambda的乘项$\geq 0$。</script><p>对这个先min再max的原始优化问题，有一个对偶形式（先max再min）的问题，这个对偶形式总是<script type="math/tex">\leq</script>原问题的（弱对偶关系），而在当前的原始问题下这两个问题是等价的（<script type="math/tex">==</script>）「满足KKT条件」：</p>
<script type="math/tex; mode=display">
max_{\lambda}\ min_{\{w,b\}}\ \mathcal{L}(w,b,\lambda)</script><p>这个对偶问题比原始问题更容易求解。</p>
<p>然后从里到外推偏导数等于0的公式，推推推，得到了<script type="math/tex">min_{\lambda} f(\lambda, x, y)</script>的形式，w和b都被约去了。还有两个约束<script type="math/tex">\lambda \geq 0, \sum_{i=1}^N \lambda_i y_i = 0</script>。</p>
<h4 id="3-优化问题的求解。"><a href="#3-优化问题的求解。" class="headerlink" title="3.优化问题的求解。"></a>3.优化问题的求解。</h4><p>根据上面的最优化问题可以求出<script type="math/tex">\lambda</script>的值。然后就求出了参数<script type="math/tex">w,b</script>的最优解析<script type="math/tex">w^*,b^*</script>。</p>
<p>其中<script type="math/tex">w^* = \sum_{i=1}^{N} \lambda_i y_i x_i</script>是w的最佳值。b的最佳值如下，其中<script type="math/tex">x_k, y_k</script>是支持向量，距离超平面距离为1。</p>
<script type="math/tex; mode=display">
b^* = y_k - \sum_{i=1}^{N} \lambda_i y_i x_i^Tx_k</script><p>这样就求解出了硬间隔SVM。</p>
<h3 id="四、概率图模型（PGM）"><a href="#四、概率图模型（PGM）" class="headerlink" title="四、概率图模型（PGM）"></a>四、概率图模型（PGM）</h3><blockquote>
<p>什么样的方法可以转化为PGM？概率图又是如何得到的？</p>
</blockquote>
<p>高维随机变量的困难：联合概率计算复杂。</p>
<p>Naive Bayes：增加维度独立的假设进行简化。这样就可以方便的计算后验分布<script type="math/tex">p(x|y), x\in R^p</script>。但是这个假设过强。（样本x的第3维特征是年龄，第8维特征是收入，就无法独立）</p>
<p>Markov假设：并没有要求每个维度完全独立，而是第i个维度从依赖前i-1个维度变成只依赖第i-1个维度。但是这样顺序性的依赖太单一。（比如收集的样本x的第3维特征刚好依赖于第5、6维特征，就不能满足这个假设）</p>
<p>条件独立性假设：只是假设把维度分为若干个集合<script type="math/tex">\{X_1, \cdots, X_k\}</script>，其中两个集合的特征在第三个集合被观测到的时候独立。比如划分出A、B、C三个集合：$A\bot B|C$。</p>
<h4 id="1-概率图表示"><a href="#1-概率图表示" class="headerlink" title="1.概率图表示"></a>1.概率图表示</h4><ul>
<li>有向图<ul>
<li>离散：贝叶斯网络</li>
<li>连续：高斯贝叶斯网络</li>
</ul>
</li>
<li>无向图<ul>
<li>离散：马尔科夫网络</li>
<li>连续：高斯马尔科夫网络</li>
</ul>
</li>
</ul>
<h5 id="有向图-贝叶斯网络"><a href="#有向图-贝叶斯网络" class="headerlink" title="有向图-贝叶斯网络"></a>有向图-贝叶斯网络</h5><p>利用拓扑排序进行图的构造。那么根据构造了的有向图，就能够把高维随机变量的联合概率因子分解为条件概率和边缘概率的乘积。</p>
<p>对图的局部进行观察，会发现几种特殊的节点关系：</p>
<ul>
<li><p>1.从图上面看，某个节点上面若有tail-tail路径或者head-tail路径，那么当该节点事件发生时，这条路径被阻塞；间接相连的两个节点所代表的维度在这个条件下就是独立的。</p>
</li>
<li><p>2.而如果存在head-head路径（该节点C同时依赖于另外两个维度的变量A、B），那么刚好相反，如果C不发生那么A和B独立；反之路径连通，A和B不独立。</p>
</li>
<li><p>3.另一种特殊的局部拓扑，两个节点A、B同时指向节点C，节点C又指向节点D，那么若节点D被观测到，那么节点A和B也是连通的，就不独立了。</p>
</li>
</ul>
<p>D划分：一种划分特征节点构成的图的方式，对于划分出来的集合A、B、C，要满足<script type="math/tex">A\bot B|C</script>。那么C中所有的点都要是上面这几种关系中第一种关系中的特殊节点。D划分可以应用在bayes定理，用来计算<script type="math/tex">p(x_i|x_{\neq i})</script>，计算的结果只涉及到<script type="math/tex">parent(x_i), child(x_i)</script>，这两个集合中的节点叫做Markov blanket（马尔科夫毯）。</p>
<p>模型：</p>
<ul>
<li>单一，朴素贝叶斯：<script type="math/tex">x_i\bot x_j|y</script>，体现在图上就是y和<script type="math/tex">x_i, x_j</script>构成tail-to-tail的子图。</li>
<li>混合，GMM(Gaussian Mixture Model)，隐变量z表示聚类的类别，</li>
<li>时间，马尔科夫链和高斯过程。</li>
<li>动态系统，{HMM，LDS，粒子滤波}。</li>
<li>连续，高斯贝叶斯网络。</li>
</ul>
<h5 id="无向图-马尔科夫网络"><a href="#无向图-马尔科夫网络" class="headerlink" title="无向图-马尔科夫网络"></a>无向图-马尔科夫网络</h5><p>无向图组成的网络，适用于离散多维变量。</p>
<ul>
<li>Global Markov：根据无向图找到具有条件独立性的三个节点集合。</li>
<li>Local Markov：一个节点a，节点集合N，邻居节点集合<script type="math/tex">N(\cdot)</script>，<script type="math/tex">a \bot \{N - a - N(a)\}|N(a)</script>。</li>
<li>成对 Markov：不相连的两个节点在其他节点被观测时独立。</li>
</ul>
<blockquote>
<p>团和最大团：图中若干节点的集合，其中任意两个节点都互相连通。最大团就是节点最多再也无法添加时的团。</p>
</blockquote>
<p>那么就可以利用团的概念把<script type="math/tex">p(x)</script>因子分解成：<script type="math/tex">p(x) = \frac{1}{Z}\prod_{i=1}^{K} \varphi(x_{c_i})</script></p>
<p>其中<script type="math/tex">\varphi(\cdot)</script>称为势函数大于0，最终<script type="math/tex">p(x)</script>的分布的形式叫做Gibbs分布。这个分布的形式和指数族分布一致，也就满足了样本集合的最大熵原理。</p>
<h4 id="2-概率图推断"><a href="#2-概率图推断" class="headerlink" title="2.概率图推断"></a>2.概率图推断</h4><p>已知概率图、参数和观测样本，推测未知变量的分布。比如计算联合概率、条件概率或者是极大后验估计。</p>
<ul>
<li>精确推断<ul>
<li>变量消除Variable Elimination</li>
<li>信念传播Belief Propagation</li>
<li>Junction Tree</li>
</ul>
</li>
<li>近似推断<ul>
<li>Loop Belief</li>
<li>Monte Carlo采样：重要性采样、MCMC</li>
<li>确定性近似（变分推断）</li>
</ul>
</li>
</ul>
<h5 id="变量消除"><a href="#变量消除" class="headerlink" title="变量消除"></a>变量消除</h5><p>已知一个概率图，求某个变量d的边缘概率<script type="math/tex">p(d)</script>，也就是对其他所有变量做求和/积分：<script type="math/tex">\sum_{a,b,c}p(a,b,c,d)</script>。首先根据概率图对联合概率进行因子分解，分解之后逐步求积分。（从依赖的发起节点开始），比如<script type="math/tex">p(d) = \sum_{b, c} p(d|c)p(c|b)\cdot \sum_a p(a)p(b|a)</script>。其实也就是先对内部求和/求积分，再把求得的概率函数和外部的其他部分相乘。</p>
<p>缺点就是会有很多重复计算的部分，还有就是在图的计算中很难选择最优的消除次序，可能会导致消除的时间复杂度很高。</p>
<h5 id="信念传播"><a href="#信念传播" class="headerlink" title="信念传播"></a>信念传播</h5><p><img src="/images/image-20200908192437985.png" alt="无向无环图"></p>
<p>对于上面这种图（无向无环），定义<script type="math/tex">m_{c\rightarrow b}(b)</script>为<script type="math/tex">\sum_c\varphi(c)\varphi(b,c) \sim \sum_c p(c)p(b|c)</script>，这可以认为是把从C到B的路径上的依赖C约掉，只保留B的部分，那么可以这样求<script type="math/tex">p(a)</script>。</p>
<ul>
<li><p><script type="math/tex">p(a) = \sum_{\{b,c,d\}} p(a,b,c,d)</script>。</p>
</li>
<li><p>首先写出四条边四个点因子分解式：<script type="math/tex">p(a,b,c,d) = \frac{1}{z}\varphi_a(a)\cdot \varphi_b(b)\cdot \varphi_c(c)\cdot \varphi_d(d) \cdot \varphi_{ab}(a,b)\cdot \varphi_{bc}(b,c)\cdot \varphi_{bd}(b,d)</script>。</p>
</li>
<li><p>求出<script type="math/tex">m_{c\rightarrow b}(b),m_{d\rightarrow b}(b)</script>，即消去b上游的所有依赖节点。<br><script type="math/tex">p(a) = \sum_b \varphi(a) m_{b\rightarrow a}(a)</script>。<script type="math/tex">m_{b\rightarrow a}(a) = \sum_b \varphi(a,b)\varphi(b)\cdot m_{c\rightarrow b}(b)\cdot m_{d\rightarrow b}(b)</script>。</p>
</li>
<li><p>最后得到了无向图节点j到i的信息传播的公式，以及i节点的边缘概率：</p>
</li>
</ul>
<script type="math/tex; mode=display">
m_{j\rightarrow i}(a) = \sum_j \varphi(i,j)\varphi(i)\cdot \prod_{k\in N(j) - i}m_{k\rightarrow j}(j) \\\\
p(i) = \varphi(i)\cdot \prod_{k\in N(i)} m_{k\rightarrow i}(i)</script><p>所以只要有<script type="math/tex">m_{i\rightarrow j}</script>即可计算边缘概率，对n条边的无向图来说遍历图求得2n个<script type="math/tex">m</script>就可以计算任意的边缘概率了。其中的<script type="math/tex">\varphi(i)\prod_{k\in N(i)} m_{k\rightarrow i})(i)</script>叫做节点i的belief。 这种变量消除+Cache的方法就是BP信念传播。流程见下图，其中搜集信息即递归计算入信息，分发信息即递归计算出信息。</p>
<p><img src="/images/image-20200908192459775.png" alt="信念传播"></p>
<h5 id="Max-Product"><a href="#Max-Product" class="headerlink" title="Max Product"></a>Max Product</h5><p>Max Product可以看作是BP的一种改进。是Viterbi算法i的推广形式。</p>
<p>对于上面信念传播图中的图，首先我们明确一个概念，就是a、b、c、d都是随机变量，而信念传播是一种求它们边缘概率的方法。</p>
<p>那知道了这个图，假如说我希望求一个序列<script type="math/tex">(\hat a, \hat b, \hat c, \hat d)</script>使得它们的联合概率<script type="math/tex">p(a,b,c,d)</script>最大要怎么办呢？可以稍微修改BP方法中m函数的定义为如下形式：</p>
<script type="math/tex; mode=display">
m_{j\rightarrow i}(a) = \max_j \varphi(i,j)\varphi(i)\cdot \prod_{k\in N(j) - i}m_{k\rightarrow j}(j)</script><p>这样把sum改成max的意思就是，我消去变量j的方式从求和/积分，变成了求使得后面的部分最大化的j的值。</p>
<p>这实际上就是一种动态规划求最佳序列的方式，只是在图/树上求解，所以是Viterbi的一种推广。</p>
<h5 id="势函数"><a href="#势函数" class="headerlink" title="势函数"></a>势函数</h5><p>势函数<script type="math/tex">\varphi(\cdot)</script>是来自物理中的一个概念。我们在无向图中，利用极大团来因子分解联合概率的时候引入了它，<script type="math/tex">p(x) = \frac{1}{Z}\prod_{i=1}^{K} \varphi(x_{c_i})</script>。实际应用中需要对势函数进行设计，并满足配分函数<script type="math/tex">Z</script>的求和/积分等于1。</p>
<h5 id="道德图"><a href="#道德图" class="headerlink" title="道德图"></a>道德图</h5><p>将有向图转化为无向图的一种途径。需要特殊处理的只有head-2-head类型的结构，也就是对一个节点i，转化的时候需要将其所有的父亲节点两两互相连接。（有向树转化为无向图）</p>
<h5 id="因子图"><a href="#因子图" class="headerlink" title="因子图"></a>因子图</h5><p>可以视为对有向图更精细的分解，而且分解之后的节点需要满足<script type="math/tex">p(x) = f_s(x_s)</script>，其中<script type="math/tex">x_s</script>是因子节点。</p>
<h4 id="3-学习"><a href="#3-学习" class="headerlink" title="3.学习"></a>3.学习</h4><ul>
<li>参数学习</li>
<li>结构学习</li>
</ul>
<h3 id="五、EM算法"><a href="#五、EM算法" class="headerlink" title="五、EM算法"></a>五、EM算法</h3><p>有隐变量情况下的参数估计算法。</p>
<h4 id="1-收敛性"><a href="#1-收敛性" class="headerlink" title="1.收敛性"></a>1.收敛性</h4><p>首先从MLE参数估计的公式，最大化观测数据x的似然概率：<script type="math/tex">\theta^{t+1} = argmax_\theta \int_z log\ p(x, z|\theta)\cdot p(z|x;\theta^t)dz</script>。可以证明这样求得的参数一定满足随着$t$越来越大$log p(x|\theta^t)$越来越大。</p>
<h4 id="2-公式推导（KL-ELBO角度）"><a href="#2-公式推导（KL-ELBO角度）" class="headerlink" title="2.公式推导（KL + ELBO角度）"></a>2.公式推导（KL + ELBO角度）</h4><p>E步：根据当前隐变量z的后验分布，求出$E_{z|x} [log p(x,z|\theta)]$。</p>
<p>M步：根据当前的期望，用上一时刻的参数$\theta^t$从后验分布中采样z，优化参数$\theta$以最大化联合分布的期望<script type="math/tex">\theta^{t+1} = argmax_{\theta} E_{z|x,\theta^t} [\log p(x,z|\theta]</script>。</p>
<p>其中我们从KL + ELBO角度进行如下推导（有点像VAE的推导过程），首先对数似然变形，并引入一个隐变量的先验分布$q(z)$【因为$p(z|x)$不好求】：</p>
<p>$\log p(x|\theta) = \log p(x, z|\theta) - \log p(z|\theta) = \log \frac{p(x, z|\theta)}{q(z)} - \log \frac{p(z|\theta)}{q(z)}$。</p>
<p>然后对等式两边分别求$q(z)$的期望。左边等于不求。右边等于：</p>
<p>$\int_{z} q(z) \log \frac{p(x, z|\theta)}{q(z)}dz - \int_z q(z) \log \frac{p(z|\theta)}{q(z)}dz$。</p>
<p>也就是：$ELBO + KL(q(z)||p(z|x,\theta))$，第二项KL散度$\geq 0$，所以第一项也被称为ELBO证据下界。</p>
<p>那么，当KL散度等于0的时候，也就是$q(z) = p(z|x, \theta^t)$的时候，我们在M步做的事情是：</p>
<script type="math/tex; mode=display">argmax_\theta \int_z q(z) \log \frac{p(x, z|\theta)}{q(z)}dz = argmax_\theta \int_z p(z|x, \theta^t) (\log p(x,z|\theta) - \log p(z|x, \theta^t))dz</script><p>而由于此时的$\theta^t$是个常数，所以可以省略掉上面的式子中和$\theta$无关的部分，留下：$argmax_\theta \int_z p(z|x, \theta^t)\log p(x|\theta)dz$，也就是我们一开始写的那个与z的后验有关的联合分布期望。</p>
<h4 id="3-公式推导（Jensen不等式角度）"><a href="#3-公式推导（Jensen不等式角度）" class="headerlink" title="3.公式推导（Jensen不等式角度）"></a>3.公式推导（Jensen不等式角度）</h4><blockquote>
<p>Jensen不等式，concave函数的中点函数值$\geq$其两端连线直接在当前点的值。</p>
</blockquote>
<p>首先从log likelihood开始，$\log p(x|\theta) = \log E_{q(z)}[\frac{p(x,z|\theta)}{q(z)}] $。</p>
<p>利用jensen不等式得到它 $\geq  E_{q(z)}[\log\frac{p(x,z|\theta)}{q(z)}]$即ELBO。等式成立的情况下有$p(x,z|\theta) = c\cdot q(z)$，其中c是常数。这里经过推导，就可以得到$c == p(x|\theta)$，所以z的先验概率$q(z) = \frac{p(x,z|\theta)}{p(x|\theta)} = p(z|x, \theta)$，就等于后验概率。</p>
<p>EM实际上是为了生成模型估计参数，引入隐变量z来更新参数。</p>
<h4 id="4-广义EM"><a href="#4-广义EM" class="headerlink" title="4.广义EM"></a>4.广义EM</h4><p>log likelihood = ELBO + KL(q||p)。由此可以推广EM算法到其广义的形式。</p>
<p>E步，固定$\theta$求z的分布q使得$KL$最小或者$ELBO$最大；M步，固定$\hat q$求参数$\theta$去最大化ELBO也就是联合分布$E_q[\log p(x, z)] + H(q)$。</p>
<p>而原始的EM中分布q就等于后验$p(z|x)$。</p>
<h3 id="六、变分推断（VI）"><a href="#六、变分推断（VI）" class="headerlink" title="六、变分推断（VI）"></a>六、变分推断（VI）</h3><h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><p>频率角度-&gt;优化问题。（模型、策略、算法）</p>
<p>贝叶斯角度-&gt;积分问题。</p>
<ul>
<li>贝叶斯推断计算参数后验。（精确推断或者近似推断）</li>
<li>贝叶斯决策，使用参数后验求期望。</li>
</ul>
<h4 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h4><p>$x$: 观测数据，$z$：隐变量 + 参数，$(x, z)$完整数据。目标：找到一个变分分布$q(z)$，使得ELBO：$1： \mathcal{L}(z) = \int_z q(z) \frac{p(x,z)}{p(z)}dz$最大，也就是变分接近后验分布。</p>
<p>$q(z) = \prod_{i=1}^M q_i(z_i)$，假设z可以划分为M份互不相关的分量，那么可以因子分解为左边。然后固定其他所有分量求解分量$z_j$的话，1式可以变成：</p>
<script type="math/tex; mode=display">
\int_{z_j} q_j(z_j) \cdot E_{\prod_{i\neq j}q_i(z_i)}[ \log p(x,z) ]\ dz_j - (\int_{z_j} q_j(z_j) \log q_j(z_j)dz_j + C)</script><p>再进行变换就得到$-KL(q_j||P(x, z_j))$。</p>
<p>经典变分推断的目标就是找到一个变分分布，可以最小化KL，最大化ELBO。然后每次固定M-1组，交替求解其中的一组隐变量。（坐标上升的思想），迭代求解直到$\mathcal{L}$不再下降。</p>
<p>但是这样做：1）平均场假设太强，2）intractable。</p>
<h4 id="SGVI（Stochastic-Gradient）"><a href="#SGVI（Stochastic-Gradient）" class="headerlink" title="SGVI（Stochastic Gradient）"></a>SGVI（Stochastic Gradient）</h4><p>随机梯度变分推断利用梯度下降来计算变分分布的参数。</p>
<p>经过推导，ELBO对变分分布参数$\phi$的导数是：</p>
<script type="math/tex; mode=display">
\nabla_\phi \mathcal{L}(\phi) = \nabla_\phi E_{q_\phi} [\log P_\theta(x^{i}, z) - \log q_\phi] = E_{q_\phi} [\nabla_\phi \log q_\phi\cdot (\log P_{\theta}(x^{i}, z) - \log q_\phi)]</script><p>能写成期望的形式意味着可以通过在q分布采样的方式计算下一轮的导数，也就是采样L个隐变量$z^{(l)}$，梯度约等于：</p>
<script type="math/tex; mode=display">
\frac{1}{L}\sum_{l=1}^L \nabla_\phi \log q_\phi(z^{(l)})\cdot (\log P_{\theta}(x^{i}, z^{(l)}) - \log q_\phi(z^{(l)}))</script><p>但是这样采样的方差很大，因为q很大和很小的时候对log q求期望差别很大。那如何降低方差呢？引入了重参数化技巧（即VAE中的那个，不仅仅可以处理不可微问题，而且可以减少方差）。找一个$\epsilon\sim p(\epsilon)$，这个p分布要尽量简单一点，然后保证得到的$\epsilon$可以用来采样$z$，也就是z可以通过现有后验分布的参数$\phi$和这个$\epsilon$进行简单的变换$g_\phi(\epsilon, x)$得到。然后进行一系列推导，最后得到的结论是这样采样$\epsilon$并计算$z$最终算出来的梯度就减少了方差。</p>
<p>举例：比如VAE里面$z$服从高斯分布，那么分布参数$\phi$就是均值$\mu$和方差$\sigma$，这时候这个$\epsilon$从标准高斯分布中采样，$g_\phi = \mu + \sigma * \epsilon$就可以得到服从$z$分布的隐变量。</p>
<p>得到了梯度之后，就可以利用梯度上升/下降对q分布的参数$\phi$进行参数优化了。</p>
<h3 id="七、MCMC"><a href="#七、MCMC" class="headerlink" title="七、MCMC"></a>七、MCMC</h3><h4 id="采样方法介绍"><a href="#采样方法介绍" class="headerlink" title="采样方法介绍"></a>采样方法介绍</h4><p>蒙特卡洛方法是一种基于采样的随机近似方法，可以用来进行近似推断。</p>
<p>也就是把积分形式$\int p(z|x)f(z)dz$近似为$\frac{1}{N}\sum_{i=1}^N f(z_i)$。但是假如说后验分布比较复杂，怎么才能采样得到$z_i$呢？</p>
<ul>
<li><p>1）概率分布采样：计算机能够从均匀分布$U(0, 1)$中采样，如果能够根据z的分布的概率密度函数能够计算累计分布函数cdf，则可以根据cdf的逆函数将$o\sim U(0, 1)$作为逆函数输入进行采样，但是这也仅限于简单的分布。</p>
</li>
<li><p>2）拒绝采样，引入一个提议分布$q(z)$和正整数$M$，保证处处都有$Mq(z)\geq p(z|x)$。那么对$z_i$可以计算出接受率$\alpha$，每次从$q(z)$中采样出一个点，从$U(0, 1)$中采样出一个$u$，如果$u \leq \alpha$那么接受当前采样，否则拒绝。</p>
</li>
<li><p>3）重要性采样，从$q(z)$中采样，每个样本的权重等于$p$与$q$的比值，对两个分布的形式有要求否则采样效率低：</p>
</li>
</ul>
<script type="math/tex; mode=display">
\int p(z|x)f(z)dz = \int \frac{p(z|x)}{q(z)}f(z)q(z)dz \simeq \frac{1}{N}\sum_{i=1}^{N} f(z)\frac{p(z|x)}{q(z)}</script><p>这些采样方法要求proposal distribution <script type="math/tex">q</script>和原始分布<script type="math/tex">p</script>接近，且简单易于采样，这有时很困难。</p>
<h4 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h4><p>马尔科夫链：时间和状态都是离散的。<script type="math/tex">\{...,x_t,...\}, x_t\in \{1, 2, \cdots, N\}</script>，<script type="math/tex">Q\in R^{N\times N}</script>为状态转移矩阵。</p>
<p>齐次（一阶）马氏链：<script type="math/tex">q(x_t|x_{<t}) = q(x_t|x_{t-1})</script>。</p>
<p>平稳分布：如有<script type="math/tex">\pi(x^*) = \int \pi (x)\cdot Q(x\rightarrow x^*)dx</script>或者写成<script type="math/tex">\pi (x^t) = \sum_{i=1}^N \pi(x^{t-1}=i)\cdot Q(x^{t-1}\rightarrow x^t)</script>，则称<script type="math/tex">\{\pi (k)\}</script>是<script type="math/tex">\{x_t\}</script>的平稳分布，也就是最终的状态分布稳定了，状态$k$的概率值为<script type="math/tex">\pi(k)</script>。（对离散型随机变量）</p>
<p>细致平衡：<script type="math/tex">q (x)\cdot Q(x\rightarrow x^*) = q (x^*)\cdot Q(x^*\rightarrow x)</script>，detailed balance是产生平稳分布的充分条件，其中$q (x)$表示取值为$x$的概率，$q$表示一个平稳分布。</p>
<p>MCMC采样：</p>
<ul>
<li>从<script type="math/tex">\{x_1, \cdots, x_{m-1}, x_m, \cdots, x_{m + n}, \cdots\}</script>这组序列中，选择n个采样样本<script type="math/tex">\{x_m, \cdots, x_{m + n}, \cdots\}</script>，其中到第m步马氏链形成了平稳分布。</li>
<li>平稳分布接近或者等于目标分布，那么在平稳分布上采样就等价于在目标分布上采样了。</li>
<li>关于能够到达平稳分布的证明：<ul>
<li>首先能够得到<script type="math/tex">q^{t+1} = q^t\cdot Q</script>，也就是<script type="math/tex">q^{t+1} = q^1\cdot Q^t</script>。</li>
<li>状态转移矩阵<script type="math/tex">Q</script>是一个随机矩阵，方阵 &amp; 每一行之和等于1。它有一个性质即特征值<script type="math/tex">\leq 1</script>，所以<script type="math/tex">Q^T = (ALA^{-1})^T = AL^TA^{-1}</script>。（A是一个矩阵，其每一列以此是A的特征向量的分量。L是一个矩阵，其对角线依次是A的特征值；其他元素为0。）</li>
<li>因此存在足够大的T，使得<script type="math/tex">L^T</script>只有若干个元素为1（即Q本身为1的那些特征值）。此时<script type="math/tex">L^{T} = L^{T+1}</script>，且<script type="math/tex">q^{t+1} = q^1\cdot AL^TA^{-1}</script>，而<script type="math/tex">q^{t+2} = q1\cdot AL^{T+1}A^{-1}</script>，所以q形成了平稳分布。</li>
</ul>
</li>
</ul>
<p>Metropolis-Hastings：</p>
<ul>
<li>均匀分布中采样$u\sim U(0, 1)$，Q为proposal matrix可以由任意先验给出，<script type="math/tex">z^*\sim Q(z|z^{(i-1)})</script>，接受率<script type="math/tex">\alpha = min(1, \frac{p(z^*)\cdot Q(z|z^*)}{p(z)\cdot Q(z^*|z)})</script>。</li>
<li>如果<script type="math/tex">u < \alpha</script>，那么<script type="math/tex">z^{(i)} = z^*</script>，否则<script type="math/tex">z^{(i)} = z^{(i-1)}</script>。</li>
<li>这里的原理就是<script type="math/tex">p(z)Q(z^*|z)\cdot \alpha(z, z^*) = p(z^*)Q(z|z^*)\cdot \alpha(z^*, z)</script>保证了最终得到的是平稳分布。</li>
<li>在x是连续变量的情况下，转换矩阵Q变为核函数积分形式。</li>
<li>问题：1）高维变量<script type="math/tex">\alpha</script>计算很慢，而且转移会被拒绝效率较低；2）在多维变量采样时可能面临联合分布难以计算的问题。</li>
</ul>
<p>Gibbs采样：在采样第i维的时候，固定住其他维度不变。它需要知道每个维度在其他维度下的条件概率。</p>
<ul>
<li>比如对一个m维的变量，<script type="math/tex">p(z) = p(z_1, \cdots, z_m)</script>，<script type="math/tex">z^{t+1}_i\sim p(z_i|z_{\neq i}^t)</script>。</li>
<li>MH采样中的<script type="math/tex">P(z^*)</script>在某一时刻对第i维度采样就等于<script type="math/tex">p(z_i^*|z_{\neq i}^*)\cdot p(z^*_{\neq i})</script>，而<script type="math/tex">Q(z|z^*)</script>就等于<script type="math/tex">p(z_i|z^*_{\neq i})</script>。</li>
<li>而且，因为某一个时刻采样i维的时候，<script type="math/tex">z_{\neq i}</script>是固定不变的，所以<script type="math/tex">z_{\neq i} == z_{\neq i}^*</script>。</li>
<li>这样算出的接受率<script type="math/tex">\alpha = 1</script>，也就是每个时刻都选择从<script type="math/tex">p(z_i|z^t{\neq i})</script>中采样的值。</li>
</ul>
<p>Tricks：</p>
<ul>
<li>通常多维变量采样Gibbs采样。</li>
<li>MH或者Gibbs采样的相邻样本之间是相关的，如果要求独立性，需要重新在生成的平稳序列下再次随机采样。</li>
<li>关于“燃烧期”，即抛弃的不稳定的MC序列的长度，通常是经验性的判定，或者可以使用序列窗口的均值来判断是否平稳。</li>
</ul>
<p>MCMC的问题：</p>
<ul>
<li>不能保证多少步收敛；</li>
<li>如果p分布过于复杂或者维数很大，燃烧期可能很长（比如分布很陡峭、或者具有某些概率极低的低谷）</li>
<li>样本之间有关联性，不符合采样的独立性原则。</li>
</ul>
<h4 id="HMC"><a href="#HMC" class="headerlink" title="HMC"></a>HMC</h4><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.bilibili.com/video/BV1aE411o7qd" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1aE411o7qd</a></p>
<p><a href="https://www.yuque.com/books/share/f4031f65-70c1-4909-ba01-c47c31398466" target="_blank" rel="noopener">https://www.yuque.com/books/share/f4031f65-70c1-4909-ba01-c47c31398466</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读笔记:Collaborative Filtering for Implicit Feedback Datasets</title>
    <url>/2015/07/27/machine-learning/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-Collaborative-Filtering-for-Implicit-Feedback-Datasets/</url>
    <content><![CDATA[<h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h3><ul>
<li><p>推荐系统分类——基于策略</p>
<ul>
<li>基于行为的策略：信息冗余且难以收集</li>
<li>协同过滤：冷启动问题</li>
</ul>
</li>
<li><p>推荐系统分类——基于输入类型</p>
<ul>
<li>显示反馈</li>
<li>隐式反馈</li>
</ul>
</li>
<li><p>隐式反馈的特点</p>
<ul>
<li>没有负反馈，也就是说即使用户没有行为也并不意味着用户不喜欢</li>
<li>隐式反馈本质上是含有很多噪音的，并不能真正反映出用户的偏好</li>
<li>显示反馈的数值反映了用户的偏好；而隐式反馈的数值反映了置信度（也就是用户有多大可能性对该物品感兴趣）</li>
<li>隐式反馈的推荐需要合适的方法来进行评估</li>
</ul>
</li>
</ul>
<h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2.准备工作"></a>2.准备工作</h3><ul>
<li>准备了用户数据矩阵$R$，这里的$R<em>{ui}$不是偏好值，而是对用户行为的观察。论文中$R</em>{ui}$是用户$u$全程观看电影$i$的次数。如果用户$u$观看了电影$i$的70%，那么$R_{ui}$就被设置为0.7</li>
</ul>
<h3 id="3-以前的研究"><a href="#3-以前的研究" class="headerlink" title="3.以前的研究"></a>3.以前的研究</h3><ul>
<li>近邻模型</li>
<li>潜在因子模型（即PPT中的显式反馈）</li>
</ul>
<h3 id="4-我们的模型"><a href="#4-我们的模型" class="headerlink" title="4.我们的模型"></a>4.我们的模型</h3><script type="math/tex; mode=display">p_{ui} = \left\{ \begin{array}
\overline 1 & r_{ui} > 0 \\ 0 & r_{ui} = 0 \\
\end{array}\right.</script><ul>
<li>使用$置信度C<em>{ui}来为偏好度$P</em>{ui}$加权$<script type="math/tex; mode=display">C_{ui}=1+αR_{ui}</script></li>
<li>$λ(\sum_u|x_u|^2 + \sum_i|y_i|^2)$防止过拟合</li>
<li>交替最小二乘过程，前文已经叙述</li>
<li>每次迭代时间开销$O(f^2N+f^3M)$，其中$N$是非0观察值的数量，$M$是用户数量，$f$是特征数量</li>
<li>特点<ul>
<li>转换直接观察值($r<em>{ui}$)到两个解释数值：偏好度$p</em>{ui}$和置信度$r_{ui}$</li>
<li>输入规模线性倍数的时间开销</li>
</ul>
</li>
</ul>
<h3 id="5-推荐解释"><a href="#5-推荐解释" class="headerlink" title="5.推荐解释"></a>5.推荐解释</h3><ul>
<li>好的推荐需要有理论解释 —— <strong>文献10</strong>[Well Accepted]</li>
<li>通过对于${P_{ui}}^\prime=X_u^T\times Y_i$的推导，证明推荐的数学合理性<ul>
<li>$W^u$被看做是用户$u$的权重矩阵</li>
<li>物品$i$和物品$j$在用户$u$眼中的加权相似度为$S_{ij}^u=Y_i^TW^uY_j$<script type="math/tex; mode=display">
\begin{align*}
{P_{ui}}^\prime & =Y_i^T\times X_u\\
& =Y_i^T\times (Y^TC^uY + λI)^{-1}\times Y^TC^up(u)\\
& =Y_i^T\times W^u\times Y^TC^up(u)\\
& =\sum_{j:r_{ui}>0}S_{ij}^uC_{uj}\\
\end{align*}</script></li>
</ul>
</li>
</ul>
<h3 id="6-实验"><a href="#6-实验" class="headerlink" title="6.实验"></a>6.实验</h3><ul>
<li>实验数据处理<ul>
<li>预处理</li>
<li>评估方法</li>
<li>对比策略<ul>
<li>(1) 按照节目流行度进行推荐排序</li>
<li>(2) 基于物品的邻域模型$ P<em>{ui} = \sum_j \frac{ r_i^T r_j } { ||r_i|| ||r_j|| } r</em>{uj} $</li>
</ul>
</li>
<li>评估结果<ul>
<li>使用$r<em>{ui}$效果不佳，改为使用$p</em>{ui}$</li>
<li>Dense SVD算法用于协同过滤<strong>参考文献18</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="7-讨论"><a href="#7-讨论" class="headerlink" title="7.讨论"></a>7.讨论</h3><p>这篇论文主要做了哪些工作，创新点在哪里</p>
<ul>
<li>(1) <code>隐式</code>用户观察值分解为偏好值和置信度（对于显示用户行为无效）</li>
<li>(2) 使用了潜在因子（特征向量）算法，将所有User-Item矩阵中的数值作为输入（即使没有观察值）</li>
<li>(3) 优化算法，使迭代的时间开销只随输入数量线性增长</li>
<li>(4) 使用代数演算转换为基于物品的邻域公式，从而进行推荐解释</li>
</ul>
<h3 id="8-未来的工作"><a href="#8-未来的工作" class="headerlink" title="8.未来的工作"></a>8.未来的工作</h3><ul>
<li>(1) 使用一个随时间变化的变量来确定用户在特定时间观看特定节目的可能</li>
<li>(2) 特定节目在不同时间段的吸引度</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>阅读笔记</tag>
        <tag>推荐算法</tag>
        <tag>协同过滤</tag>
        <tag>Spark</tag>
        <tag>ALS</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式文件系统GFS</title>
    <url>/2015/11/21/machine-learning/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FGFS/</url>
    <content><![CDATA[<h3 id="一、文件系统"><a href="#一、文件系统" class="headerlink" title="一、文件系统"></a><strong>一、文件系统</strong></h3><p>文件系统最初用来解决信息的长期存储，也就是把信息以一个单元——<code>文件</code>的形式存储在磁盘上。1965年开发的<code>Multics</code>（Unix的前身）就已经详细设计了文件系统。</p>
<p><strong>1.1 问题：</strong></p>
<ul>
<li>1.文件是什么</li>
</ul>
<p>文件是命名的，存储在设备中的信息的字节流。</p>
<ul>
<li>1.文件由什么组成</li>
</ul>
<p>文件名，数据，以及其他必要信息即文件的<code>元数据</code>（诸如文件创建日期、文件长度等信息)。</p>
<ul>
<li>2.文件如何命名</li>
</ul>
<p>一般使用一定长度的字符串来命名。</p>
<ul>
<li>3.如何保护文件</li>
</ul>
<p>由权限控制来保护。</p>
<ul>
<li>4.可以对文件进行哪些操作</li>
</ul>
<p>应该具有创建、修改、删除、读取、移动、设置属性等操作。</p>
<ul>
<li>5.文件的组织</li>
</ul>
<p>一般使用<code>目录</code>和<code>目录树</code>的概念。目录作为存储一组文件和目录的虚拟容器。利用<code>分隔符</code>加<code>目录名</code>来定位文件所在的<code>路径</code>。使用目录的文件系统会形成一个目录树，从根目录开始。文件由文件所在路径及文件名定位。</p>
<p><strong>1.2 文件系统：</strong></p>
<ul>
<li>1.如何组织文件的物理存储结构</li>
</ul>
<p>文件的物理存储结构是文件在设备上的存取方式，而不同的存储设备会以不同方式把存储空间以单元划分。以磁盘为例，它的基础存取单元就是<code>扇区</code>——一个扇区一般是<code>4096Byte</code>。而文件的数据通常也会以一定数量来进行划分——一个文件的数据划分为一个或多个逻辑存储单元。</p>
<p>物理存储单元和逻辑存储单元的关联关系一般有下面三种方式：</p>
<p>（1）<code>连续分配方式</code>：简单，直接把文件的N个逻辑块映射到连续的N个物理块上，但浪费存储空间、且可能出现连续存储空间不够导致大文件无法存储。</p>
<p>（2）<code>链接表</code>：每个逻辑块的携带文件下一个物理块的位置，这样劣势在于随机读取效率太低，必须顺序读取。</p>
<p>（3）<code>索引链式表</code>：取出每个存储设备单元的指针字段，把它们存储到一张索引表中，文件的第一块地址存储在目录项中。系统加载后该表存储在内存中，<code>MS-DOS</code>使用这种文件物理结构。</p>
<p><img src="/images/Ft4mZNqvLSfi-KlUHQb9Q00E2Fmu.jpg" alt=""></p>
<ul>
<li>2.如何管理目录</li>
</ul>
<p>目录要存储的数据主要是其子目录和文件的记录，在存储设备中把目录当做一种特殊的结构化文件保存，目录的数据项就是指向其子文件和子目录的元数据的指针。</p>
<ul>
<li>3.如何管理物理存储空间</li>
</ul>
<p>当进行文件系统中操作时，如何判断文件系统所在设备的空闲空间数量。存储空间管理的主要方法有：空闲文件目录、空闲块链、位示图等。</p>
<ul>
<li>4.如何保护文件</li>
</ul>
<p>文件系统的保护，涉及到共享、保密和保护这三个问题。分别对应着对文件读、写、执行的权限。通常使用<code>存取控制表</code>和<code>权限表</code>来进行访问控制。</p>
<p>存取控制表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>文件</th>
<th>访问控制表</th>
</tr>
</thead>
<tbody>
<tr>
<td>/bin/file1</td>
<td>(root,RWX),(cairo,RX),(Tom,x)</td>
</tr>
<tr>
<td>/bin/file2</td>
<td>(Bin,RW),(cairo,RWX)</td>
</tr>
</tbody>
</table>
</div>
<p>用户cairo的权限表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>权限</th>
<th>文件</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>RX</td>
<td>指向/bin/file1的指针</td>
</tr>
<tr>
<td>1</td>
<td>RWX</td>
<td>指向/bin/file2的指针</td>
</tr>
</tbody>
</table>
</div>
<h3 id="二、The-Google-File-System"><a href="#二、The-Google-File-System" class="headerlink" title="二、The Google File System"></a><strong>二、The Google File System</strong></h3><p><strong>2.1. 假设和适用环境</strong></p>
<ul>
<li>1.硬件失效是常态</li>
<li>2.文件大小比较大，文件数量多</li>
<li>3.文件操作主要是尾部追加写而不是覆盖写或者随机写入，主要是批量读写而不是随机读写，一次写入多次读取是常见访问方式</li>
<li>4.需要支持并发写操作，对于吞吐率要求高于对于延迟要求</li>
</ul>
<p><strong>2.2. 架构</strong></p>
<p>一个GFS集群包含：一个逻辑上的<code>Master</code>、多个<code>chunkServer</code>，Master存储文件的<code>元数据</code>并将文件分为chunk（<strong>64MB</strong>）存储在多个chunkServer上（每个块在不同的chunkServer存储多份提供可靠性保证），存储在chunkServer上的文件基于<code>Linux</code>的文件系统，使用64位的<code>chunk标识</code>来区别，无需额外的缓存设计。GFS的架构如下图：</p>
<p><img src="/images/FkstRZ0uKGdBarUI5wf71Pwgt9GR.jpg" alt=""></p>
<p><strong>2.3. 用户接口</strong></p>
<ul>
<li>创建</li>
<li>删除</li>
<li>打开</li>
<li>关闭</li>
<li>修改元数据</li>
<li>写入数据（随机）</li>
<li>读取</li>
<li><strong>快照</strong></li>
<li><strong>追加写入</strong></li>
</ul>
<p><strong>2.4. 设计点</strong></p>
<ul>
<li>1.元数据操作相关的设计——读写锁、检查点和日志</li>
</ul>
<p><code>读写锁</code>用来确保在Master中进行的元数据操作串行化。在进行需要原子性的元数据修改操作时申请写锁来确保操作的原子性。</p>
<p><code>操作日志</code>是元数据变化的历史纪录，起到定义<code>元数据</code>操作顺序的逻辑时间线，以避免Master失效导致元数据修改丢失的作用。</p>
<p><code>检查点</code>机制定期把操作日志的影响写入<code>检查点文件</code>（也就是写入了当前的元数据），可以避免日志过大影响Master重新启动花费的时间。</p>
<p>注意，元数据的持久化只针对<code>名字空间</code>和<code>文件与chunk</code>的映射表，而<code>chunk与所在位置</code>的映射表由Master向chunk server获取并更新。</p>
<ul>
<li>2.数据一致性相关的设计——租约（Lease）、版本号（Version）和校验码（Checksum）</li>
</ul>
<p><code>租约</code>是每个chunk元数据中的内容，用来保证并发写入时多个chunk server写入块中数据具有最终一致性（即写入完成后数据是一致的），由Master授予一个chunk server租约，并规定不同客户端写入同一个chunk文件的顺序并发送至其他chunk server。</p>
<p><code>版本号</code>是每个chunk元数据中的内容，用来防止chunk server失效等原因造成数据不一致，每次Master授予租约时提高版本号，并把新的版本号应用在所有未失效的块节点上。这样失效的chunk server回到集群时，相应的失效块就能够被检测出来具有较低的版本号判定为失效块。客户端由于请求Master时获得了新版本号也不会获得错误的旧数据。</p>
<p><code>校验码</code>：每个chunk被分为64KB的小块，每个小块具有一个校验码。它用来防止磁盘错误等原因造成数据不一致，校验码在写入时一并发送到每个块所在的chunk server，并在chunk server空闲时或者读取块数据时进行扫描比对数据和校验码，若比对不一致则会报告给Master标识此chunk失效。</p>
<p><img src="/images/FhitOgZnLScOLPwzA7rhlAO-qG-3.jpg" alt=""></p>
<ul>
<li>3.失效处理相关的设计——热备份</li>
</ul>
<p>Master在进行任何修改元数据的操作时，都会存储操作日志到<code>shadow Master(s)</code>中，存储成功后再向客户端返回成功消息，而检查点文件也会在shadow master机器上定时创建。因此当Master失效时，外部运行的监控系统（比如chubby）会选举shadow master(s)中的一台机器成为新的Master节点。</p>
<p><img src="/images/Fl--CX5Us-kHsGbNXn-Z8SA4NtSY.jpg" alt=""></p>
<ul>
<li>4.懒惰式（周期化）处理的设计——快照、文件删除、再负载均衡</li>
</ul>
<p>对于一些操作，GFS采用了懒惰式设计，把开销较大的操作部分分摊到系统运行的一个较长时间中，减少系统性能的颠簸。</p>
<p><code>快照</code>，用来保存文件或目录的当前状态，GFS使用<code>COW</code>(copy-on-write)技术来实现。在建立快照时仅复制一份原有文件或目录的元数据指向同样的chunk，然后在chunk被修改的时候才真正的复制chunk数据并把快照元数据指向复制后的chunk。</p>
<p><code>文件删除</code>，删除时仅仅对元数据进行重命名，在删除操作一段时间后删除元数据，并在周期性的Master-chunkServer交互中告知chunk server该chunk已被删除，真正删除chunk文件。</p>
<p><code>再负载均衡</code>，移动部分chunk使集群中chunk server的负载较为平均，不会存在性能瓶颈。GFS的Master在空闲时周期性的扫描chunk-位置映射表，发现负载热点或者负载较低节点（比如新加入chunk server）时 <strong>逐渐地</strong>进行负载均衡操作。</p>
<p><strong>2.5. 其他一些问题</strong></p>
<ul>
<li>并发操作支持吗？</li>
</ul>
<p>对于并发的读操作是支持的。并发写操作要分两种情况，并发的修改数据和并发的修改元数据。GFS支持多个Writer并发写入一个文件，但是对于元数据的修改必须串行化，这是使用Master中目录树节点的<code>读写锁</code>（共享-互斥锁）来实现的。</p>
<ul>
<li>如何保证并发写的副本一致性？</li>
</ul>
<p>对于写操作，使用Master授予租约给chunk所在的某一个服务器。若多个client同时写入一个chunk，那么该服务器首先确定写入顺序并保存在日志中，然后以改顺序修改自己保存的chunk数据，并把日志分发到另外几个<code>二级副本</code>（块所在的其他几个chunk server）中，让他们以相同顺序执行。</p>
<ul>
<li>若副本执行失败，一致性如何保证？</li>
</ul>
<p>如果主副本或其他任意副本执行修改操作失败，返回失败信息或超时并由客户端重试；由于写入操作规定了写入的偏移量，所以重试成功后会覆盖写入失败的数据，各副本一致性能够保证。</p>
<ul>
<li>原子性的追加写入？</li>
</ul>
<p>追加写入由于client不规定写入偏移量，写入前主副本序列化写入顺序时，会选择当前文件的末尾，所以在并发追加写时不会出现各客户端写入数据混杂的情况，所以追加写入相对于随机写入具有原子性，写入的片段一定是完整的。而随机写入即使操作本质上是串行的也可能写入数据不完整。</p>
<ul>
<li>如何保证append at least once？</li>
</ul>
<p>追加写入失败后自动重试，直到在所有chunk server上操作成功。但是由于追加写入的偏移量重新进行了选择，所以chunk server上会出现多段追加写入的数据，或者数据片段。但是每个chunk server在最后成功返回client的偏移量之后一定都有一段完整的数据片段。</p>
<ul>
<li>并发追加写的结果？</li>
</ul>
<p>GFS保证了每个client的写操作都会追加成功至少一次，而且不同client的数据不会交叉追加在文件尾部，但并不保证追加顺序，也可能重复追加。</p>
<ul>
<li>客户端缓存元数据的时间和策略？如果缓存的这段时间文件副本所在的chunkserver失效怎么办？如果读取时文件被改变怎么办？</li>
</ul>
<p>client缓存元数据直到元数据过期或者重新打开文件。如果chunkserver失效就重新请求Master获取元数据。读取时Master会对文件元数据加读锁，文件元数据不会改变，也就是文件（及所在namespace）不会被删除、重命名（移动）或者快照，但是不保证不会被修改。</p>
<ul>
<li>Master如何确定新建chunk及其副本的物理位置？</li>
</ul>
<p>根据Master内存中存储的chunk和chunk sever的映射，以<code>低磁盘利用率优先</code>、<code>最近无大量创建chunk优先</code>、<code>副本不能全部分配在同一个机架</code>这三个策略来确定chunk副本的物理服务器存储位置。</p>
<h3 id="三、HDFS"><a href="#三、HDFS" class="headerlink" title="三、HDFS"></a><strong>三、HDFS</strong></h3><p><img src="/images/FgNxBZ9EgDRmTPvtd8ufU_pIaQ9O.jpg" alt=""></p>
<p>主要对比HDFS和GFS的差异，仅对于论文中讨论的HDFS版本有效（2010年之前的某个版本）：</p>
<p><strong>3.1 HDFS具有更加简单的一致性模型</strong></p>
<p>由于HDFS不支持并发写操作和并发追加写操作，所以不需要使用租约（lease）机制来保证副本一致性。不过即使是但客户端的追加写操作，也可能由于chunk server失效产生重复记录或者记录片段，需要客户端进行额外处理。</p>
<p><strong>3.2 HDFS相对于GFS减少的功能</strong></p>
<ul>
<li>HDFS不具有具体文件快照功能，只支持整个系统存在一个快照（Hadoop2.6.0已经开发）</li>
<li>HDFS不支持覆盖写，写入数据无法改变</li>
<li>HDFS没有采用惰性删除文件</li>
<li>HDFS不支持Master的热备份，单点故障（Secondary Namenode）后需要手动介入</li>
</ul>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><p>《Hadoop技术内幕》</p>
</li>
<li><p><a href="https://eportal.stust.edu.tw/eshare/EshareFile/2013_6/2013_6_07e25b90.pdf" target="_blank" rel="noopener">The Hadoop Distributed File System - Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler, Sunnyvale, California USA, {authors}@Yahoo-Inc.com, IEEE2010
</a></p>
</li>
<li><p><a href="http://dl.acm.org/citation.cfm?id=1165389.945450" target="_blank" rel="noopener">Ghemawat S, Gobioff H, Leung S T. The Google file system[J]. Acm Sigops Operating Systems Review, 2003, 37:29-43.</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
      </tags>
  </entry>
  <entry>
    <title>自编码器和去噪自编码器</title>
    <url>/2015/11/03/machine-learning/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E5%8E%BB%E5%99%AA%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</url>
    <content><![CDATA[<h3 id="一、自编码器"><a href="#一、自编码器" class="headerlink" title="一、自编码器"></a>一、自编码器</h3><p>自编码器是神经网络的一种。它被用来做这样一件事情，那就是让输出尽可能的去模拟输入，从而找到输入的压缩表示。</p>
<p>自编码器适用于无监督学习。一个自编码器（<code>Autoencoder</code>）具有输入层、隐藏层和输出层，输入层接受形如$x \in [0,1]^d$的输入,隐藏层对于输入层做编码（Encode）操作，而输出层对于隐藏层做解码（Decode）操作。具体映射如下。</p>
<script type="math/tex; mode=display">y = s(Wx + b)</script><script type="math/tex; mode=display">z = s(W'y + b')</script><p>其中，y是隐藏层输出，被称为隐藏表示（<code>latent representation</code>）；<br><br>z是最终的输出，被称为x的重建（<code>reconstruction</code>）；<br><br>s是非线性映射函数如<code>sigmoid</code>；<br><br>一般权重矩阵$W’$被设置为$W’ = W^T$，以减少优化时间，这种方式叫做<code>权重绑定</code>。<br><br>优化参数$W,(W’),b,b’$来减少自编码器的重建误差，也就是减少z和x之间的不同，损失函数可以选择均方误差或者是交叉熵函数等(<code>如下</code>)。</p>
<script type="math/tex; mode=display">L_H(x,z) = -\sum_{k=1}^d[x_k\log z_k + (1 - x_k)\log (1-z_k)]</script><p>那么自编码器的<strong>作用</strong>是什么呢？</p>
<p>它的主要作用就是学习有效的输入表示（<code>representation learning</code>），或者说学习输入的压缩表示。有时数据的输入空间维度太大需要降低维度进行处理，人工进行特征选择代价高效果又不见得好，比如音乐数据的特征提取；或者数据没有标签无法使用监督学习的方式进行特征提取。那么就可以使用自编码器来做这件事。</p>
<p><img src="{filename}/imgs/DL/AE/autoencoder.png" alt="自编码器神经网络"></p>
<p>如上图所示是自编码器的一种结构。令其隐藏层神经元的数量小于输入层和输出层，这时对参数进行优化，如果重建输出$z$与输入$x$很接近，那么就可以认为潜在表示$y$捕捉到了$x$的有效特征，是$x$的有效压缩表示，就可以达到数据降维和特征提取的目的。</p>
<h3 id="二、稀疏自动编码器"><a href="#二、稀疏自动编码器" class="headerlink" title="二、稀疏自动编码器"></a>二、稀疏自动编码器</h3><p>即使自编码器的隐藏层神经元数量较大，也可以利用添加<code>稀疏性限制</code>的方式来达到相同的目的。这里的稀疏性可以认为是让大部分的隐藏层神经元输出接近于0（sigmoid作为激活函数）或者-1（tanh作为激活函数）。</p>
<p>采用在损失函数中增加惩罚项的方式来保证稀疏性。</p>
<script type="math/tex; mode=display">J_{sparse}(W,b) = J(W,b) + \beta \sum_{j=1}^{s2} KL(\rho||\hat\rho_j)</script><p>其中，KL距离（相对熵函数）为：</p>
<script type="math/tex; mode=display">KL(\rho||\hat \rho_j) = \rho \log\frac {\rho}{\hat\rho_j} + (1-\rho) \log\frac {1-\rho} {1 - \hat\rho_j}</script><p>$\beta$控制惩罚项的权重；$\rho$代表稀疏性参数，也就是神经元的目标平均活跃度；$s2$表示隐藏层神经元的数量；索引$j$代表每一个神经元；$\hat\rho_j$代表神经元$j$的平均活跃度。</p>
<p><code>文献[4]</code>的实验表明，在使用随机梯度下降方法进行实验时，隐藏单元神经元数量超过可见单元的自编码器（过完备的自编码器）能够产生更有用的表示，这里“<strong>有用的表示</strong>”代表产生的表示在同一个神经网络中有更好的分类效果。</p>
<h3 id="三、去噪自编码器"><a href="#三、去噪自编码器" class="headerlink" title="三、去噪自编码器"></a>三、去噪自编码器</h3><p>由于自编码器的潜在表示$y$是对于输入$x$的一种有损压缩。优化和训练只能让它对于训练集合来说是很好的压缩表示，但并不是对于所有的输入都是这样。为了增加隐藏层的特征表示的鲁棒性和泛化能力，引入去噪自编码器。</p>
<p>去噪自编码器在自编码器的基础上，在输入中加入随机噪声再传递给自编码器，通过自编码器来重建出无噪声的输入。</p>
<p>加入随机噪声的方式有很多种。在<code>参考文献[3]</code>中，该过程随机的把输入的一些位（最多一半位）设置为0，这样去噪自编码器就需要通过没有被污染的位来猜测被置为零的位。能够从数据的抽样部分预测整体数据的任何子集是在该抽样中能够找到变量联合分布的充分条件(<a href="https://en.wikipedia.org/wiki/Gibbs_sampling" target="_blank" rel="noopener">Gibbs抽样</a>的理论依据)，这说明去噪自编码器能够从理论上证明潜在表示能够获取到输入的所有有效特征。</p>
<p>去噪自编码器在2008年发表在ICML上的<code>文献[3]</code>中被提出，文献实验证明了去噪自编码器进行预训练的<code>DNN</code>能够得到更好的分类效果，并从流形学习、随机算子、信息论等角度对去噪自编码器的良好效果进行分析。</p>
<h3 id="四、基于Theano的实现"><a href="#四、基于Theano的实现" class="headerlink" title="四、基于Theano的实现"></a>四、基于Theano的实现</h3><p>最后，跟着<a href="http://deeplearning.net/tutorial/dA.html#daa" target="_blank" rel="noopener">DeepLearning.net</a>的教程，在<code>Theano</code>上实现去噪自编码器。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="http://deeplearning.net/tutorial/dA.html#daa" target="_blank" rel="noopener">DeepLearning.net的DA部分</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7" target="_blank" rel="noopener">UFLDL-自编码算法与稀疏性</a></li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.8111&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Vincent, H. Larochelle Y. Bengio and P.A. Manzagol, Extracting and Composing Robust Features with Denoising Autoencoders, Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML‘08), pages 1096 - 1103, ACM, 2008.</a></li>
<li><a href="http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/190" target="_blank" rel="noopener">Bengio, P. Lamblin, D. Popovici and H. Larochelle, Greedy Layer-Wise Training of Deep Networks, in Advances in Neural Information Processing Systems 19 (NIPS‘06), pages 153-160, MIT Press 2007.</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自编码器</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读笔记：《统计学习方法》-K近邻法</title>
    <url>/2015/11/17/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-k%E8%BF%91%E9%82%BB%E6%B3%95/</url>
    <content><![CDATA[<p><code>k近邻</code>算法在1968年被Cover和Hart提出，是一种基本的分类和回归的算法。它没有显式的学习过程，而是通过训练集对特征向量空间进行划分，作为对样本分类的模型。<code>k的选择</code>、<code>距离度量</code>和<code>分类决策规则</code>是它的三个基本要素。</p>
<h3 id="一、k近邻算法"><a href="#一、k近邻算法" class="headerlink" title="一、k近邻算法"></a>一、k近邻算法</h3><p>输入：<script type="math/tex">T = {(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)}, x_i \in R^n, y_i \in {c_1,c_2,\dots,c_k}</script></p>
<p>输出：实例对应的类别y。</p>
<ul>
<li><p>1.找到最近邻集合$N_k(x)$。</p>
</li>
<li><p>2.在$N_k(x)$中按照决策规则决定类别y。</p>
</li>
</ul>
<h3 id="二、k近邻模型"><a href="#二、k近邻模型" class="headerlink" title="二、k近邻模型"></a>二、k近邻模型</h3><p><strong>1.距离度量</strong></p>
<p>距离度量的方式有很多种：</p>
<ul>
<li><p>$L<em>p$距离：$L_p(x_i,x_j) = (\sum</em>{l=1}^n |x_i^{(l)} - x_j^{(l)}|^p )^{(\frac 1 p)} ,p\geq 1$</p>
</li>
<li><p>欧式距离p=2：$L<em>2(x_i,x_j) = (\sum</em>{l=1}^n |x_i^{(l)} - x_j^{(l)}|^2 )^{(\frac 1 2)}$，是最常见的两点之间距离的度量，又称为欧几里得度量。</p>
</li>
<li><p>曼哈顿距离p=1：$L<em>p(x_i,x_j) = \sum</em>{l=1}^n |x_i^{(l)} - x_j^{(l)}|$，也叫城市街区距离（L1距离），即两点之间的线段对所有坐标轴投影的距离的总和。</p>
</li>
<li><p>切比雪夫距离$p\rightarrow\infty$： $D<em>{Chebyshev}(x,y) = max_i(|x_i - y_i|) = lim</em>{k \rightarrow \infty} L_p(x,y)$，所以切比雪夫距离也叫$L \infty$度量。</p>
</li>
</ul>
<p><strong>2.k的选择</strong></p>
<p>k的选择过大对应简单模型，近似误差会增大；过小对应复杂模型，估计误差会增大。实际应用中一般使用交叉验证方法取一个较小的值。</p>
<p><strong>3.决策规则</strong></p>
<p>一般使用多数表决规则。</p>
<h3 id="三、k近邻实现——kd树"><a href="#三、k近邻实现——kd树" class="headerlink" title="三、k近邻实现——kd树"></a>三、k近邻实现——kd树</h3><p>在k近邻<code>实现</code>的过程中主要考虑的问题是如何对于训练数据进行快速k近邻<code>搜索</code>，最基本的方法是线性扫描，但训练数据集很大时这样时间开销过大。为了减少时间开销，可以使用特殊数据结构存储数据，减少计算距离的次数，这里介绍了<code>kd树</code>方法。这种数据结构对应着对搜索空间进行无重叠的层次划分，而另一种数据结构<code>R树</code>对应着对搜索空间进行有重叠的划分。</p>
<p>kd树是一种空间划分树，按照一定规则把k维空间划分为多个子空间，如下图所示是三维空间的示例：</p>
<p><img src="/images/FnPrSb8CADU87UVaQPQP8XKckClV.jpg" alt=""></p>
<p><strong>1.构造kd树</strong></p>
<ul>
<li><p>1.确定split域：也就是从哪个维度开始进行空间划分，通常使用数据集方差从大到小的维度集合进行循环，如三维中可能对{x,y,z}轴进行循环划分。</p>
</li>
<li><p>2.确定Node-data：也就是如何划分当前维度，通常取数据集在该维度的中位数所在的点。</p>
</li>
<li><p>3.将Node-data作为根节点，所有$x^{(split)}\leq NodeData $的点为左子树，所有$x^{(split)} &gt; NodeData $的点为右子树。</p>
</li>
<li><p>递归上述过程，直到左右子树为空。</p>
</li>
</ul>
<p>对于n个实例的k维数据，建立<code>kd树</code>的时间复杂度为$O(k<em>n</em>logn)$。下面是一个生成的<code>2d树</code>的示例图：</p>
<p><img src="/images/FnUga-YIsRk1n9VB4PNunEz4-PHA.jpg" alt=""></p>
<p><strong>2.搜索kd树</strong></p>
<p>对一个N个节点的kd树，其中一个结点插入和检索的平均代价是$O(log2N)$，而删除随机节点的时间开销上界也是$O(log2N)$。</p>
<p>下面对于kd树搜索最近邻节点的过程进行介绍：</p>
<p>输入：构造的kd树，目标点x。</p>
<p>输出：x的最近邻。</p>
<ul>
<li><p>1.从root出发，在kd树中找到包含目标点x的叶节点$x’$，作为近似最近邻（有可能不是最近邻），并保存搜索路径$root\rightarrow r1 \rightarrow\dots \rightarrow x’$；</p>
</li>
<li><p>2.递归的向搜索路径的上一层回溯，并在每一层做如下操作：</p>
<ul>
<li><p>2.1 如果该节点的实例点与x距离比$x’$更近，则以该点作为近似最近邻；</p>
</li>
<li><p>2.2 如果该节点的另一个子树所在的空间与 <strong>以目标点为球心、以目标点与近似最近邻距离为半径的球体</strong> 相交，则移动到另一个子树并递归搜索最近邻；</p>
</li>
</ul>
</li>
<li><p>3.回溯到根节点时，搜索结束，最近邻点为当前的近似最近邻。</p>
</li>
</ul>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><p>《统计学习方法》</p>
</li>
<li><p><a href="http://blog.csdn.net/v_july_v/article/details/8203674" target="_blank" rel="noopener">July的博客</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>阅读笔记</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读笔记:spark-meetup</title>
    <url>/2015/07/25/machine-learning/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-spark-meetup/</url>
    <content><![CDATA[<h3 id="1-显式矩阵分解（Explicit-Matrix-Factorization）"><a href="#1-显式矩阵分解（Explicit-Matrix-Factorization）" class="headerlink" title="1.显式矩阵分解（Explicit Matrix Factorization）"></a>1.显式矩阵分解（Explicit Matrix Factorization）</h3><ul>
<li>(1) 用户显式的给电影目录的一个子集打分</li>
<li><p>(2) 目标：预测用户给新电影打的分数</p>
</li>
<li><p>(3) 使均方根<code>RMSE</code>误差最小</p>
</li>
</ul>
<p>公式：<script type="math/tex">min_{x,y}\sum_{u,i}(r_{u,i} - x{_u^T}y_i - b_u - b_i)^2 - λ(\sum_u\|x_u\|^2 + \sum_i\|y_i\|^2)</script></p>
<ul>
<li>(4) 公式解析：<ul>
<li>$λ$是正则化参数，控制着正则化的程度，用来避免过拟合的问题，通过交叉验证确定</li>
<li>$μ$表示所有评分的平均值，$b_u$和$b_i$描述用户和物品相对于平均值$μ$的偏差（有些用户普遍评价较低，有些用户普遍评价较高，添加一阶偏置项）</li>
<li>$X_u$表示用户$u$的潜在因子向量（特征向量），$Y_i$表示物品$i$的潜在因子向量（特征向量）</li>
</ul>
</li>
</ul>
<h3 id="2-隐式矩阵分解"><a href="#2-隐式矩阵分解" class="headerlink" title="2.隐式矩阵分解"></a>2.隐式矩阵分解</h3><p>和显式矩阵不同的地方在于隐式矩阵的输入是用户的购买浏览等记录，隐式矩阵通常是稠密矩阵，而显式矩阵通常是稀疏矩阵。</p>
<p>公式：<script type="math/tex">min_{x,y}\sum_{u,i}c_{u,i}(p_{u,i} - x{_u^T}y_i - b_u - b_i)^2 - λ(\sum_u\|x_u\|^2 + \sum_i\|y_i\|^2)</script></p>
<ul>
<li>公式解析：<ul>
<li>$C_{ui}$ 可信度，用户$u$对物品$i$隐式评价的权值</li>
<li>$P_{ui}$ 为<code>user-item</code>矩阵$P$的值，表示用户$u$对物品$i$的隐式评价</li>
<li>$μ$表示所有评分的平均值，$b_u$和$b_i$描述用户和物品相对于平均值$μ$的偏差（有些用户普遍评价较低，有些用户普遍评价较高，添加一阶偏置项）</li>
</ul>
</li>
</ul>
<h3 id="3-迭代逼近的方法"><a href="#3-迭代逼近的方法" class="headerlink" title="3.迭代逼近的方法"></a>3.迭代逼近的方法</h3><p>由于数据量大，随机梯度下降的方法不再适合，使用易于并行的交替最小二乘法来逐步迭代逼近。</p>
<ul>
<li>(1) 目标：对于每一个用户$u$计算出$X<em>u$；对于每一个物品$i$计算出$Y_i$，最终通过计算出的$X_u$和$Y_i$向量计算出$P</em>{ui}$</li>
<li>(2) 交替确定$X_u$和$Y_i$向量，计算出另外一个使得消耗函数的值减少，不断重复过程直至收敛或稳定</li>
<li>(3) 矩阵计算优化：$Y^TC_uY$的计算时间开销为$O(f^2n)$，而由于$C_u$中非0元素个数远小于n，因此可以在每次迭代时先提前计算出$Y^TY$，再计算$Y^T(C_u-I)Y$，可以减少计算的时间开销</li>
<li>(4) 每次迭代计算特征向量的公式：<script type="math/tex; mode=display">X_u = (Y^TC^uY + λI)^{-1}Y^TC^up(u)</script></li>
</ul>
<h3 id="4-算法输入输出"><a href="#4-算法输入输出" class="headerlink" title="4.算法输入输出"></a>4.算法输入输出</h3><ul>
<li><p>(1) 矩阵加载函数<code>load_Matrix()</code></p>
<ul>
<li><strong>参数</strong>：数据输入文件文件名，用户数量，物品数量</li>
<li><strong>输入</strong>：该函数输入数据为观测值统计文件，文件每行格式如下：<br><code>[用户 物品 偏好值]</code><ul>
<li>观测值的意义尚未确定，可能是一项隐式反馈的值，比如用户观看物品的次数；也可能是多项隐式反馈的综合值</li>
<li>观测值的定义是直接观测到的用户行为，在论文所做的实验中，是用户观看整部电影的次数，例如用户只观看了70%的时间，那么偏好值为0.7；而若用户观看了两次，那么偏好值为2</li>
</ul>
</li>
<li><strong>输出</strong>：<code>user-item</code>观测值矩阵，其中第$u$行第$i$列元素为用户$u$对物品$i$的观测值，该观测值的意义和输入文件中观测值相同</li>
<li><strong>输出结构</strong>：<code>Scipy</code>库中的<code>csr_Matrix</code>类型，即压缩稀疏行矩阵（compressed sparse row）]</li>
</ul>
</li>
<li><p>(2) <strong>ImplicitMF</strong>类</p>
<ul>
<li><strong>参数</strong>：<code>user-item</code>观测值矩阵，特征数量，迭代次数，归一化因子$λ$<ul>
<li><code>user-item</code>观测值矩阵即矩阵加载函数输出的观测值矩阵</li>
<li>特征数量即用户特征向量和物品特征向量的长度</li>
<li>迭代次数即使用交替最小二乘计算用户特征向量和物品特征向量的次数</li>
</ul>
</li>
<li><strong>计算过程</strong>：<ul>
<li>首先对于输入的用户观测值矩阵$R$，将其转化为用户偏好矩阵$P$，$P_{ui}$为1说明用户对物品表现出偏好，为0说明用户对物品没有表现出偏好<script type="math/tex; mode=display">p_{ui} = \left\{ \begin{array}
\overline 1 & r_{ui} > 0 \\ 0 & r_{ui} = 0 \\
\end{array}\right.</script></li>
<li>使用置信度$C<em>{ui}$来为偏好度$P</em>{ui}$加权</li>
<li>交替最小二乘的每次迭代首先固定物品特征矩阵$Y$来计算所有用户的用户特征向量$x_u,u\epsilon [0,Num_u]$；然后固定用户特征矩阵$X$来计算所有物品的物品特征向量$Y_i,i\epsilon [0,Num_i]$</li>
</ul>
</li>
<li><strong>输出</strong>：所有用户特征向量<code>user_vectors</code>，所有物品特征向量<code>item_vectors</code><ul>
<li>迭代结束后，根据最后输出的用户特征矩阵$X$和物品特征矩阵$Y$，即可计算出特定用户$u$对特定物品$i$的预测偏好度${P_{ui}}^\prime=Y_i^T\times X_u$，该偏好度代表用户可能喜欢该物品的概率</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-隐式矩阵分解的Hadoop扩展"><a href="#5-隐式矩阵分解的Hadoop扩展" class="headerlink" title="5.隐式矩阵分解的Hadoop扩展"></a>5.隐式矩阵分解的Hadoop扩展</h3><ul>
<li>(1).Map阶段<ul>
<li>u % K = x &amp; i % L = y 的向量，其中$u$，$i$为用户ID和物品ID，$K$和$L$是定义的分块数量</li>
</ul>
</li>
<li>(2).Reduce阶段<ul>
<li>Reduce阶段将[用户/物品]分到$K$个节点进行处理，分别计算[用户向量/物品向量]</li>
</ul>
</li>
<li>(3).Hadoop的I/O瓶颈</li>
</ul>
<h3 id="6-隐式矩阵分解-in-Spark"><a href="#6-隐式矩阵分解-in-Spark" class="headerlink" title="6.隐式矩阵分解 in Spark"></a>6.隐式矩阵分解 in Spark</h3><ul>
<li>(1).第一次尝试<ul>
<li>计算过程<ul>
<li>计算$Y^TY$并广播到所有节点</li>
<li>将用户向量$X_u$和所有评分矩阵中的评分以及用户$u$有评分的物品向量$Y_i$进行Join</li>
<li>将$Y^TCuIY$和$Y^TCuP(u)$加起来并求解</li>
</ul>
</li>
<li>缺陷<ul>
<li>多次将物品向量$Y$复制到各个节点上(没必要)</li>
<li>每次迭代都对数据进行了shuffle(没必要)</li>
<li>没有利用Spark的内存计算能力</li>
</ul>
</li>
</ul>
</li>
<li>(2).第二次尝试<ul>
<li>计算过程<ul>
<li>计算$Y^TY$并广播到所有节点</li>
<li>将评分矩阵按块分组，将每块与用户$u$有评分的物品向量$Y_i$进行Join</li>
<li>将$Y^TCuIY$和$Y^TCuP(u)$加起来并求解</li>
</ul>
</li>
<li>缺陷<ul>
<li>每次迭代都对数据进行了shuffle(没必要)</li>
<li>没有利用Spark的内存计算能力</li>
</ul>
</li>
</ul>
</li>
<li>(3).第三次尝试<ul>
<li>计算过程<ul>
<li>首先，将评分矩阵、用户向量和物品向量按照用户和物品分区并缓冲在内存中</li>
<li>其次，计算用户和物品的inlink映射和outlink映射</li>
<li>计算$Y^TY$并广播到所有节点</li>
<li>对于每个物品块，使用outlink映射将他们复制到必要的用户块中</li>
<li>对于每个用户块，使用inlink映射和物品向量来更新用户向量</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>阅读笔记</tag>
        <tag>推荐算法</tag>
        <tag>协同过滤</tag>
        <tag>Spark</tag>
        <tag>ALS</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读笔记：《统计学习方法》-感知机</title>
    <url>/2015/11/16/machine-learning/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B-%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    <content><![CDATA[<h3 id="一、感知机模型"><a href="#一、感知机模型" class="headerlink" title="一、感知机模型"></a>一、感知机模型</h3><p>感知机是二元分类的线性分类模型，对应于将实例分类的分离超平面，因此属于判别模型。</p>
<p>感知机由输入到输出的映射为：</p>
<script type="math/tex; mode=display">y = f(x) = sign(wx + b)</script><p>其中，</p>
<script type="math/tex; mode=display">
sign(x) = \left\{ \begin{array}{rcl}
+1 & \mbox{for} & x \geq 0 \\
-1 & \mbox{for} & x < 0
\end{array}\right.</script><p>$wx+b = 0$可以看作是特征空间$R^n$中的超平面，这个超平面把空间分成两个部分。</p>
<h3 id="二、学习策略"><a href="#二、学习策略" class="headerlink" title="二、学习策略"></a>二、学习策略</h3><p>损失函数，首先给出空间$R^n$中一点$x_0$到超平面S的距离：</p>
<script type="math/tex; mode=display">\frac 1{||w||} |wx_0 + b|</script><p>误分类点到超平面的距离是：</p>
<script type="math/tex; mode=display">- \frac 1{||w||} y_i(wx_0 + b)</script><p>故，损失函数是：</p>
<script type="math/tex; mode=display">L(w,b) = \sum_{x_i\in M} y_i(wx_i + b)</script><h3 id="三、学习算法"><a href="#三、学习算法" class="headerlink" title="三、学习算法"></a>三、学习算法</h3><p>感知机学习问题转化为最优化损失函数式问题，可以使用<code>SGD</code>随机梯度下降来优化。</p>
<p>梯度：</p>
<script type="math/tex; mode=display">\nabla_wL(w,b) = -\sum_{x_i\in M} y_ix_i</script><script type="math/tex; mode=display">\nabla_bL(w,b) = -\sum_{x_i\in M} y_i</script><p>梯度下降：</p>
<script type="math/tex; mode=display">w_{t + 1} = w_t - \eta \nabla_w</script><script type="math/tex; mode=display">b_{t + 1} = b_t - \eta \nabla_b</script><p><code>Novikoff</code>在1962年证明了对于线性可分数据集（存在超平面可以完全正确分割该数据集），感知机的误分类次数存在上界即能够收敛。<a href="http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf" target="_blank" rel="noopener">这里是别人的证明过程PDF</a>，结论如下式：</p>
<script type="math/tex; mode=display">k \leq \frac {R^2} {\gamma ^ 2}</script><p>也就是说，对线性可分数据集，感知机算法修正的次数不会大于k次。</p>
<p><strong>感知机学习算法的对偶形式</strong></p>
<p>基本思想就是把w和b表示为$x_i,y_i$的线性组合形式，通过求解系数来求得w和b，也就是：</p>
<script type="math/tex; mode=display">w=\sum_{i=1}^N \alpha_iy_ix_i</script><script type="math/tex; mode=display">b=\sum_{i=1}^N \alpha_i y_i</script><p>选取$(x_i,y_i)$，如果：</p>
<script type="math/tex; mode=display">y_i(\sum_{j=1}^N \alpha_j y_j x_j x_i + b) \leq 0</script><p>那么进行参数更新直到收敛。</p>
<p>由于这里实例只以内积的形式出现，所以可以先把数据集中的实例内积计算出来存储在矩阵中，即Gram矩阵：</p>
<p>$G = [ {x<em>i} \cdot {x_j}]</em>{N \times N}$</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li>《统计学习方法》</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>阅读笔记</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读笔记：《统计学习方法》-统计学习方法概论</title>
    <url>/2015/11/02/machine-learning/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/</url>
    <content><![CDATA[<h3 id="一、统计学习的三要素"><a href="#一、统计学习的三要素" class="headerlink" title="一、统计学习的三要素"></a>一、统计学习的三要素</h3><ul>
<li>1.模型</li>
</ul>
<p>寻找假设函数。<br>模型是什么，模型在监督学习中就是要学习的决策函数$f(x)$或者条件概率分布$P(y|x)$。模型的假设空间就是所有可能的决策函数或者条件概率分布的集合。</p>
<p>假设空间$F$就是参数向量决定的<code>决策函数族</code>，或者参数向量决定的<code>条件概率分布族</code>。</p>
<script type="math/tex; mode=display">F = {f|Y = f_\theta(X),\theta \in R^n}</script><p>或者</p>
<script type="math/tex; mode=display">F = {P|Y = P_\theta(Y|X),\theta \in R^n}</script><p>决策函数表示的模型叫非概率模型，给定一个样本，决策函数得出唯一的输出标签，属于硬分类；<br>条件概率表示的模型叫做概率模型，给定一个样本，决策函数给出该样本对应每一种输出的概率，属于软分类。</p>
<ul>
<li>2.策略</li>
</ul>
<p>有了模型之后，就要有一个在假设空间中学习和选择最优模型的准则。</p>
<p>2.1 损失函数:</p>
<blockquote>
<p>表示一次预测的错误程度。</p>
<p>常用的损失函数有：0-1损失，对数损失log loss，平方损失，绝对损失，交叉熵损失函数<br>。<br>2.2 风险函数：</p>
<p>风险函数度量平均意义下模型度量的错误程度。</p>
<p>期望风险：就是理论上模型f(x)关于联合分布P(x,y)的平均损失。（由于无法获得x和y的联合分布，因此期望风险是无法直接求出的）</p>
<p>经验风险：对于测试样本集的平均损失。用经验风险来估计期望风险，样本不足时需要矫正（ERM和SRM）。</p>
</blockquote>
<p>2.3 经验风险最小化ERM &amp; 结构风险最小化SRM</p>
<blockquote>
<p>结构风险最小化，也就是正则化，通过添加一个作为惩罚项的范式，来限制模型的复杂度。</p>
</blockquote>
<ul>
<li>3.算法</li>
</ul>
<p>建立模型并找到寻找最优模型的策略（SRM，ERM）之后，就变成了在一定数据集上针对经验风险函数或者结构风险函数的最优化问题。<br><br><br>选择相应的算法求解最优模型，比如如下几种算法：</p>
<blockquote>
<p>梯度下降法家族：SGD,OGD等，</p>
<p>牛顿法家族：BFGS等，</p>
<p>信赖域算法：求解非线性优化问题</p>
<p>FTRL：在线学习的优化算法</p>
</blockquote>
<h3 id="二、模型泛化能力"><a href="#二、模型泛化能力" class="headerlink" title="二、模型泛化能力"></a>二、模型泛化能力</h3><p><code>训练误差</code>：模型在训练数据集上的误差；<br><br><code>测试误差</code>：模型在测试数据集上的误差；<br><br>测试误差小的模型具有更强的对未知数据的预测能力即<code>泛化</code>能力。然而只减小训练误差可能发生过拟合，使模型变得复杂，泛化能力又很低，无法有效对于其他数据进行预测。可以通过在损失函数中加入正则项来选择更简单的模型。在训练模型时可以使用<code>交叉验证</code>的方式。</p>
<p>可以通过证明<code>泛化误差上界</code>的方式从理论上证明模型的优越性。</p>
<h3 id="三、生成模型和判别模型"><a href="#三、生成模型和判别模型" class="headerlink" title="三、生成模型和判别模型"></a>三、生成模型和判别模型</h3><p>生成方法和判别方法是监督学习的两种学习方法，学习到的模型又称为生成模型和判别模型。<br><br>生成模型先学习$P(x,y)$的概率分布然后计算$argmax_yP(y|x;\theta)$，也就是先计算$P(x|y;\theta)$，再利用贝叶斯公式$P(y|x) = \frac {P(x|y)P(y)} {P(x)}$计算出$P(y|x)$，比如贝叶斯模型、隐马尔科夫模型等。<br><br>判别模型是利用给定的$x$的特征直接计算$argmax_yP(y|x;\theta)$，比如逻辑回归、K-means模型、感知机等。</p>
<h3 id="四、监督学习分类"><a href="#四、监督学习分类" class="headerlink" title="四、监督学习分类"></a>四、监督学习分类</h3><ul>
<li>1.分类问题</li>
</ul>
<p>根据已知的训练数据学习出一个分类模型预测样本的输出类别。分类问题的输出是<code>离散的</code>。<br>分类问题的评价指标一般是分类<code>准确率</code>，<code>精确率</code>和<code>召回率</code>。</p>
<ul>
<li>2.回归问题</li>
</ul>
<p>根据数据学习出一个函数，对样本输入确定对应的输出。回归问题的输出是<code>连续的</code>。</p>
<ul>
<li>3.标注问题</li>
</ul>
<p>分类问题的推广，对于一个观测序列输入，学习模型以给出一个标记或状态序列的输出。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li>《统计学习方法》</li>
<li><a href="http://blog.csdn.net/zouxy09/article/details/8195017" target="_blank" rel="noopener">生成模型与判别模型</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>阅读笔记</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title>AoA-Reader实现总结-Tensorflow</title>
    <url>/2017/05/09/ml-coding-summarize/AoA-Reader%E5%9C%A8tensorflow%E4%B8%8B%E5%AE%9E%E7%8E%B0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>复现的代码在这里：<a href="https://github.com/zhanghaoyu1993/RC-experiments" target="_blank" rel="noopener">https://github.com/zhanghaoyu1993/RC-experiments</a>。</p>
<h4 id="1、Attention-Sum实现"><a href="#1、Attention-Sum实现" class="headerlink" title="1、Attention Sum实现"></a>1、Attention Sum实现</h4><p>使用<code>tf.gather</code>和<code>tf.unsorted_segment_sum</code>两个API实现了attention的sum和根据candidate选择操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_hat_bi = tf.scan(fn=<span class="keyword">lambda</span> prev, cur: tf.gather(tf.unsorted_segment_sum(cur[<span class="number">0</span>], cur[<span class="number">1</span>], vocab_size), cur[<span class="number">2</span>]),</span><br><span class="line">                   elems=[s_bd, documents_bt, candidates_bi],</span><br><span class="line">                   initializer=tf.Variable([<span class="number">0</span>] * self.A_len, dtype=<span class="string">"float32"</span>))</span><br></pre></td></tr></table></figure>
<h4 id="2、复用参数：tf-get-variable"><a href="#2、复用参数：tf-get-variable" class="headerlink" title="2、复用参数：tf.get_variable()"></a>2、复用参数：tf.get_variable()</h4><p>同一命名空间中如果使用tf.get_variable()，那么第一次使用会初始化这个变量，之后的使用就直接获取同一个变量，避免重复产生不需要的变量，便于重构。如果需要在一个variable_scope中两次调用该方法，需要调用scope.reuse_variables()。</p>
<h4 id="3、好用的张量运算方法：tf-einsum"><a href="#3、好用的张量运算方法：tf-einsum" class="headerlink" title="3、好用的张量运算方法：tf.einsum()"></a>3、好用的张量运算方法：tf.einsum()</h4><p>详见官网API，可以用下面这种方式定义张量运算（虽然比matmul等方法要慢），如果在想不起来应该调用什么API的时候，用它。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">einsum(<span class="string">'ij,jk-&gt;ik'</span>, m0, m1)  <span class="comment"># output[i,k] = sum_j m0[i,j] * m1[j, k]</span></span><br></pre></td></tr></table></figure>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/variable_scope.html" target="_blank" rel="noopener">http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/variable_scope.html</a></p>
]]></content>
      <categories>
        <category>编程总结</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>机器阅读理解</tag>
        <tag>attention</tag>
        <tag>GRU</tag>
        <tag>tensorflow</tag>
        <tag>编程总结</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow中GRU和LSTM的权重初始化</title>
    <url>/2017/05/05/ml-coding-summarize/Tensorflow%E4%B8%ADGRU%E5%92%8CLSTM%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/</url>
    <content><![CDATA[<h4 id="GRU和LSTM权重初始化"><a href="#GRU和LSTM权重初始化" class="headerlink" title="GRU和LSTM权重初始化"></a>GRU和LSTM权重初始化</h4><p>在编写模型的时候，有时候你希望RNN用某种特别的方式初始化RNN的权重矩阵，比如<code>xaiver</code>或者<code>orthogonal</code>，这时候呢，只需要：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cell = LSTMCell <span class="keyword">if</span> self.args.use_lstm <span class="keyword">else</span> GRUCell</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(initializer=tf.orthogonal_initializer()):</span><br><span class="line">    input = tf.nn.embedding_lookup(embedding, questions_bt)</span><br><span class="line">    cell_fw = MultiRNNCell(cells=[cell(hidden_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)])</span><br><span class="line">    cell_bw = MultiRNNCell(cells=[cell(hidden_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)])</span><br><span class="line">    outputs, last_states = tf.nn.bidirectional_dynamic_rnn(cell_bw=cell_bw,</span><br><span class="line">                                                           cell_fw=cell_fw,</span><br><span class="line">                                                           dtype=<span class="string">"float32"</span>,</span><br><span class="line">                                                           inputs=input,</span><br><span class="line">                                                           swap_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>那么这么写到底是不是正确的初始化了权重呢，我们跟着bidirectional_dynamic_rnn的代码看进去，先只看forward：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> vs.variable_scope(<span class="string">"fw"</span>) <span class="keyword">as</span> fw_scope:</span><br><span class="line">    output_fw, output_state_fw = dynamic_rnn(</span><br><span class="line">        cell=cell_fw, inputs=inputs, sequence_length=sequence_length,</span><br><span class="line">        initial_state=initial_state_fw, dtype=dtype,</span><br><span class="line">        parallel_iterations=parallel_iterations, swap_memory=swap_memory,</span><br><span class="line">        time_major=time_major, scope=fw_scope)</span><br></pre></td></tr></table></figure>
<p>发现它增加了一个variable_scope叫做fw_scope，继续看dynamic_rnn发现这个scope只用在了缓存管理中，而dynamic_rnn实际调用了下面的内容：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(outputs, final_state) = _dynamic_rnn_loop(</span><br><span class="line">        cell,</span><br><span class="line">        inputs,</span><br><span class="line">        state,</span><br><span class="line">        parallel_iterations=parallel_iterations,</span><br><span class="line">        swap_memory=swap_memory,</span><br><span class="line">        sequence_length=sequence_length,</span><br><span class="line">        dtype=dtype)</span><br></pre></td></tr></table></figure>
<p>总之，调用来调用去，最后调用到了一个语句：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">call_cell = <span class="keyword">lambda</span>: cell(input_t, state)</span><br></pre></td></tr></table></figure>
<p>好，最后都调用了GRUCell或者LSTMCell的__call__()方法，我们顺着看进去，比如GRU的__call__()长下面这个样子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">    <span class="string">"""Gated recurrent unit (GRU) with nunits cells."""</span></span><br><span class="line">    <span class="keyword">with</span> _checked_scope(self, scope <span class="keyword">or</span> <span class="string">"gru_cell"</span>, reuse=self._reuse):</span><br><span class="line">        <span class="keyword">with</span> vs.variable_scope(<span class="string">"gates"</span>):  <span class="comment"># Reset gate and update gate.</span></span><br><span class="line">            <span class="comment"># We start with bias of 1.0 to not reset and not update.</span></span><br><span class="line">            value = sigmoid(_linear(</span><br><span class="line">                [inputs, state], <span class="number">2</span> * self._num_units, <span class="literal">True</span>, <span class="number">1.0</span>))</span><br><span class="line">            r, u = array_ops.split(</span><br><span class="line">                value=value,</span><br><span class="line">                num_or_size_splits=<span class="number">2</span>,</span><br><span class="line">                axis=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">with</span> vs.variable_scope(<span class="string">"candidate"</span>):</span><br><span class="line">                c = self._activation(_linear([inputs, r * state],</span><br><span class="line">                                             self._num_units, <span class="literal">True</span>))</span><br><span class="line">                new_h = u * state + (<span class="number">1</span> - u) * c</span><br><span class="line">                <span class="keyword">return</span> new_h, new_h</span><br></pre></td></tr></table></figure>
<p>咦？怎么没有权重和偏置呢？好像__init__()方法里也没有，看到这个_linear()了吧，其实所有的权重都在这个方法里面（LSTMCell也一样），这个方法中有玄机了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> vs.variable_scope(scope) <span class="keyword">as</span> outer_scope:</span><br><span class="line">    weights = vs.get_variable(</span><br><span class="line">        _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)</span><br><span class="line"><span class="comment"># ....some code    </span></span><br><span class="line"><span class="keyword">with</span> vs.variable_scope(outer_scope) <span class="keyword">as</span> inner_scope:</span><br><span class="line">    inner_scope.set_partitioner(<span class="literal">None</span>)</span><br><span class="line">    biases = vs.get_variable(</span><br><span class="line">        _BIAS_VARIABLE_NAME, [output_size],</span><br><span class="line">        dtype=dtype,</span><br><span class="line">        initializer=init_ops.constant_initializer(bias_start, dtype=dtype))</span><br></pre></td></tr></table></figure>
<p>所以，这个方法里面，就是又增加了一个variable_scope，然后调用get_variable()方法获取权重和偏置。所以，我们的variable_scope里面嵌套了若干层variable_scope后，我们定义的初始化方法还有没有用呢，实验一下吧：</p>
<p><img src="/images/FpuZmtRf2SuSlzVIz6bhAkUJsBSl.jpg" alt=""></p>
<p>好的，经过我们的测试，嵌套的variable_scope如果内层没有初始化方法，那么以外层的为准。所以我们的结论呼之欲出：</p>
<ul>
<li>RNN的两个变种在Tensorflow版本1.1.0的实现，只需要调用它们时在variable_scope加上初始化方法，它们的权重就会以该方式初始化；</li>
<li>但是无论是LSTM还是GRU，都没有提供偏置的初始化方法（不过好像可以定义初始值）。</li>
</ul>
]]></content>
      <categories>
        <category>编程总结</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>GRU</tag>
        <tag>LSTM</tag>
        <tag>tensorflow</tag>
        <tag>编程总结</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读笔记:《Stacked Denoising Autoencoders： Learning Useful Representations in a Deep Network with a Local Denoising Criterion》</title>
    <url>/2015/11/05/machine-learning/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-Stacked-Denoising-Autoencoders-Learning-Useful-Representations-in-a-Deep-Network-with-a-Local-Denoising-Criterion/</url>
    <content><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>基于堆叠去噪自编码器来建立用于分类的深度网络可以将分类性能提高到接近或者超过<code>DBN</code>的水平。这种无监督式的学习方式学习到的高级表示也能够提高后续的SVM分类器的性能。定性实验表明，相比于经典的自编码器，去噪自编码器能够从图像块中学习到<code>Gabor-like</code>边缘检测器和<code>stroke detectors</code>。这些工作说明了，在非监督方式下，使用去噪自编码器是学习有用的高级表示的好方法。</p>
<p>关键字：深度学习，非监督特征学习，深度置信网络，自编码器，去噪</p>
<h3 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h3><p>长久以来神经网络方面的研究认为很多非线性层的组合才能够有效的对变量间的复杂关系建模并在复杂的识别任务中有更好的泛化性能。但是由于多层神经网络的非凸优化问题，很长一段时间超过一到两层隐藏层的神经网络无法得到期望的结果。因此很多机器学习方面的研究转向了能够进行凸优化的浅层模型。最近它的复兴得益于<code>Hinton</code>教授等人提出的能够有效训练参数的新方法。在他2006年将这个方法应用于深度置信网络（<code>DBN</code>）之后很多类似的技术和改进被提出。但是，所有这些改进都基于相同的原理，总结如下：</p>
<ul>
<li>1.深度网络的训练，随机初始化参数、并直接通过监督式学习的方式以梯度下降法优化损失函数，这样的方式效果不好；</li>
<li>2.效果更好的方式是：首先通过局部的非监督式方式依次对每一层进行预训练，学习的目的是由前一层的低级表示输出有用的高级表示；然后再通过监督式学习来进行参数微调。</li>
</ul>
<p>但是我们还不知道一个表示是好还是坏是由什么决定的。我们只知道一些算法的效果非常好，比如受限玻尔兹曼机（<code>RBM</code>）2006、自编码器（<code>autoencoders</code>）2007、以及<code>semi-supervised embedding</code>2008和核主成分分析(<code>Kernel PCA</code>)2010。</p>
<p>值得注意的是RBM和经典的自编码器在函数形式上非常相似，虽然他们的解释和训练过程很不同。尤其是，这两个模型将输入映射到平均隐藏表示的决定函数是相同的。一个不同是确定的自编码器将平均值作为它的隐藏表示而随机的RBM从均值中抽样出一个二进制的隐藏表示；但是，RBM预训练完成后，堆栈式叠加是通过真实的均值传递而进行的，这点更符合自编码器的解释。自编码器的重建误差可以看作RBM的对数似然梯度的近似，尤其是RBM使用<code>对比差异更新</code>2009。所以使用自编码器堆叠的深度网络具有接近DBN的分类性能，但是为什么只是接近而不是超过呢？本研究的一个初始动机就是找到一种方法来缩小这个性能差距。</p>
<p>在第二节我们从信息保存的直观概念出发，给出经典自编码器的泛化形式化表达，而后指出它的局限性。第三节我们推导出去噪自编码器的模型，并给出直观的几何解释。之后会在基本模型的基础上推导更多的扩展。第四节讨论相关的工作和方法。第五节进行实验，定性的学习不同条件下单层去噪自编码器学习到的特征检测器。第六节描述在多层去噪自编码器形成的网络进行实验，对比它和其他优秀模型的分类性能。第七节尝试把SDaE应用在实际的生成模型中，对比它和DBN生成的样本。第八节总结我们的工作和发现。</p>
<p><strong>1.1 使用的符号</strong></p>
<p><strong>1.2 基本设置</strong></p>
<p>我们考虑一般的监督式学习的设置，$n$对训练集$D_n = { (x^{(1)},t^{(1)})  …, (x^{(n)},t^{(n)}) }$，训练集来自于从未知联合分布$q(X,T)$以及边缘分布$q(X)$和$q(T)$中<code>i.i.d</code>抽样。我们把样本$D_n$的经验分布定义为$q^0(X,T)$和$q^0(X)$。$X$是一个$d$维随机向量，即$X\in R^d$或者$X\in [0,1]^d$。</p>
<p>我们的工作主要关心给出一个$X$的新的高级表示$Y$。$Y$是一个$d’$维随机变量，如果$d’ &gt; d$那么我们讨论过完备表示，而如果$d’ &lt; d$则变成了欠完备表示。Y和X由一个确定性的或者随机的映射$q(Y|X;\theta)$来表示，其参数是向量$\theta$。</p>
<h3 id="二、什么才是一个好的表示？从互信息到自编码器"><a href="#二、什么才是一个好的表示？从互信息到自编码器" class="headerlink" title="二、什么才是一个好的表示？从互信息到自编码器"></a>二、什么才是一个好的表示？从互信息到自编码器</h3><p>我们从一开始就可以从实用主义观点给出好的表示的定义，那就是让系统在使用这些表示之后性能有提高。比如我们根据最顶层的分类器的分类性能来判定“表示”的好坏。但是，最近深度网络训练技术的突破给我们带来的启示是，不要把狭隘的分类任务定义的分类误差作为指导“表示学习”的唯一准则。首先，实验证明，即使仅针对无监督标准优化，一些特定的分类问题仍然可以得到性能的提升，而这些性能提升并非得益于特征表示的学习。其次可以肯定，人类能够很快熟悉新任务的能力来自于他们之前面对类似任务时学到了什么（先验）。</p>
<p>这一节，从保留信息（<code>retaining information</code>）开始，从一个更好的角度对传统的自编码器范式进行介绍。</p>
<p><strong>2.1 输入的保留信息</strong></p>
<p>我们希望学习以$\theta$为参数的一个$X$到$Y$的映射$q(Y|X;\theta)$。一个自然的评判准则就是好的表示就是相对于输入保留更多的信息，从信息论的角度也就是最大化$X$和$Y$之间的互信息$I(X;Y)$，这是1989年Linsker提出的<code>infomax定理</code>的内容。</p>
<p>互信息可以被分解为一个熵和一个条件熵的形式，如下所示。</p>
<script type="math/tex; mode=display">I(X;Y) = H(Y) - H(Y|X)</script><script type="math/tex; mode=display">I(X;Y) = H(X) - H(X|Y)</script><p>这里我们选择第二个式子，因为$X$来自于一个不受参数影响的未知分布$q(X)$，所以$H(X)$是一个未知常数。这样的话<code>infomax定理</code>简化为以下形式：</p>
<script type="math/tex; mode=display">
\begin{align*}
 argmax_\theta I(X;Y) &= argmax_\theta - H(X|Y) \\
 &= argmax_\theta E_{q(X,Y)}[\log {q(X|Y)}]
\end{align*}</script><p>对于任何$p(X|Y)$，我们有：</p>
<script type="math/tex; mode=display">E_{q(X,Y)} [\log {p(X|Y)}] \leq E_{q(X,Y)} [\log {q(X|Y)}] = -H(X|Y)</script><p>考虑分布$p(X|Y;\theta’)$和下面的优化：</p>
<script type="math/tex; mode=display">max_{\theta,\theta'} E_{q(X,Y;\theta)} [\log {p(X|Y;\theta')}]</script><p>由之前的不等式可知，这对应着最大化$-H(X|Y)$的下界也就是最大化互信息。也就是<br>找到$\exists \theta’ s.t. q(X|Y) = p(X|Y;\theta’)$</p>
<p>由于$Y = f_\theta(X) $，因此优化式如下：</p>
<script type="math/tex; mode=display">max_{\theta,\theta'} E_{q(X)} 
[\log {p(X|Y=f_\theta(X);\theta')}]</script><p>因为$q(X)$未知，因此使用样本估计的分布$q_0(X)$，优化式变为：</p>
<script type="math/tex; mode=display">max_{\theta,\theta'} E_{q_0(X)} 
[\log {p(X|Y=f_\theta(X);\theta')}]</script><p>在下一节我们可以看到这个等式对应着自编码器的重建误差。</p>
<p><strong>2.2 传统自编码器</strong></p>
<ul>
<li>1.编码器：</li>
</ul>
<script type="math/tex; mode=display">\begin{align*}
& y = f_\theta(x) = s(Wx + b) \\
& \theta = \{W,b\} \\
& W \in R^{ d' \times d} \\
& b \in R^{d'}
\end{align*}</script><ul>
<li>2.解码器：</li>
</ul>
<script type="math/tex; mode=display">\begin{align*}
& z = g_{\theta'}(y) = s(W'y + b') \\
& \theta' = \{W',b'\} \\
& W \in R ^ {d \times d'} \\
& b' \in R^{d}
\end{align*}</script><p>总之$z$不能被理解为$x$的一个确定的重建，而是一个概率项$p(X|Z=z)$中概率最高的$z$。</p>
<script type="math/tex; mode=display">p(X|Y = y) = p(X|Z = g_{\theta'}(y))</script><p>那么重建误差可以被优化为如下形式：</p>
<script type="math/tex; mode=display">L(x,z) \varpropto - \log{p(x|z)}</script><p>一般$p(x|z)$和$L(x,z)$的选择有以下两种：</p>
<ul>
<li><p>1.如果$x\in R^d$那么$X|z \sim N(z,\sigma^2 I)$，$L(x,z) = C(\sigma^2)||x - z||^2$，因为$C(\sigma^2)$是仅与$\sigma^2$有关的常数，因此可以在优化时忽略。这是经典自编码器中常见的均方误差目标。这种设定下，译码器通常不使用非线性函数如<code>sigmoid</code>函数。</p>
</li>
<li><p>1.对于二进制向量输入$x\in {0,1}^d$那么$X|z \sim B(z)$，此时通常使用<code>sigmoid</code>这样的非线性函数。那么有$L(x,z) = L_H(x,z) = -\sum_j[x_jlogz_j + (1 - x_j)log(1 - z_j)] = H(B(x)||B(z))$，也被称为交叉误差，因为这是两个独立的多元伯努利分布的交叉熵。这种方式对于$x\in[0,1]^d$也可以适用。</p>
</li>
</ul>
<p>编码器和解码器的函数和损失函数还可以有其他的形式。在Vincent2009年的论文中说明了一个更复杂的编码器函数的作用，我们进行了研究。但我们在这里做一个限制，那就是<code>仿射</code>+<code>sigmoid</code>编码器，搭配一个<code>仿射</code>解码器+均方根误差损失函数或者一个<code>仿射</code>+<code>sigmoid</code>解码器+交叉熵损失函数。还可以增加一个权重绑定的限制，也就是令<code>W&#39; = W^T</code>，这样可以加速RBM的训练。</p>
<p>自编码器的训练目的是最小化重建误差，也就是下面的优化式：</p>
<script type="math/tex; mode=display">min_{\theta,\theta'} E_{q_0(x)} [L(x,z(x))]</script><p>从我们对$L(x,z)$的定义，推导上式为：</p>
<script type="math/tex; mode=display">
 min_{\theta,\theta'} E_{q_0(x)} [p(X|Z = g_{\theta'}(f_\theta(x)))] =min_{\theta,\theta'} E_{q_0(x)} [p(X|Y = f_\theta(x))]</script><p>这个优化式就是我们之前推导的最大化$X$和$Y$的互信息下界的式子。因此训练一个自编码器就是要最大化输入和潜在表示之间互信息的下界。</p>
<p><strong>2.3 仅仅保留信息是不够的</strong></p>
<p>$Y$应当保留尽可能多的$X$的信息这一准则足够产生一个有用的表示。但是实际上设置$Y=X$就可以最大化互信息。如果$Y$的维度大于等于$X$的维度，那么自编码器很有可能通过学习出一个自映射来得到不错的重建误差，但这样的表示却没有什么用。比如如果是仿射编码那么可以学习出$Y = X$，如果是<code>sigmoid</code>那可以保证权重<code>W</code>的项足够小来保持在线性部分。</p>
<p>所以必须应用更多的准则来把有用信息从噪音里分离出来。传统的自编码器使用$d’&lt; d$来产生<code>欠完备</code>的表示。当使用这种方法以及一个线性仿射函数和均方根误差时，本质上是在做<code>主成分分析</code>[1989,第4篇文献]。当使用非线性函数、交叉熵损失函数以及权值绑定时，可以避免自编码器学习到的表示停留在<code>sigmoid</code>的线性部分。</p>
<p>当然可以想到的是使用另一种策略而不是低维度。使用<code>过完备</code>（$d’&gt;d$）而稀疏的表示的方法在之后得到了关注。因为首先大脑就是这样工作的[1996年稀疏编码的论文]；其次稠密的表示容易扰乱信息（原始信息稍有改变表示就会变化很多）而稀疏表示易于解释和被后面的分类器使用。这些方法已经被加入到传统的自编码器架构中来学习稀疏的表示。稀疏过完备表示可以被认为是数据的另一种压缩方式，通过大量的零而不是降低数据的维度来压缩。</p>
<h3 id="三、使用去噪准则"><a href="#三、使用去噪准则" class="headerlink" title="三、使用去噪准则"></a>三、使用去噪准则</h3><p>我们可以看到，仅仅使用重建准则并不能获得很好的表示，有时会简单的复制输入有时会无用的最大化互信息但得到我们不感兴趣的表示。虽然可以通过对表示增加一些限制解决这个问题。</p>
<p>这里我们使用一种不同的策略：清洁被部分污染的输入数据。我们把一个好的表示的定义改为如下：</p>
<blockquote>
<blockquote>
<p>一个好的表示要能够从被污染的输入中获取而且能够被用来重建干净的原始输入。</p>
</blockquote>
</blockquote>
<p>这有两个隐含意思：</p>
<ul>
<li><p>1.在被污染输入下高级表示应该更稳定和鲁棒。</p>
</li>
<li><p>2.通过去噪要能够抽取特征，这些特征包含了输入分布的有用结构。</p>
</li>
</ul>
<p>我们的目的不是去噪本身，而是通过去噪来建立一个训练准则学习如何找出有用的特征。</p>
<p><strong>3.1 去噪自编码器算法</strong></p>
<p>如下图所示是dA的原理：</p>
<p><img src="/images/FnQ1M5oqxpdiV2zSI4tKaLpbXmiI.jpg" alt=""></p>
<p>与自编码器的不同是原始输入$x$被通过映射$q_D$随机污染为$\tilde x$,$\tilde x \sim q_D(\tilde x|x)$。</p>
<p><strong>3.2 几何解释</strong></p>
<p>去噪的过程，可以给出一种直观的几何解释，也就是基于<code>流行假设</code>[Chapelle，2006年提出]的解释。它认为高维的自然数据集中靠近低维度的非线性流型。如下图所示：</p>
<p><img src="/images/Fi9NTwcuwSrh4J6Oke0M_uIicbfh.jpg" alt=""></p>
<p>在去噪过程中，我们学习到一个随机算子$p(X|\tilde X)$把污染数据映射到原始数据。由于污染数据比未被污染数据更可能出现在流型之外更远的距离。随机算子$p(X|\tilde X)$学习到一个映射，该映射倾向于从低概率点$\tilde X$到附近的高概率点$X$，在流型附近。当$\tilde X$远离流型时，随机算子需要学习迈更大的步子来到达流型。成功的去噪意味着算子把离流型很远的点映射到了流型附近的很小区域。</p>
<p>去噪自编码器可以看做是定义和学习出一个流型的方法。如果我们约束$d’ &lt; d$，那么$Y = f(X)$就可以被解释为流型上的点的坐标系统。</p>
<p><strong>3.3 污染的种类</strong></p>
<p>污染的过程中可以使用很多先验知识，但是在目前我们主要探究那些可以广泛应用到所有场景的技术，尤其是那些可以用在堆栈式去噪自编码器学习的技术。所以我们把讨论和实验条件限定在下面的简单污染过程：</p>
<ul>
<li><p>1.附加各向同性高斯噪声<code>GS</code>：$\tilde x|x \sim N(x,\sigma^2 I)$</p>
</li>
<li><p>2.掩码噪声<code>MN</code>：输入的一些位被置为0</p>
</li>
<li><p>3.<code>Salt-and-pepper</code>噪声<code>SP</code>：随机把输入的一段置为最大（最小）值，具体置为哪一种也是随机的</p>
</li>
</ul>
<p>GS通常在实际输入($x\in R^d$)被经常使用，而在二进制输入或者接近二进制输入（例如黑白图像）中通常使用MN。但实际我们在工作中关注掩码噪声，因为它可以看做是缺失了一些值，而去噪自编码器应该设法把这些“空白”填补上。</p>
<p>我们要提醒读者的是，MN和SP这两种方式，实际上污染了输入的一段而没有管其他的部分。去噪，就是利用其他的部分恢复被污染部分的过程，这个恢复过程只有高维分布中维度之间有相关性的时候才有用，否则去噪的过程没有意义。也就是如果问题的维度太低，就没有必要使用去噪这个方法了。</p>
<p><strong>3.4 扩展：被污染的维度上的着重措施</strong></p>
<p>因为MN和SP这两种方式实际上是污染了输入的一个变化的子集，所以我们可以直接的扩展一下它的去噪自编码器的训练准则。那就是，对于输入的污染部分和未污染部分，给予不同的权重矩阵，这里我们用$\alpha,\beta$来表示，它们被认为是模型的超参数，下面是这种措施下的均方误差损失函数和交叉熵损失函数：</p>
<script type="math/tex; mode=display">
L_{2,\alpha}(x,z)=\alpha (\sum_{j\in\tau(\tilde x)} (x_j - z_j)^2)+\beta (\sum_{j\notin\tau(\tilde x)} (x_j - z_j)^2)</script><script type="math/tex; mode=display">
L{2,\alpha}(x,z) = \alpha (-\sum{j\in\tau(\tilde x)} [x_jlogz_j + (1-x_j)log(1-z_j)])+\beta (-\sum_{j\notin\tau(\tilde x)} [x_jlogz_j + (1-x_j)log(1-z_j)])</script><p>我们把这个扩展叫做着重去噪自编码器。一个特例就是当$\alpha=1,\beta=0$的时候我们只把对被污染值的预测计入重建误差。</p>
<p><strong>3.5 利用堆栈式去噪自编码器建立深度结构</strong></p>
<p>利用<code>SdA</code>建立深度网络的方法与利用<code>RBM</code>建立<code>DBN</code>的方法以及利用原始自编码器建立深度网络的方式基本上相同。注意我们只在逐层预训练的时候对输入进行污染。而预训练时仅仅污染当前层接受的原始数据，数据仅在当前层进行一次污染过程。网络的最顶层可以加一层接受最高级表示的监督学习算法层，例如<code>SVM</code>或者<code>softmax</code>。然后利用梯度下降法对整个网络的参数进行微调。SdA建立的网络结构图如下：</p>
<p><img src="/images/FgkB9s0fjiCXKlbbAl43JfD361sC.jpg" alt=""></p>
<h3 id="四、文献中相关的方法"><a href="#四、文献中相关的方法" class="headerlink" title="四、文献中相关的方法"></a>四、文献中相关的方法</h3><p>本节我们主要介绍三个方向之前的相关工作。</p>
<p><strong>4.1 之前训练神经网络用于去噪的工作</strong></p>
<p>在去噪任务中利用<code>BP</code>算法训练多层感知机的方法最早在1987年就被<code>Lecun</code>提出了，1987年提出了一个相似的方法来学习自感知存储。这些之前的工作，无论是模型还是训练方法与本文提出的去噪自编码器都很类似。不同之处有两点，二进制输入数据在使用sigmoid函数后它们使用了均方根误差损失函数而我们使用了交叉熵损失函数；它们的去噪过程经过自编码器网络循环进行很多次，就像在一个自循环网络中一样。</p>
<p>尽管如此，我们的动机和目标很不一样，<code>Lecun</code>的目标是训练网络的记忆能力，也就是测试网络能够想起多少测试输入片段，这个工作也利用了一个非线性隐藏层。而我们的动机是探究和理解用来初始化深度网络的非监督预训练准则。因此我们的兴趣在于利用去噪来学习出好的特征提取器，然后组合这些特征提取器初始化深度网络。</p>
<p>另外一个与我们工作相关的是1998年Seung的工作，使用一个递归神经网络和BP训练得到被污染的数据。这项工作与我们的工作的主要不同是它：a）主要关注递归神经网络；b）关注图像去噪任务本身。所以该工作大量的使用到了图像拓扑学的先验知识，而我们的去噪过程是一个更加泛化的过程。</p>
<p>最近Jain和Seung在2008年提出了使用深度卷积神经网络<code>CNN</code>来进行图像去噪的成功方法。这个方法的性能超过了马尔科夫随机场和小波方法。</p>
<p><strong>4.2 利用含噪声输入训练分类器</strong></p>
<p>利用含噪声输入（或者叫抖动）来训练神经网络[1988年被提出]被证明可以提高监督学习任务的泛化性能[1991,1992,1996年的论文]。这方面的研究跟自编码器和去噪没有什么关联。</p>
<p><strong>4.3 伪似然度和依赖网络</strong></p>
<p>把去噪训练看作是“填补空白”以及着重在被污染区域训练这两个观点与1975年的伪相似度和2000年的依赖网络很相似。伪似然度提出把似然项$p(X)$替换为条件概率之积$\prod<em>{i=1}^d p(X_i|X</em>\neg i)$，其中$X<em>i$代表输入向量的第$i$维，而$X</em>\neg i$代表除了第$i$维之外的输入向量。依赖网络也同样考虑学习$d$个条件分布，每一个被用来使用输入的其余部分预测输入的第$i$位。这两种思想和<code>着重去噪自编码器</code>很像。</p>
<p>这段关于相关工作和本论文的相似处和区别部分没有仔细读。</p>
<h3 id="五、单个去噪自编码器的实验：定性的评估学习到的特征检测器"><a href="#五、单个去噪自编码器的实验：定性的评估学习到的特征检测器" class="headerlink" title="五、单个去噪自编码器的实验：定性的评估学习到的特征检测器"></a>五、单个去噪自编码器的实验：定性的评估学习到的特征检测器</h3><p>本节实验使用简单的dA，也就是没有堆栈式叠加或者监督式微调。实验的目的是测试对于不同的噪音种类，dA和经典的自编码器学习到的特征检测器有什么不同。</p>
<p>在图像数据上进行训练的第一个隐藏层的特征检测器的输出可以直接可视化表示。每一个神经元$y_j$由对应的权重向量$W_j$和输入向量叉乘产生，对应着图像的某一个部分。</p>
<p><strong>5.1 从自然图像中学到的特征检测器</strong></p>
<p>我们使用12*12的白噪声化自然图像训练经典自编码器和dA。对于这些自然输入，我们使用线性解码器和均方误差，参数进行随机初始化，使用SGD方法进行训练，学习率0.05，使用权值绑定（即使不使用学习到的权重矩阵也差不多是一样的）。</p>
<p>下图是训练结果，欠完备的自编码器看起来学习到了一些没什么用的局部对象检测器，而过完备的自编码器看起来完全是随机输出：</p>
<p><img src="/images/FvzQaS6sKU7kyGf9pdgxj1SvK3VX.jpg" alt=""></p>
<ul>
<li><p>左边：一些用来被训练的12*12图像</p>
</li>
<li><p>中间：被经典欠完备自编码器学习到的过滤器，使用50个隐藏单元，权值绑定和L2惩罚项</p>
</li>
<li><p>右边：被经典过完备自编码器学习到的过滤器，使用200个隐藏单元，L2惩罚项</p>
</li>
</ul>
<p>我们接下来训练了过完备的带L2权重衰减的经典自编码器（输入无噪音）和200个隐藏单元的去噪自编码器（输入带高斯噪声，无L2）。注意噪声等级是0的去噪自编码器就是一个正则化的经典自编码器，所以噪声等级接近0的dA结果应该和上图结果差不多。</p>
<p>如果有足够大的噪声等级（$\sigma = 0.5$），如下图，去噪自编码器就可以学习到<code>Gabor-like局部有向边检测器</code>，这和<code>稀疏编码</code>[1996]或者<code>ICA</code>[1997]得到的结果类似。但是带L2的欠完备的自编码器，尽管我们尝试了很多不同的正则化超参数的值，但只能学习到一些没用的大对象检测器。从实验中我们可以得出以下结论：</p>
<blockquote>
<blockquote>
<p>如4.2中所述，对于非线性自编码器来说，带足够多噪声输入的训练和带权重衰减项训练性质完全不同。</p>
</blockquote>
</blockquote>
<p><img src="/images/FiOUeLvmSFwJJaA4ELrEziFXiMTF.jpg" alt=""></p>
<p>下图考虑了另外两种噪声的类型，<code>SP</code>和<code>MN</code>。我们试验了三个噪声等级：10%，25%，55%。隐藏单元是100个，但是对于50或者200个结果也没有什么不同。</p>
<p><img src="/images/FolOOw8jEMUK2sugJzVzEjOLQ5dW.jpg" alt=""></p>
<p>我们发现，SP可以产生<code>Gabor-like局部有向边检测器</code>，而MN产生了该滤波器和<code>光栅滤波器</code>的混合体。确实不同的噪声类型可以产生不同的过滤器，但是我们发现这三种类型的噪声都可以帮助产生有用的边缘检测器。</p>
<p><strong>5.2 从手写数字中学到的特征检测器</strong></p>
<p>我们使用<code>Mnist</code>数据集（包含28*28的灰阶手写数字图像）来训练dA。当噪声等级为0%的时候，过滤器的输出看起来是随机的；增加噪声等级后，过滤器的输出看起来有用的多，比如局部相关<code>stroke</code>检测器和数字部分检测器。</p>
<p><img src="/images/FqAZwfFJ7slk4mfgcPteNJnm8Tz9.jpg" alt=""></p>
<h3 id="六、堆栈式去噪自编码器的实验"><a href="#六、堆栈式去噪自编码器的实验" class="headerlink" title="六、堆栈式去噪自编码器的实验"></a>六、堆栈式去噪自编码器的实验</h3><p>本节对比<code>SDAE</code>、<code>SAE</code>和<code>DBN</code>在分类问题上的基准测试性能。</p>
<p><strong>6.1 考虑分类问题和实验方法</strong></p>
<p>使用十个测试数据集进行测试。</p>
<p>采用下面的方式对参数进行初始化：</p>
<ul>
<li><p>1.MLP：随机</p>
</li>
<li><p>2.DBN：RBM预训练</p>
</li>
<li><p>3.SAE：自编码器预训练</p>
</li>
<li><p>4.SDAE：去噪自编码器预训练</p>
</li>
</ul>
<p>对于超参数，根据验证集性能进行调整。</p>
<p><strong>6.2 深度网络训练策略的经验比较</strong></p>
<p>实验结果，SDAE-3超过了作为基准的SVM和SAE-3（这里的3指三层隐藏层）。除了一个结果外，其他结果SDAE-3都超过了SAE-3和DBN。</p>
<p><strong>6.3 网络层数、隐藏层单元数量和污染等级的影响</strong></p>
<p>现在我们讨论一下一些重要的超参数设置对于性能的影响。这里我们选择数据集中最复杂参数最多的bg-img-rot，这个数据集是Mnist手写数字数据集的图像进行随机旋转和加背景图片得来的。网络层数、隐藏层单元数量对于三个模型的影响如下图：</p>
<p><img src="/images/FkgOBra5wx92nRD1z06-eNZ0gBdz.jpg" alt=""></p>
<p>网络层数、污染等级对于SDAE的影响如下图：</p>
<p><img src="/images/FvxxjnjZnc8vClnGyWawr0I972hY.jpg" alt=""></p>
<p><strong>6.4 去噪预训练 VS 含噪声输入训练</strong></p>
<p>SDAE使用去噪训练准则来学习初始的特征提取器然后使用无噪声输入进行监督式学习，这和进行含噪声输入训练是非常不一样的。你可以只在预训练部分添加噪声，也可以在微调部分也添加噪声。我们使用三个数据集进行测试，两种方法对于SDAE的影响如下图：</p>
<p><img src="/images/Ftyn4FnHkh_cwZWsZxSKG3GpgVHn.jpg" alt=""></p>
<p>我们可以看到，使用含噪声输入训练对SDAE并没有明显的性能增益（相比于SAE），有时还会下降。</p>
<p><strong>6.5 去噪自编码器的变种：使用交替污染种类和着重训练</strong></p>
<p>本系列实验，我们准备探究交替污染种类和进行被污染区域着重训练的效果。实验结果说明：</p>
<ul>
<li><p>1.明智的根据数据选择污染种类和进行着重训练有利于提高训练结果</p>
</li>
<li><p>2.bg-rand数据集中，SDAE-3性能始终不如DBN-3，给出的解释是bg-rand问题中的数据非常适合于RBM学习它的表示[2007年Larochelle的论文]。</p>
</li>
</ul>
<p><strong>6.6 SDAE非监督式学习到的特征对于SVM有用吗</strong></p>
<p>接下来的实验中，我们希望确定学习到的高级表示对于不是神经网络中的算法是否有用，比如<code>SVM</code>。</p>
<p>我们把SDAE学习到的表示提供给<code>线性SVM</code>和<code>核SVM</code>（使用RBF核），实验结果显示，学习到的表示能够提高SVM的性能。（为了证明是SDAE提高了性能而不是随机的非线性变换，把同一个神经网络随机初始化参数得出的表示传递给SVM，表现要低得多）而且越高级的表示，SVM的性能就越好。线性SVM，显然能够从我们非线性的表示中获益；但让我们吃惊的是，使用RBF核的SVM同样可以从SDAE学习出的非线性映射中得到提高。</p>
<p><img src="/images/Fo8dJ4ce3eoWBQif23FJSnsisfsZ.jpg" alt=""></p>
<h3 id="七、从SdA网络中生成样本"><a href="#七、从SdA网络中生成样本" class="headerlink" title="七、从SdA网络中生成样本"></a>七、从SdA网络中生成样本</h3><p><strong>7.1 给予一个顶层表示自上而下生成可见样本</strong></p>
<p>给予一个顶层表示，那么DBN就是一个图模型可以自上而下的取样，也就是从基于上一层在本层取样，直到最下面的一层。在一个simoid深度置信网络中，有$X|Y \sim B(g<em>{\theta’} (Y))$，而$g</em>{\theta ‘}$和自编码器中的是相同的形式，因此SAE和SDAE也可以用相似的方式来生成。</p>
<p><strong>7.2 对于给定输入自下而上推导顶层表示</strong></p>
<p>在SAE/SDAE架构中，最底层给定一个输入表示，可以通过确定的自下而上的过程给出相应得的高级表示。在DBN中相同的过程在图模型的角度可以看做是给定底层输入对因子的顶层伯努利分布进行近似推理，顶层表示可以理解为真实输入的因子伯努利分布的参数。</p>
<p><strong>7.3 对于SAE，SDAE，DBN使用同样过程生成样本</strong></p>
<p><img src="/images/FirpluS_Eu5C2qsm5qkJeN3oqduq.jpg" alt=""></p>
<p>通过对比生成的样本可以看出三种模型学习到的表示优劣。</p>
<h3 id="八、总结和未来工作"><a href="#八、总结和未来工作" class="headerlink" title="八、总结和未来工作"></a>八、总结和未来工作</h3><p>我们的工作受最近训练深度网络的方法的启发。动机是弥补使用SAE网络与DBN之间的性能差距。这让我们去探究传统的自编码器的理论缺陷。经过改进训练准则，SDAE生成的表示能够提高浅层分类器比如<code>Kernel SVM</code>的效果。对于特征抽取器的仔细试验表明，去噪自编码器可以学习到数据中有用的结构（比如自然图像中的<code>Gabor-like</code>边缘检测器）而正则化自编码器学习不到。</p>
<p>我们基于易理解的容易实现的传统编码器改进算法。而且仅仅使用简单直接的噪声类型和噪声等级的微调就可以得到不错的结果。我们的去噪过程和权值衰减并不相同，也和监督学习中加入含噪声输入不一样。</p>
<blockquote>
<blockquote>
<p>我们的实验表明，使用去噪训练准则作为非监督学习的目标能够学习出有用的高级表示。这是我们工作中最重要的贡献。</p>
</blockquote>
</blockquote>
<p>希望我们的工作能够使更多的人研究这个方向。理论上的（研究去噪过程和表示学习之间的关系）以及实践中的（基于理论设计更好的算法）。</p>
<p>肯定有比我们这种简单的局部训练更好的使用去噪训练方式的方法。尤其是，虽然SDAE可以帮助建立深度网络，但是去噪自编码器本身确实浅层的模型，我们只是把它们叠加起来了。探索深度去噪自编码器并把它们叠加在一起建立高级表示一定很有意思。污染过程的类型选择和其所扮演的角色一样值得研究。如果更多的污染类型被证明效果不错，它们可以被根据数据类型参数化叠加，从而污染类型不需要手动的进行选择，而是从数据中自动生成。</p>
<h3 id="我的总结"><a href="#我的总结" class="headerlink" title="我的总结"></a>我的总结</h3><p>该篇论文是去噪自编码器的原始论文，论文让人印象深刻的地方有：</p>
<ul>
<li>1.论文中对于去噪自编码器从<code>信息论</code>和<code>流形学习</code>的角度解释很特别（虽然这两个角度来自自编码器论文）。</li>
<li>2.论文的实验部分记录详尽完善、数据充分、不同模型对比结果让人信服。</li>
<li>3.相关工作部分从多个方面阐述了与自己工作相关的论文，对两者相同和不同之处进行说明。</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>阅读笔记</tag>
        <tag>自编码器</tag>
      </tags>
  </entry>
  <entry>
    <title>ML implementations for common methods</title>
    <url>/2017/08/09/ml-coding-summarize/ml%20implementations%20for%20common%20methods/</url>
    <content><![CDATA[<h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h3><p>1.选择K个中心点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_centroids</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    N = X.shape[<span class="number">0</span>]</span><br><span class="line">    rand_indices = np.random.permutation(N)[:K]</span><br><span class="line">    centroids = X[rand_indices, :]</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></figure>
<p>2.计算数据点离中心的距离，并归类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findClosestCentroids</span><span class="params">(X, centroids)</span>:</span></span><br><span class="line">    N = X.shape[<span class="number">0</span>]</span><br><span class="line">    K = centroids.shape[<span class="number">0</span>] </span><br><span class="line">    dis = np.zeros((m,K))</span><br><span class="line">    idx = np.zeros((m,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(K):</span><br><span class="line">            dis[i,j] = np.dot((X[i,:] - centroids[j,:]).reshape(<span class="number">1</span>,<span class="number">-1</span>),(X[i,:] - centroids[j,:]).reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">   </span><br><span class="line">    _, idx = np.where(dis == np.min(dis, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> idx[<span class="number">0</span>: dis.shape[<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<p>3.重新计算中心点</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_centroids</span><span class="params">(X, K, idx)</span>:</span></span><br><span class="line">	d = X.shape[<span class="number">1</span>]</span><br><span class="line">    centroids = np.zeros((K,d))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">        centroids[i, :] = np.mean(X[np.ravel(idx==i), :], axis=<span class="number">0</span>).reshape(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></figure>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><h3 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h3><h4 id="拒绝采样"><a href="#拒绝采样" class="headerlink" title="拒绝采样"></a>拒绝采样</h4><h4 id="概率分布采样"><a href="#概率分布采样" class="headerlink" title="概率分布采样"></a>概率分布采样</h4><h4 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h4><h4 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h4>]]></content>
      <categories>
        <category>编程总结</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>算法实现</tag>
      </tags>
  </entry>
  <entry>
    <title>as-reader在tensorflow和keras下改写总结</title>
    <url>/2017/04/08/ml-coding-summarize/as-reader%E5%9C%A8tensorflow%E5%92%8Ckeras%E4%B8%8B%E6%94%B9%E5%86%99%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h4 id="1、搬了什么砖，在什么条件下搬的砖"><a href="#1、搬了什么砖，在什么条件下搬的砖" class="headerlink" title="1、搬了什么砖，在什么条件下搬的砖"></a>1、搬了什么砖，在什么条件下搬的砖</h4><p>在tensorflow和keras上复现了ACL2016的一篇论文《Text Understanding with the Attention Sum Reader Network》里的模型attention-sum-reader。能够跑CBT的数据。</p>
<p>论文可以在<a href="http://arxiv.org/abs/1603.01547" target="_blank" rel="noopener">http://arxiv.org/abs/1603.01547</a>下载，具体代码见这里<a href="https://github.com/zhanghaoyu1993/attention-sum-reader" target="_blank" rel="noopener">https://github.com/zhanghaoyu1993/attention-sum-reader</a>。</p>
<p>搬砖环境：</p>
<ul>
<li>windows10</li>
<li>python-3.5.2</li>
<li>tensorflow-1.0.1</li>
<li>keras-2.0.2</li>
</ul>
<h4 id="2、遇到了哪些坑，怎么踩过-进-去的"><a href="#2、遇到了哪些坑，怎么踩过-进-去的" class="headerlink" title="2、遇到了哪些坑，怎么踩过(进)去的"></a>2、遇到了哪些坑，怎么踩过(<del>进</del>)去的</h4><p>因为原作者是在theano上的blocks和fuel两个框架下实现的，所以算是改写吧，写的过程中在这么几个点上花的时间比较多：</p>
<ul>
<li>1.结合keras和tf，简单标准化的层用keras提供的，特殊的操作使用K.*这种API结合Lambda层，再特殊的操作使用tf.*这种API完成。</li>
<li>2.tensorflow的scan函数，如果你想对tensor进行一些迭代操作而库函数里没有直接提供的话，就要使用scan函数了，这个函数在tensorflow和theano下面都有。</li>
</ul>
<p>tf.scan有下面几个主要参数，依靠他们来完成对一个或者多个tensor的迭代：</p>
<p>fn：一个函数接受两个参数表示上一轮的输出和这一轮的输入，注意！！：这个函数的输出只需要提供这一轮的就可以了，没有必要和上一轮的拼起来，scan会自己帮你concat。</p>
<p>elems：一个tensor或者tensor列表，在scan的过程中，会把elems（或者其中每个元素）取一行提供给fn作为第二个参数；</p>
<p>initializer：初始化，如果不提供那么fn返回的元素结构要和elems一样，否则就返回你提供的这个数据的结构。注意！！：你提供的这个数无关紧要，因为最后并不会返回。</p>
<p>还有一点需要注意的是tf的tf.scan()并没有提供non-sequences这个参数，这个参数里的tensor在scan的过程中直接提供给fn（而不是按照第一阶解包），但是通过全局变量，或者python的partial function（在多层scan中）还是能够实现相同的功能：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">some_func</span><span class="params">(tensor1, tensor2, tensor3)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> tf.concat(tensor1 * tensor2, tensor3, axis=<span class="number">-1</span>)</span><br><span class="line">i2 = tf.Variable([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">i3 = tf.Variable([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">tf.scan(fn=<span class="keyword">lambda</span> prev_output, cur: some_func(cur,i2,i3))</span><br></pre></td></tr></table></figure>
<ul>
<li>3.在使用RNN的时候如何处理变长的序列？</li>
</ul>
<p>首先当然是要pad_sequences，因为无论如何keras的层接受的输入都是定长的。（也可能有我不知道的）</p>
<p>如果使用tf，那么可以使用dynamic_rnn()，把序列的实际长度列表作为输入参数，只会unroll实际的步长，对于多余的，返回的output补0，能够减少浪费在pad字符上的处理时间。这里<strong>需要注意</strong>的是，使用了这个方法后，如果你对于output只是需要部分time step的，那一定要想清楚是不是有效的time step，必要的时候用tensor的操作进行选取。另外这个操作只能返回最后时刻的状态，中间的RNN隐藏状态无法获得。</p>
<p>如果是keras的层，就只有调API了，不知道内部是不是调用dynamic_rnn。</p>
<ul>
<li>4.在搭建模型的过程中，怎么确定自己写的代码没有小错误呢？</li>
</ul>
<p>读取和预处理数据的过程中，数据还是numpy array格式的时候，可以依靠Debug和输出确定。</p>
<p>在keras层的模型之间，可以搭一层测试一层的输出，使用如下的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestModel</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		q_input = Input(batch_shape=(<span class="literal">None</span>, self.q_len), dtype=<span class="string">"int32"</span>, name=<span class="string">"q_input"</span>)</span><br><span class="line">        q_encode = Embedding(input_dim=self.vocab_size,</span><br><span class="line">                             output_dim=FLAGS.embedding_dim,</span><br><span class="line">                             weights=[embedding_matrix],</span><br><span class="line">                             mask_zero=<span class="literal">True</span>)(q_input)</span><br><span class="line">        self.q_embed = Model(inputs=[q_input], outputs=q_encode)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># compile model</span></span><br><span class="line">        print(self.q_embed.predict(x))</span><br></pre></td></tr></table></figure>
<p>在搭建tf的模型时，可以采用类似的方法，用notebook（比较方便）进行测试，把中间每层的tensor输出一下，看看长得是不是有点奇怪。</p>
<ul>
<li>5.keras和tf的区别？</li>
</ul>
<p>keras很方便快捷，而且可以基本和tf无缝衔接，不过使用keras搭建模型的时候好像内存用的比tf多一些，keras搭建的模型在tensorflow-gpu上运行老是报OOM，但是使用tf搭建就没这个问题。（可能是有哪个用法不知道吧）</p>
<ul>
<li>6.tf怎么使用crossEntropy交叉熵损失函数？</li>
</ul>
<p>tf自带的好像只有softmax_cross_entropy这样的交叉熵损失函数，没有只做交叉熵的，下面的代码片段可以实现tf下的交叉熵，并保持数值稳定性防止NaN错误（摘自keras）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_EPSILON = <span class="number">10e-8</span></span><br><span class="line">epsilon = tf.convert_to_tensor(_EPSILON, output.dtype.base_dtype, name=<span class="string">"epsilon"</span>)</span><br><span class="line">output = tf.clip_by_value(output, epsilon, <span class="number">1.</span> - epsilon)</span><br><span class="line">self.loss = tf.reduce_mean(- tf.reduce_sum(self.y_true * tf.log(output),</span><br><span class="line">                           reduction_indices=len(output.get_shape()) - <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>7.tf怎么实现梯度裁剪？</li>
</ul>
<p>如下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grad_vars = optimizer.compute_gradients(self.loss)</span><br><span class="line">grad_vars = [</span><br><span class="line">    (tf.clip_by_norm(grad, grad_clip), var)</span><br><span class="line">    <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> (grad, var)</span><br><span class="line">    <span class="keyword">for</span> grad, var <span class="keyword">in</span> grad_vars]</span><br><span class="line">train_op = optimizer.apply_gradients(grad_vars)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>编程总结</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>tensorflow</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP implementations for common methods</title>
    <url>/2018/03/09/ml-coding-summarize/nlp%20implementations%20for%20common%20methods/</url>
    <content><![CDATA[<p>Hi。</p>
]]></content>
      <categories>
        <category>编程总结</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>算法实现</tag>
      </tags>
  </entry>
  <entry>
    <title>稀疏图计算</title>
    <url>/2020/01/09/ml-coding-summarize/%E7%A8%80%E7%96%8F%E5%9B%BE%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h4 id="稀疏图计算的实现"><a href="#稀疏图计算的实现" class="headerlink" title="稀疏图计算的实现"></a>稀疏图计算的实现</h4><p>遇到图中元素很稀疏时可以使用sparse tensor计算所需要的值，然后再转化为dense tensor。假如涉及图的运算中稀疏程度很大、或者中间结果的维度很高，都能够有效的降低时间跟内存开销。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 选择所有非0元素的索引</span></span><br><span class="line">indices = torch.nonzero(x)  </span><br><span class="line"><span class="comment"># 从参与计算的tensor中取值</span></span><br><span class="line">values = x[tuple(indices[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(indices.shape[<span class="number">0</span>]))]</span><br><span class="line"><span class="comment"># 进行需要的索引变换</span></span><br><span class="line">j_indices = indices.clone()</span><br><span class="line">value_indices = list(j_indices[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(j_indices.shape[<span class="number">0</span>]))</span><br><span class="line">value_indices[<span class="number">1</span>] = value_indices[<span class="number">1</span>].zero_()</span><br><span class="line"><span class="comment"># 从某个参与计算的tensor中取相应的值</span></span><br><span class="line">emb_j = node_j[tuple(value_indices)]</span><br><span class="line"><span class="comment"># 进行计算</span></span><br><span class="line">final_values = func(values, emb_j)</span><br><span class="line"><span class="comment"># 最后可能需要再次对索引进行变换（比如加上一维）</span></span><br><span class="line">extra_indices = torch.arange(<span class="number">0</span>, window_size).to(indices.device).unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_num).t().reshape(</span><br><span class="line">            (<span class="number">1</span>, n_num, window_size))</span><br><span class="line">indices = torch.cat([indices[<span class="number">0</span>:<span class="number">1</span>], extra_indices, indices[<span class="number">1</span>:]], dim=<span class="number">0</span>)</span><br><span class="line">indices = indices.reshape((<span class="number">4</span>, <span class="number">-1</span>))</span><br><span class="line"><span class="comment"># 最后生成sparse tensor，再转换回来</span></span><br><span class="line">x_typename = torch.typename(x).split(<span class="string">'.'</span>)[<span class="number">-1</span>]</span><br><span class="line">sparse_tensortype = getattr(torch.sparse, x_typename)</span><br><span class="line">res = sparse_tensortype(indices, final_values, (b, window_size, l, l, <span class="number">1</span>)).requires_grad_(<span class="literal">True</span>).to_dense()</span><br></pre></td></tr></table></figure>
<p>另外，中间尝试了把几个计算涉及的tensor分别转为sparse tensor然后运算最后再转回来，但是在backward的时候会出错，用<code>tensor.contiguous()</code>也没解决。</p>
]]></content>
      <categories>
        <category>编程总结</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>GNN</tag>
        <tag>编程总结</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>ORDERED NEURONS： INTEGRATING TREE STRUCTURES INTO RECURRENT NEURAL NETWORKS</title>
    <url>/2019/10/07/nlp/ORDERED-NEURONS-INTEGRATING-TREE-STRUCTURES-INTO-RECURRENT-NEURAL-NETWORKS/</url>
    <content><![CDATA[<h3 id="Recent-Language-Model-Papers"><a href="#Recent-Language-Model-Papers" class="headerlink" title="Recent Language Model Papers"></a>Recent Language Model Papers</h3><h3 id="On-LSTM"><a href="#On-LSTM" class="headerlink" title="On-LSTM"></a>On-LSTM</h3><p>ICLR’19的Best Paper， By YiKang。</p>
<h4 id="1-解决的问题"><a href="#1-解决的问题" class="headerlink" title="1.解决的问题"></a>1.解决的问题</h4><p>RNN把序列顺序的处理，但是语言本身常常是具有非序列化结构的（比如树结构）。这样可以获取语言组合性语义的影响，学习到层次化的表示。首先，无监督的Grammer Induction还是一个open problem，通常训练的模型倾向于产生trivial的结构（左分支/右分支树）或者在利用RL学习branching策略的时候有困难。其次LSTM本质上适合建模链式结构的数据，虽然已有的研究表明它能够学习到语言中的树形结构，但是显式的引入一个带有树形结构的inductive bias会不会帮助LSTM更好的建模呢？</p>
<h4 id="2-相关的方法"><a href="#2-相关的方法" class="headerlink" title="2.相关的方法"></a>2.相关的方法</h4><p>1.一些工作尝试LSTM中引入结构化信息，但是没有解决如何从观测数据中推导结构化信息的问题；2.也有一些工作针对1中的问题，即grammar induction；3.在recurrent结构模型中引入不同scale的recurrent结构从而让链式的先验具有层次化。（比如ClockWorkRNN和nested dropout，但是本文提出的方法更灵活更具有泛化能力）</p>
<h4 id="3-提出的方法（Ordered-Neurons）思路"><a href="#3-提出的方法（Ordered-Neurons）思路" class="headerlink" title="3. 提出的方法（Ordered Neurons）思路"></a>3. 提出的方法（Ordered Neurons）思路</h4><p>预先在模型中定义神经元的序，高阶神经元更新的更久代表长程/全局信息，低阶神经元更新步数更少代表较小的constituent，当高阶神经元的信息在某个step被抹去其对应的子神经元的信息也应当被抹去（对应一个constituent结束）。神经元的序被如下定义，由parse得到的成分句法树决定。</p>
<p><img src="/images/image-20200604170224175.png" alt=""></p>
<h4 id="4-具体实现（ON-LSTM）"><a href="#4-具体实现（ON-LSTM）" class="headerlink" title="4.具体实现（ON-LSTM）"></a>4.具体实现（ON-LSTM）</h4><p>LSTM中利用三个门<script type="math/tex">i_t,g_t,o_t</script>来进行输入遗忘和输出，而每个时间步具有Cell State  <script type="math/tex">c_t</script>和输出向量<script type="math/tex">h_t</script>，而<script type="math/tex">c_t</script>中低维的部分代表低层级信息高维的部分代表高层级信息，经典LSTM中<script type="math/tex">c_t = f_t\odot c_{t-1} + f_i\odot \hat c_t</script>。而这篇工作希望在更新的时候能够计算得到当前信息代表的信息层级$d_i$和历史信息代表的信息层级<script type="math/tex">f_i</script>，而后对于不同的维度使用不同的信息更新策略。作者为了这个目标，增加了主遗忘门和主输入门<script type="math/tex">\tilde f_t</script>和<script type="math/tex">\tilde i_t</script>，并进行了软分类，如下公式。其中前者是单调递增的，表示低阶信息应当受历史信息影响较小；后者是单调递减的，其隐含意义是高阶信息应当受当前输入影响较小（更新较慢）。</p>
<p><img src="/images/image-20200604170249303.png" alt=""></p>
<p>这样的设计，保证了高层信息可能保留相当长的距离（因为高层直接复制历史信息，导致历史信息可能不断被复制而不改变），而低层信息在每一步输入时都可能被更新（因为低层直接复制输入，而输入是不断改变的），所以就通过信息分级来嵌入了层级结构。**更通俗地说就是分组更新，更高的组信息传得更远（跨度更大），更低的组跨度更小，这些不同的跨度就形成了输入序列的层级结构。</p>
<p>具体的理解这个blog讲的很棒：<a href="https://www.linkresearcher.com/theses/54a0ba37-b625-4200-9065-5120b94933b1" target="_blank" rel="noopener">https://www.linkresearcher.com/theses/54a0ba37-b625-4200-9065-5120b94933b1</a></p>
<p>处理维度不同：针对树形结构层级和LSTM隐层神经元数量的不同，作者将层级维度重复q次得到主遗忘门和主输入门。</p>
<h4 id="5-实验"><a href="#5-实验" class="headerlink" title="5.实验"></a>5.实验</h4><p>1.在语言模型任务上和一些LSTM变种进行了比对。2.无监督的句法成分分析，从作为语言模型预训练的模型中利用主遗忘门找到每个序列元素代表的层级，并利用贪心算法自顶向下的构建成分分析树。3.此外还在句法评价（2018）和逻辑推断任务上进行了实验。</p>
<p>Future：Bi-direction Context；CNN or Attention + Syntactic Structure；modeling syntactic role as well as syntatic structure。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>RNN</tag>
        <tag>Best Paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow的RNN和Attention实现过程</title>
    <url>/2017/06/05/ml-coding-summarize/Tensorflow%E7%9A%84RNN%E5%92%8CAttention%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<p>今天就来看一看不同种类的RNN和Attention在Tensorflow中到底是怎么实现的。</p>
<h4 id="1、从-RNNCell到LSTM"><a href="#1、从-RNNCell到LSTM" class="headerlink" title="1、从_RNNCell到LSTM"></a>1、从_RNNCell到LSTM</h4><p>任何Recurrent Neural Network都必须有一个或者多个cell，而这些cell的公共父类就是<em>RNNCell，一个抽象类。拥有`<em>_call</em></em>()`方法，每次调用接受一个input（<script type="math/tex">BatchSize \times input\_size</script>）和一个state，输出一个output和new state的元组。</p>
<ul>
<li>1.BasicRNNCell，也就是经典的RNN，其调用的时候输出和状态的计算公式是：output = new_state = act(W <em> input + U </em> state + B)，其内部调用了_linear()函数。</li>
<li>2._linear()函数，接受输入，并将输入与参数矩阵W相乘，加上偏置b，并返回。</li>
<li>3.BasicLSTMCell，也就是LSTM，其调用函数：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">  <span class="string">"""Long short-term memory cell (LSTM)."""</span></span><br><span class="line">  <span class="keyword">with</span> _checked_scope(self, scope <span class="keyword">or</span> <span class="string">"basic_lstm_cell"</span>, reuse=self._reuse):</span><br><span class="line">    <span class="comment"># Parameters of gates are concatenated into one multiply for efficiency.</span></span><br><span class="line">    <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line">      <span class="comment"># 一般都走这个分支，取出c_t和h_t</span></span><br><span class="line">      c, h = state</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      c, h = array_ops.split(value=state, num_or_size_splits=<span class="number">2</span>, axis=<span class="number">1</span>)</span><br><span class="line">  <span class="comment"># 参考了《Recurrent Neural Network Regularization》，一次计算四个gate  </span></span><br><span class="line">  concat = _linear([inputs, h], <span class="number">4</span> * self._num_units, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># i = input_gate, j = new_input, f = forget_gate, o = output_gate</span></span><br><span class="line">    i, j, f, o = array_ops.split(value=concat, num_or_size_splits=<span class="number">4</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) *</span><br><span class="line">             self._activation(j))</span><br><span class="line">    new_h = self._activation(new_c) * sigmoid(o)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line">      new_state = LSTMStateTuple(new_c, new_h)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      new_state = array_ops.concat([new_c, new_h], <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 注意这里返回的输出是h_t,而state是(c,h)</span></span><br><span class="line">    <span class="keyword">return</span> new_h, new_state</span><br></pre></td></tr></table></figure>
<p>和下图中论文中公式是完全对应的：</p>
<p>这个公式总的看来就是：<script type="math/tex">h_t = G(h_{t-1},x_t,c_t)</script></p>
<p><img src="/images/FoBgWjwy9u9GgN0z9nY1nclZMEL_.jpg" alt=""></p>
<ul>
<li>4.GRUCell，参考了2014年EMNLP论文《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》中的实现，论文中公式如下（参数简化表示了）：</li>
</ul>
<p>下面的公式总的看来就是：<script type="math/tex">h_t=G(h_{t-1},x_t)</script></p>
<script type="math/tex; mode=display">
r = \sigma (Wx_t + Uh_{t-1}) \\
z = \sigma (Wx_t + Uh_{t-1}) \\
h_t = zh_{t-1} + (1-z)\tilde h_t \\
\tilde h_t = \phi(Wx+U(r\odot h_{t-1}))</script><p>而代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">  <span class="string">"""Gated recurrent unit (GRU) with nunits cells."""</span></span><br><span class="line">  <span class="keyword">with</span> _checked_scope(self, scope <span class="keyword">or</span> <span class="string">"gru_cell"</span>, reuse=self._reuse):</span><br><span class="line">    <span class="keyword">with</span> vs.variable_scope(<span class="string">"gates"</span>):  <span class="comment"># Reset gate and update gate.</span></span><br><span class="line">      <span class="comment"># We start with bias of 1.0 to not reset and not update.</span></span><br><span class="line">      <span class="comment"># 一次计算出两个gate的值</span></span><br><span class="line">      value = sigmoid(_linear(</span><br><span class="line">        [inputs, state], <span class="number">2</span> * self._num_units, <span class="literal">True</span>, <span class="number">1.0</span>))</span><br><span class="line">      <span class="comment"># 这里的u就是上面的z</span></span><br><span class="line">      r, u = array_ops.split(</span><br><span class="line">          value=value,</span><br><span class="line">          num_or_size_splits=<span class="number">2</span>,</span><br><span class="line">          axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">with</span> vs.variable_scope(<span class="string">"candidate"</span>):</span><br><span class="line">      c = self._activation(_linear([inputs, r * state],</span><br><span class="line">                                   self._num_units, <span class="literal">True</span>))</span><br><span class="line">    new_h = u * state + (<span class="number">1</span> - u) * c</span><br><span class="line">  <span class="comment"># GRU里面输出和state都是一个h</span></span><br><span class="line">  <span class="keyword">return</span> new_h, new_h</span><br></pre></td></tr></table></figure>
<p>此外，还有支持peephole和projection的LSTMCell。也是<code>__call__()</code>方法不一样。</p>
<h4 id="2、Cell的Wrapper"><a href="#2、Cell的Wrapper" class="headerlink" title="2、Cell的Wrapper"></a>2、Cell的Wrapper</h4><p>包括inputProjectionWrapper，outputProjectionWrapper在内的一些用于映射输入输出的类，往往没有直接在外面用tf的操作快。</p>
<p>DropoutWrapper，将cell作为属性，并实现call方法，在调用cell前后进行dropout，支持对于输入，state和输出进行dropout。</p>
<p>ResidualWrapper，就是把输入concat到输出上一起返回。</p>
<p>DeviceWrapper，确保这个cell在指定的设备上运行（2333）。</p>
<p>MultiRNNCell，这个也算是一个wrapper，因为可以拥有cell的数组作为属性，用于实现多层RNN。</p>
<p>AttentionCellWrapper，参照了《Neural Machine Translation by Jointly Learning to Align and Translate》的实现，也就是Bahdanau风格的实现，公式如下，其中y是t时刻的输入（在这篇文章中同时也是t-1时刻的输出），s是隐藏状态，c通过和encoder的隐藏状态进行相似度计算、归一化、加权求和得到：</p>
<p><img src="/images/FmtCw1pVS6xQsFSBY2gLoycoIehK.jpg" alt=""></p>
<script type="math/tex; mode=display">
s_i=(1-z_i)\circ s_{i-1}+z_i\circ \tilde s_i \\
\tilde s_i = tanh(W\times e(y_{i-1}) +U[r_i\circ s_{i-1}]) \\
z_i = \sigma (f(y_{i-1},s_{i-1},c_i)) \\
r_i = \sigma (f(y_{i-1},s_{i-1},c_i))</script><p>这篇文章中，相似度对比函数a是通过一个前馈神经网络实现的。</p>
<p><img src="/images/FlYb2kCqxEHD-ac2z7JxnV7pFVdU.jpg" alt=""></p>
<script type="math/tex; mode=display">
e_{ij}=a(s_{i-1},h_j)=Vtanh(g(s_{i-1},h_j))</script><p>接下来是TF的代码部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">    <span class="string">"""Long short-term memory cell with attention (LSTMA)."""</span></span><br><span class="line">    <span class="comment"># state \in R^&#123;B\times T&#125;</span></span><br><span class="line">    <span class="keyword">with</span> _checked_scope(self, scope <span class="keyword">or</span> <span class="string">"attention_cell_wrapper"</span>,</span><br><span class="line">                        reuse=self._reuse):</span><br><span class="line">      <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line">        <span class="comment"># 这里把state分为三个部分，LSTM的state，attns（代表attention向量）和attn的state</span></span><br><span class="line">        state, attns, attn_states = state</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不是元组，就按照长度切分</span></span><br><span class="line">        states = state</span><br><span class="line">        state = array_ops.slice(states, [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">-1</span>, self._cell.state_size])</span><br><span class="line">        attns = array_ops.slice(</span><br><span class="line">            states, [<span class="number">0</span>, self._cell.state_size], [<span class="number">-1</span>, self._attn_size])</span><br><span class="line">        attn_states = array_ops.slice(</span><br><span class="line">            states, [<span class="number">0</span>, self._cell.state_size + self._attn_size],</span><br><span class="line">            [<span class="number">-1</span>, self._attn_size * self._attn_length])</span><br><span class="line">      <span class="comment"># attention状态是[None x Attention向量长度 x Attention窗口长度]</span></span><br><span class="line">      attn_states = array_ops.reshape(attn_states,</span><br><span class="line">                                      [<span class="number">-1</span>, self._attn_length, self._attn_size])</span><br><span class="line">      input_size = self._input_size</span><br><span class="line">      <span class="keyword">if</span> input_size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        input_size = inputs.get_shape().as_list()[<span class="number">1</span>]</span><br><span class="line">      <span class="comment"># 让input和attns进行一个什么运算呢？</span></span><br><span class="line">      inputs = _linear([inputs, attns], input_size, <span class="literal">True</span>)</span><br><span class="line">      lstm_output, new_state = self._cell(inputs, state)</span><br><span class="line">      <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line">        new_state_cat = array_ops.concat(nest.flatten(new_state), <span class="number">1</span>)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        new_state_cat = new_state</span><br><span class="line">      <span class="comment"># 利用attention机制计算出下一时刻需要的上下文向量c_t和attention状态（隐藏状态）h_j</span></span><br><span class="line">      new_attns, new_attn_states = self._attention(new_state_cat, attn_states)</span><br><span class="line">      <span class="keyword">with</span> vs.variable_scope(<span class="string">"attn_output_projection"</span>):</span><br><span class="line">        <span class="comment"># 利用c_t和x_t(y_&#123;t-1&#125;)计算出t时刻输出s_t</span></span><br><span class="line">        output = _linear([lstm_output, new_attns], self._attn_size, <span class="literal">True</span>)</span><br><span class="line">      <span class="comment"># 把当前时刻输出s_t增加到下一时刻attention状态去</span></span><br><span class="line">      new_attn_states = array_ops.concat(</span><br><span class="line">          [new_attn_states, array_ops.expand_dims(output, <span class="number">1</span>)], <span class="number">1</span>)</span><br><span class="line">      new_attn_states = array_ops.reshape(</span><br><span class="line">          new_attn_states, [<span class="number">-1</span>, self._attn_length * self._attn_size])</span><br><span class="line">      new_state = (new_state, new_attns, new_attn_states)</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> self._state_is_tuple:</span><br><span class="line">        new_state = array_ops.concat(list(new_state), <span class="number">1</span>)</span><br><span class="line">      <span class="comment"># 最后返回s_t和h，注意这里的h就是s_t，所以这个AttentionWrapper应用范围有限，有些情况下不能用，需要自己修改定制</span></span><br><span class="line">      <span class="keyword">return</span> output, new_state</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_attention</span><span class="params">(self, query, attn_states)</span>:</span></span><br><span class="line">    conv2d = nn_ops.conv2d</span><br><span class="line">    reduce_sum = math_ops.reduce_sum</span><br><span class="line">    softmax = nn_ops.softmax</span><br><span class="line">    tanh = math_ops.tanh</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> vs.variable_scope(<span class="string">"attention"</span>):</span><br><span class="line">      k = vs.get_variable(</span><br><span class="line">          <span class="string">"attn_w"</span>, [<span class="number">1</span>, <span class="number">1</span>, self._attn_size, self._attn_vec_size])</span><br><span class="line">      v = vs.get_variable(<span class="string">"attn_v"</span>, [self._attn_vec_size])</span><br><span class="line">      <span class="comment"># 相当于所有的h_j</span></span><br><span class="line">      hidden = array_ops.reshape(attn_states,</span><br><span class="line">                                 [<span class="number">-1</span>, self._attn_length, <span class="number">1</span>, self._attn_size])</span><br><span class="line">      <span class="comment"># 计算Uh_j,shape:[[None, attn_len, 1, attn_vec_size]]</span></span><br><span class="line">      hidden_features = conv2d(hidden, k, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="string">"SAME"</span>)</span><br><span class="line">      y = _linear(query, self._attn_vec_size, <span class="literal">True</span>)</span><br><span class="line">      <span class="comment"># 计算WS_i</span></span><br><span class="line">      y = array_ops.reshape(y, [<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, self._attn_vec_size])</span><br><span class="line">      <span class="comment"># attention相似度计算公式，s\in R^&#123;-1, attn_len&#125;，对应所有的e_&#123;ij&#125;</span></span><br><span class="line">      s = reduce_sum(v * tanh(hidden_features + y), [<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">      <span class="comment"># a \in R^&#123;-1, attn_len&#125;，对应论文中的\alpha</span></span><br><span class="line">      a = softmax(s)</span><br><span class="line">      <span class="comment"># 计算上下文向量c_i=\sum \alpha_&#123;ij&#125; * h_j</span></span><br><span class="line">      d = reduce_sum(</span><br><span class="line">          array_ops.reshape(a, [<span class="number">-1</span>, self._attn_length, <span class="number">1</span>, <span class="number">1</span>]) * hidden, [<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">      new_attns = array_ops.reshape(d, [<span class="number">-1</span>, self._attn_size])</span><br><span class="line">      <span class="comment"># 扔掉最早的一个attention-states</span></span><br><span class="line">      new_attn_states = array_ops.slice(attn_states, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">      <span class="keyword">return</span> new_attns, new_attn_states</span><br></pre></td></tr></table></figure>
<p>最后，从static<em>rnn到dynamic<em>rnn到bidirection_dynamic_rnn，其内部都是调用了这些cell的`__call</em></em>()`方法。</p>
<h4 id="3、各种Attention"><a href="#3、各种Attention" class="headerlink" title="3、各种Attention"></a>3、各种Attention</h4><ul>
<li>_BaseAttentionMechanism</li>
<li>BahdanauAttention</li>
<li>LuongAttention</li>
<li>DynamicAttentionWrapper</li>
</ul>
<p>官方给的用法参考：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cell = tf.contrib.rnn.DeviceWrapper(LSTMCell(<span class="number">512</span>), <span class="string">"/gpu:0"</span>)</span><br><span class="line">attention_mechanism = tf.contrib.seq2seq.LuongAttention(<span class="number">512</span>, encoder_outputs)</span><br><span class="line">attn_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(</span><br><span class="line">  cell, attention_mechanism, attention_size=<span class="number">256</span>)</span><br></pre></td></tr></table></figure>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li><a href="https://www.tensorflow.org/api_guides/python/contrib.seq2seq#Attention" target="_blank" rel="noopener">https://www.tensorflow.org/api_guides/python/contrib.seq2seq#Attention</a></li>
<li><a href="https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cell_wrappers_RNNCells_that_wrap_other_RNNCells_" target="_blank" rel="noopener">https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cell_wrappers_RNNCells_that_wrap_other_RNNCells_</a></li>
</ul>
]]></content>
      <categories>
        <category>编程总结</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Several Papers about Syntactic Structure Utilization</title>
    <url>/2019/11/03/nlp/Several-Papers-about-Syntactic-Structure-Utilization/</url>
    <content><![CDATA[<h3 id="Several-Papers-about-Syntactic-Structure-Utilization"><a href="#Several-Papers-about-Syntactic-Structure-Utilization" class="headerlink" title="Several Papers about Syntactic Structure Utilization"></a>Several Papers about Syntactic Structure Utilization</h3><p>近期读的几篇关于如何有效利用句法信息的论文。</p>
<h4 id="1-《Syntax-Encoding-with-Application-in-Authorship-Attribution》"><a href="#1-《Syntax-Encoding-with-Application-in-Authorship-Attribution》" class="headerlink" title="1.《Syntax Encoding with Application in Authorship Attribution》"></a>1.《Syntax Encoding with Application in Authorship Attribution》</h4><p>EMNLP’18, By Richong.</p>
<p>希望解决的问题：设计出一种通用的能够解析句法结构表示的方法，此方法需要能够和各种不同的NLP方法结合使用。同时为了验证该方法，作者将其应用到Authorship Attribution任务中（句法结构可以认为是作者的写作风格）。</p>
<p>已有的方法：利用句法信息的方法可以分为两类，一类抽取句法结构树中的特征，这种方法需要人工设计特征提取过程而且丢弃掉了很多句法结构信息；一类利用句法结构复制语义编码，其核心任务还是语义编码只是利用句法信息更好地生成语义表示，此类任务大多在LSTM模型基础上做，和CNN等方法难以结合。</p>
<p>本文的方法（思路）：</p>
<p>首先，每个单词$w_i$对应一个Syntax Path: $r(w_i) = {t_1, \cdots, t_L}$，而任意一个句法路径的无序集合$R = {(i, R(w_i))}$都可以用来完整的恢复一个句子中的句法信息。而后，把每个句法路径编码为一个向量$\bar R(w)\in R^K$，这里作者使用了句法符号和其所在的句法树层次的embedding的按元素乘的结果之和作为句法路径的向量表示，比如are这个token的句法路径embedding就是$emb(VP)^t\circ emb^d(p_1) + emb^t(VBP)\circ emb^{d}(p_2)$。作者引入并证明了两个引理，说明只要K足够大，这种Syntax Encoding的方法就不会有句法信息的损失。这样即使不同的NLP下游任务需要不同侧重点的句法信息，也可以保证都包含在encode之后的表示里面。</p>
<p><img src="/images/image-20200604170950613.png" alt=""></p>
<p>模型过程：N-Gram的CNN卷积抽取内容信息（语义信息），句法编码抽取句法信息。</p>
<p>实验：在CCAT10,CCAT50,IMDB62,Blogs10,Blogs50这五个AA任务数据集上进行了实验。并进行了各种对照辅助实验分析方法效果。</p>
<p>Future: 句法编码的信息保留，那么如何设计一个足够有效的信息抽取过程 ，保证下游任务需要的信息被抽取出来呢？</p>
<h4 id="2-《Graph-Convolutional-Encoders-for-Syntax-aware-Neural-Machine-Translation》"><a href="#2-《Graph-Convolutional-Encoders-for-Syntax-aware-Neural-Machine-Translation》" class="headerlink" title="2. 《Graph Convolutional Encoders for Syntax-aware Neural Machine Translation》"></a>2. 《Graph Convolutional Encoders for Syntax-aware Neural Machine Translation》</h4><p>EMNLP’18, by Joost Bastings, in 阿姆斯特丹&amp;爱丁堡.</p>
<p>机器翻译任务中引入句法结构信息。利用GCN编码句法依赖树。是比较早在生成任务中应用语言学结构辅助并利用图神经网络编码的工作。Bow/CNN/BiRNN + GCN分别进行了实验。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>Syntactic</tag>
      </tags>
  </entry>
  <entry>
    <title>ACL&#39;19 Poster</title>
    <url>/2019/09/10/paper/ACL&#39;19-poster/</url>
    <content><![CDATA[<h3 id="IJCAI’20-Poster和Slide"><a href="#IJCAI’20-Poster和Slide" class="headerlink" title="IJCAI’20 Poster和Slide"></a>IJCAI’20 Poster和Slide</h3><p>论文：<a href="https://www.aclweb.org/anthology/P19-1440/" target="_blank" rel="noopener"><a href="https://www.aclweb.org/anthology/P19-1440.pdf" target="_blank" rel="noopener">Complex Question Decomposition for Semantic Parsing</a></a></p>
<p>poster: <a href="https://cairohy.github.io/acl19/poster.pdf">这里</a></p>
]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>Paper</tag>
      </tags>
  </entry>
  <entry>
    <title>When Bert Forgets How To POS： Amnesic Probing of Linguistic Properties and MLM Predictions</title>
    <url>/2020/06/04/nlp/When%20Bert%20Forgets%20How%20To%20POS/</url>
    <content><![CDATA[<h3 id="When-Bert-Forgets-How-To-POS：-Amnesic-Probing-of-Linguistic-Properties-and-MLM-Predictions"><a href="#When-Bert-Forgets-How-To-POS：-Amnesic-Probing-of-Linguistic-Properties-and-MLM-Predictions" class="headerlink" title="When Bert Forgets How To POS： Amnesic Probing of Linguistic Properties and MLM Predictions"></a>When Bert Forgets How To POS： Amnesic Probing of Linguistic Properties and MLM Predictions</h3><p><a href="http://arxiv.org/abs/2006.00995" target="_blank" rel="noopener">http://arxiv.org/abs/2006.00995</a>, by 巴伊兰大学和AI2。</p>
<p>利用一种改进的Probe方法来对黑盒NN模型的行为进行分析，判断。</p>
<h4 id="1-常用的Probe方法及其不足。"><a href="#1-常用的Probe方法及其不足。" class="headerlink" title="1.常用的Probe方法及其不足。"></a>1.常用的Probe方法及其不足。</h4><p>什么是probe？增强神经网络模型可解释性的方法。理解这个黑盒子，尝试回答比如BERT的每个head编码了哪些信息？哪些hidden state的维度被实际用在了预测中？如果去掉某些信息会怎么样？等问题。probe是解决这类问题的一种方法，也可以叫做auxilliary prediction【辅助预测】或者diagnostic classification【诊断分类】。</p>
<p>probe的过程？固定pre-train的原始模型的feature层，通过在原始模型的上层训练一个简单的分类网络，以对某种属性property的预测准确率来证明该信息被编码到了原始模型的隐藏表示中。</p>
<p>不足？只能证明信息被encode到了隐藏表示中，无法证明原始模型有效利用了这些信息。</p>
<h4 id="2-这篇文章提出的方法：Amnesic-Probing。"><a href="#2-这篇文章提出的方法：Amnesic-Probing。" class="headerlink" title="2.这篇文章提出的方法：Amnesic Probing。"></a>2.这篇文章提出的方法：Amnesic Probing。</h4><p>改进方式：<strong>反事实推理</strong>，假设某种属性P被解决任务T的原始模型有效使用了，那么把P去掉肯定会影响原始模型解决T的能力 。因此如果去掉P不影响解决T的能力，那么P中的信息就没有被有效利用。Intervention介入：这个工作和之前一些工作不同的地方在于它通过修改表示层来介入。其实很像是ablation，只是这里不能重新训练原始模型，算是黑盒子版本的ablation。</p>
<p>步骤：</p>
<p>0.问题：原始模型负责将输入数据<script type="math/tex">x</script> 编码为隐藏表示<script type="math/tex">h(x)</script>，而probe负责构建一个分类器<script type="math/tex">c(h(x))</script>用来预测目标任务中的分类标签<script type="math/tex">y</script>。</p>
<p>1.构建反事实表示（counterfactual representation），这里利用了最近的一种方法Iterative Null- space Projection（INLP），试图构造出一种和属性Z无关的隐藏表示<script type="math/tex">h(x)^{\neg Z}</script>。在构造的时候，主要关注了两个问题，1）信息额外遗失，会导致因为hidden state被修改而性能下降而非属性的去除；2）其他属性信息丢失，如何保证去掉的仅仅是属性Z相关的信息，而不涉及到其他属性。构建的细节暂时略过。这个过程被叫做<strong>Amnesic Intervention</strong>。</p>
<p>2.实验。在BERT上面进行了实验，probe任务是多个语言学特征（POS、DEP、NER、phrase start、phrase end）的序列标注任务，原始任务是LM。作者实验的逻辑是：</p>
<ul>
<li>首先增加分类层，证明hidden state中有这些信息，而且利用这些信息能够得到一个不错的分类器（比如POS tagger），以往的有些probe论文就此认为BERT利用了这些信息但作者认为这只能证明hidden state中有这些信息；结果如下表中Probing行。</li>
<li>而后作者利用之前提出的介入过程，分别去掉hidden state中与这些语言学特征相关的信息，然后观察下表中LM-ACC即语言模型预测token的准确率。可以看到相比较之前的Vanilla行，Amnestic在去掉依赖、粗粒度和细粒度词性特征的时候效果下降最多，而LM基本没有利用到phrase信息。作者认为这才能准确的度量某一类信息到底有没有被使用。而相关性系数计算表明，若本文提出的方式是对的，那么Probe task的性能不能有效度量某一类信息对原始模型预测的影响。</li>
<li>额外的，为了比较对hidden state进行介入前后的LM在预测分布上的总体差别，作者引入了介入前后预测分布的KL散度（都和Vanilla比较），结果和Acc行的结果是类似的。</li>
</ul>
<p><img src="/images/image-20200604111224367.png" alt=""></p>
<p>最后，作者还进行了一系列的子实验。</p>
<h4 id="3-总结。"><a href="#3-总结。" class="headerlink" title="3.总结。"></a>3.总结。</h4><p>probe任务的效果好只能证明hidden state中包含这一类信息，但无法证明这一类信息在预测原始任务T的时候被使用了。</p>
<p>如果本文中的介入方法INLP真的可以完全去除且只去除表征特定属性Z的信息，那么由去除该信息后的隐藏表示在原始任务上的性能变化来体现原始模型中Z对原始任务的重要度是合理的。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>Probe</tag>
        <tag>Causal Inference</tag>
        <tag>Explanation</tag>
      </tags>
  </entry>
  <entry>
    <title>Recent Language Model Papers</title>
    <url>/2020/03/04/nlp/Recent%20Language%20Models/</url>
    <content><![CDATA[<h3 id="Recent-Language-Model-Papers"><a href="#Recent-Language-Model-Papers" class="headerlink" title="Recent Language Model Papers"></a>Recent Language Model Papers</h3><p>最近的语言模型和词嵌入的文章，有的读的仔细点，有的略读，不断更新中。</p>
<h4 id="1-AlBert"><a href="#1-AlBert" class="headerlink" title="1.AlBert"></a>1.AlBert</h4><p><a href="https://openreview.net/forum?id=H1eA7AEtvS" target="_blank" rel="noopener">https://openreview.net/forum?id=H1eA7AEtvS</a> ，ICLR’20, by Google, SOTA in GLUE.</p>
<p>一个轻量级的BERT但是效果更好。它所做的修改主要有：</p>
<ul>
<li><p>首先把词向量矩阵分解了，这样使词向量矩阵的维度和hidden_size解耦，否则词向量矩阵参数量太大。</p>
</li>
<li><p>把每层的参数共享了，减少参数数量。</p>
</li>
<li><p>在这个基础上，能够把hidden_size提高到6144这种量级。</p>
</li>
<li><p>为了对句子级别进行建模，ALBERT增加了一个sentence order prediction（SOP）任务而不是被证明太简单基本无效的NSP任务，给定当前句子与其下一句，或者是顺序翻转的两句话，希望模型预测句子序是否准确。</p>
</li>
</ul>
<h4 id="2-GPT-3，一个看看就好的单向语言模型"><a href="#2-GPT-3，一个看看就好的单向语言模型" class="headerlink" title="2.GPT-3，一个看看就好的单向语言模型"></a>2.GPT-3，一个看看就好的单向语言模型</h4><p><a href="http://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">http://arxiv.org/abs/2005.14165</a> ，By OpenAI。</p>
<p>回想去年年底，我在尝试在GPT-2 Small（127M参数）模型的基础上搞点东西的时候，还在想这玩意fine-tune有点慢，batch size设不大啊。害，现在拿到论文只能看看人实验结果了，连下载模型的想法都不会有~</p>
<p>GPT-3，一个最大175Billion参数的基于Transformer-decoder的单向自回归语言模型。希望解决的就是pretrain-fineTune这套框架。他们觉得这套框架虽然解决了task-specific Model的问题，但是没有解决task-specific data的问题，所以领域相关数据还是导致应用很受限。</p>
<p>因此，他们提出来了三种使用预训练模型的方式：<strong>Zero-shot</strong>、<strong>One-shot</strong>和<strong>Few-shot</strong>，但是这和我们通常所理解的这几个概念不同，它这几种setting，本质上都是zero-shot。预训练只使用了大规模的无标记语料（45TB！），而在做某个任务的时候，在一句自然语言的prompt之后，会跟上0, 1, n个例子，就像是英语听力考试的时候前面那个示例题。那这是不是作弊呢？反正，没有反向传播过程，模型是不会记得自己见过这些数据的，而且这几个随机选择的例子和任务中的sample可能差别很大。</p>
<p>所以在这种Train&amp;Use的setting下，这位model structure和GPT-2相同仅仅是参数量、数据量提高若干倍的选手，它的成绩是：</p>
<p>Cloze任务，Lambada SOTA，其他两个接近。QA任务，TrivialQA SOTA，其他两个还行。NMT：1/6接近SOTA，NMT要是再这样就SOTA那就没得玩了。CSR-QA，离SOTA差一些。SuperGlUE离SOTA差挺多的。。。。实验的任务挺多，就不细说了。Learning and using novel words这个实验里举的例子还是挺有意思的。</p>
<p>最后，新闻要放在一起看。希望OpenAI别去做Code Generation。-_-</p>
<p><img src="/images/image-20200604223912770.png" alt="人类成功分辨GPT-3和人工文章的准确率与微软用AI替换新闻编辑"></p>
<h4 id="3-Analysing-Lexical-Semantic-Change-with-Contextualised-Word-Representations"><a href="#3-Analysing-Lexical-Semantic-Change-with-Contextualised-Word-Representations" class="headerlink" title="3. Analysing Lexical Semantic Change with Contextualised Word Representations"></a>3. Analysing Lexical Semantic Change with Contextualised Word Representations</h4><p><a href="http://arxiv.org/abs/2004.14118" target="_blank" rel="noopener">http://arxiv.org/abs/2004.14118</a> ，ACL’20，by 阿姆斯特丹大学。</p>
<h5 id="（0）术语翻译和理解"><a href="#（0）术语翻译和理解" class="headerlink" title="（0）术语翻译和理解"></a>（0）术语翻译和理解</h5><p>词汇语义：word meaning/lexical semantic，每个词汇(word and sub-word)表达的概念。</p>
<p>词形：word form。类似’girl’和’girls’。</p>
<p>词义：word sense，在不同context下一个单词表现出的语义，同一个word form可能有不同的word sense。在本文中和word usage意义接近，但是是一种更加离散化的usage。可以对应中文“义项”。</p>
<p>word usage：语料中word的每次出现。</p>
<p>历时性语料：diachronic corpus，由不同时期的文本所组成的语料，可以用来分析词汇语义变化。</p>
<p>silhouette score：一种计算聚类得分的方式，类内元素一致性越高&amp;类间元素相似度越低，则该得分越高。</p>
<h5 id="（1）解决的问题"><a href="#（1）解决的问题" class="headerlink" title="（1）解决的问题"></a>（1）解决的问题</h5><p>预训练的词向量使用一个语义向量来表示一个单词的语义，现在的预训练词向量工作能够衡量不同语境context下的语义。但是每一种自然语言实际上都是在不断演化的。同样地，语言中词汇的语义也是随着时间变化的，比如，英语中的“girl”在14世纪表示小孩子而现在表示小女孩。如何去表示、比较单词在不同时期的语义呢。</p>
<p><strong>词义变化检测</strong>（lexical semantic change detection）尝试检测以及归纳词汇语义变化的规律，并用可以量化和复现的方式评估这个过程。</p>
<p>似乎这个词也会被翻译为<strong>历时词义演变</strong>。</p>
<h5 id="（2）已有的方法"><a href="#（2）已有的方法" class="headerlink" title="（2）已有的方法"></a>（2）已有的方法</h5><p>已有的词义变化检测方法可以归纳为两种，</p>
<p>第一种是form-based方法。这种方法在历时性语料上面的不同时间区间（time interval）训练单独的模型。同一个时间区间中每个word由一个语义向量表达其语义，通过计算同一单词不同时期的语义向量的距离来度量语义的变化。【由于同一个词在不同usage不同context下语义是不一样的，这种方式的假设是一个语义向量足以表示这些值】。这些工作大都基于分布式词嵌入，把每种词形（word form）抽象为一个语义表示，但是这可能导致多种word sense只有一个表示，而语义变化可能是在更细粒度下的改变（比如说只在某种特殊语境下单词语义发生改变）。</p>
<p>第二种是sense-based方法，这种方法也是训练出time-dependent的语义表示，但是考虑分开表示每个word在不同sense下的语义，比如把word的语义表示为所有word sense下的多项式分布。现有的这种方法的局限性在于，a) 依赖于上下文的词袋模型来区分word sense；以及b)监督式的方法大都需要预先确定sense的数量。</p>
<h5 id="（2）这篇文章提出的解决方法"><a href="#（2）这篇文章提出的解决方法" class="headerlink" title="（2）这篇文章提出的解决方法"></a>（2）这篇文章提出的解决方法</h5><p>一种<strong>无监督</strong>的方法来<strong>结合上下文相关词向量</strong>BERT处理<strong>词义变化</strong>。</p>
<ul>
<li>首先产生每个word的<strong>usage Representations</strong>，也就是在所有不同context下的语义。给定一个单词 $w$和它出现的（window-size=128）的上下文$s = (v_1, \cdots, v_i, \cdots, v_n), v_i=w$，用BERT对这个上下文进行编码并把每一层的hidden state加起来作为$w$在这种语境下的表示。（这里用加是因为无论是concat所有层还是选择其中某一层，不同时段的word representation都无法产生足以比较的向量距离差异）那么对于语料中单词$w$出现的N次Usage，可以得到一个Usage矩阵$U_w = (w_1, \cdots, w_N)$，同时，对每次usage除了usage representation，还存储了对应的context以及temporal label $t_i$来标记时段。</li>
</ul>
<p><img src="/images/image-20200609010426759.png" alt="Usage representations和聚类并计算得到的不同时刻的usage分布"></p>
<ul>
<li>聚类到Usage Types：对每个word的N个usage representations，用K-Means把它们聚类为2-10个类别（usage type），最终选择的类别数量K通过比较聚类结束时的silhouette score选择。而后可以计算出给定单词$w$在$t$时刻每个usage type所出现的次数$f^t_w$，并在归一化后得到$t$时刻的usage分布$u^t_w\in R^{K_w}$。上面这张图是Usage representations和聚类并计算得到的不同时刻的usage分布的一个例子。</li>
</ul>
<p><img src="/images/image-20200609005724949.png" alt="t时段每个Usage Type k的出现次数"></p>
<ul>
<li>语义变化的量化：<ul>
<li>Entropy Difference(ED)：计算$t$和$t’$时段usage分布的熵的差值，通过熵来衡量这个分布的不确定性，一个单词的分布熵越大代表这个词的含义越丰富，因此ED值的增加意味着从$t$到$t’$时段该单词的词义变得广泛(broad)。比如下图（<a href="https://clsw2020.lt.cityu.edu.hk/repo/posters/slides/CLSW2020-139-slides.pdf" target="_blank" rel="noopener">出处</a>）所示汉字危的词义分布比较先秦和元明清时期，ED值是增加的。</li>
<li>Jensen-Shannon divergence(JSD)：引入这一项用来衡量两个usage分布的差异。较大的JSD意味着从$t$到$t’$该单词的usage发生了明显改变。<ul>
<li>Average pairwise distance (APD)：为了避免聚类为usage type时引入的噪音影响历时语义分析加入这一项。直接在两个时段的usage representation计算平均点对点距离。（拿出两个时段所有usage representation向量计算距离并求这$N<em>t\cdot N</em>{t’}$项距离的均值）</li>
<li>判断多个连续时期的历时语义偏移。比如下图这种超过两个时段的情况，对相邻两个时段用上面三种metric中的一种依次计算一个得分，最终的得分向量的均值可以判断语义偏移的总体程度，最大值用来判断语义偏移发生的极限。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/images/image-20200609011508175.png" alt="中文词义历时演变"></p>
<h5 id="（3）实验"><a href="#（3）实验" class="headerlink" title="（3）实验"></a>（3）实验</h5><p>实验数据：英文历时语料，the Corpus of Historical Ameri- can English（COHA）</p>
<p>目标单词：100个标注过语义偏移得分的单词，得分与context无关，由五个人类标注者衡量两个时段下某个单词的语义偏离程度并给出得分。（GEMS数据集）</p>
<p>实验结论：</p>
<ul>
<li>为了证明K-means利用BERT产生的usage表示聚类的usage type和人类评判一致。选择了16个单词，每个单词的每个usage type标注5个usage，形成3285个usage pair，并通过众包进行相似度标注，得到了一个数据集，并且多个标注者之间的打分一致性符合要求（说明人类评判较为一致）。而最终人类在usage pair之间的相似度打分和聚类后的Usage Vector之间的相似度计算也表明，对16个单词中的10个，机器的打分和人类具有一致性。</li>
<li>通过比较GEMS数据集上的人类打分语义偏离分数和不同metric生成的偏离分数的斯皮尔曼相关系数（0-1，越小越好），说明了上下文相关词嵌入在历时语义演变分析中很有效。</li>
</ul>
<p><img src="/images/image-20200609151756717.png" alt="不同metric下生成的语义偏离分数和人类打分的相关性系数"></p>
<h5 id="（4）分析"><a href="#（4）分析" class="headerlink" title="（4）分析"></a>（4）分析</h5><p>对两个问题进行了分析：</p>
<ul>
<li>Usage Type捕获了哪些信息？<ul>
<li>区分字面用法和转喻用法。比如“我挂了一个窗帘”中“窗帘”代表其字面意思“一块悬垂的布”，而“法律战争的窗帘正在被提出”中“窗帘”代表其转喻意思“阻碍信息自由交流的障碍”。</li>
<li>用于组成短语或者命名实体的特殊用法，比如“alexander graham bell”中的“bell”。</li>
<li>…</li>
</ul>
</li>
<li>偏离得分反映了哪些语义变化？下图来自论文原文。<ul>
<li>broaden，比如增加新的usage type，增加转喻用法等。</li>
<li>narrow，很多usage type在消失或者使用频率减少。</li>
</ul>
</li>
</ul>
<p><img src="/images/image-20200609154814456.png" alt="不同时段的Usage分布"></p>
<h5 id="（5）-总结"><a href="#（5）-总结" class="headerlink" title="（5） 总结"></a>（5） 总结</h5><p>一个利用上下文相关词嵌入分析历时语义演变的工作，不同于基于word sense的工作，由预训练词嵌入和聚类方法自动生成不定数量的usage type，提供了更灵活的获得sense representation的方式。能相对更有效的估计语义演变的程度。作者也说其下一步的工作可能是探究如何去检测更细粒度的词汇语义变化，比如说usage-level的语义演变。</p>
<h4 id="4-Moving-Down-the-Long-Tail-of-Word-Sense-Disambiguation-with-Gloss-Informed-Bi-encoders"><a href="#4-Moving-Down-the-Long-Tail-of-Word-Sense-Disambiguation-with-Gloss-Informed-Bi-encoders" class="headerlink" title="4. Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders"></a>4. Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders</h4><p><a href="https://arxiv.org/pdf/2005.02590.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2005.02590.pdf</a> ，ACL’20，Zettlemoyer组，By 华盛顿大学&amp;FAIR。</p>
<p>WSD（词义消歧）任务为某个上下文下的单词预测具体含义。All-word WSD为语料中每个有歧义的单词找到具体含义，被形式化为在预定义的所有义项中找到正确的类别，这是这个工作的任务。</p>
<p>gloss：义项所对应的描述文本，来自词义体系（wordNet或其他词典）中。</p>
<p>方法结构很简单，context encoder用来编码单词及上下文；gloss encoder对义项描述进行编码，利用交叉熵最大化正确义项类别的分类概率。两个分类器底层都使用预训练的BERT。目标是把word sense gloss和context-aware word embedding编码到同一空间，而且让一个单词和对应的词义点积距离接近。</p>
<p><img src="/images/image-20200711172501833.png" alt="模型结构"></p>
<p>在WSD数据集上和包括GlossBERT在内的baseline进行了对比。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>Language Model</tag>
        <tag>Word Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>PPLM-使用预训练语言模型进行可控制的文本生成</title>
    <url>/2019/12/30/nlp/pplm/</url>
    <content><![CDATA[<h3 id="PLUG-AND-PLAY-LANGUAGE-MODELS-A-SIMPLE-APPROACH-TO-CONTROLLED-TEXT-GENERATION"><a href="#PLUG-AND-PLAY-LANGUAGE-MODELS-A-SIMPLE-APPROACH-TO-CONTROLLED-TEXT-GENERATION" class="headerlink" title="PLUG AND PLAY LANGUAGE MODELS: A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION"></a>PLUG AND PLAY LANGUAGE MODELS: A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION</h3><p><a href="https://arxiv.org/pdf/1912.02164.pdf，by" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.02164.pdf，by</a> Uber AI。</p>
<p>大规模预训练语言模型的效果不错，但如何利用它们生成属性可控的文本（比如说某一领域、某种风格、某种情感），fine-tune是一种方法，本文提出了一种不需要重新训练的方式。</p>
<p>无需fine-tune或者重新训练LM。其具体的做法是根据梯度将Transformer-decoder每一层的hidden state向LM和attribute的方向改变一步。</p>
<p>对于attribute，进行了两类属性的控制，1）情感，通过一个预训练的二分类器判断生成的候选文本的误差；2）主题，通过指定一个中心词，找到wordnet的相关词集合，以multi-hot的方式将这些词列为vocabulary中的ground-truth-labels来计算误差。</p>
<p>分为三步，第一步通过一个前向过程获取$p(a|x),p(x)$，第二步通过反向传播获取相对于H的梯度并更新H，第三步利用更新之后的$\tilde H$来预测此时刻的vocab分布。</p>
<p>计算$H_t$的更新值$\nabla H_t$通过若干次重复计算梯度并衰减求和得到。</p>
<p>为了生成文本的流畅度，增加一项KL项，缩小输出的分布和之前的分布的KL值。</p>
<p>最后，最后采样的分布是未改变的分布和改变后的分布的加权之和。</p>
<p>最后的最后，根据$p(a|x)$，采样出来的候选seq集合还可以根据与attribute一致的程度进行排序。下图来自论文原文。</p>
<p><img src="/images/image-20200604171731511.png" alt=""></p>
<p>总结，plug-and-play看似不错，但是实际使用中有两个问题：1）速度，每个inference的step都做若干步反向传播和参数更新太慢了，速度慢到基本没法用；2）效果，作者在8个topic上面尝试的结果不错，但是更广泛的属性词效果不一定会很明显。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>Language Model</tag>
        <tag>Text Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>IJCAI&#39;20 Camera-Ready</title>
    <url>/2020/12/10/paper/IJCAI&#39;20-camera-ready/</url>
    <content><![CDATA[<h3 id="IJCAI’20-Poster和Slide"><a href="#IJCAI’20-Poster和Slide" class="headerlink" title="IJCAI’20 Poster和Slide"></a>IJCAI’20 Poster和Slide</h3><p>论文：<a href="https://www.ijcai.org/Proceedings/2020/527" target="_blank" rel="noopener">learning with noise: improving distantly-supervised fine-grained entity typing via automatic relabeling</a></p>
<p>video: <a href="">这里，暂时not available</a></p>
<p>poster: <a href="https://cairohy.github.io/ijcai20/poster.pdf">这里</a></p>
<p>slide: <a href="https://cairohy.github.io/ijcai20/slide.pdf">这里</a></p>
]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>Paper</tag>
      </tags>
  </entry>
  <entry>
    <title>python基本数据类型:List</title>
    <url>/2015/10/13/python/python%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B-List/</url>
    <content><![CDATA[<p>列表<code>List</code>是Python的六种基本数据类型之一，用于顺序存储结构。列表可以用<code>[]</code>或者<code>list()</code>来建立。由于列表是Python中的可变对象，而Python中又大量使用引用传递，因此列表的使用过程中遇到了一些坑，下面对这些导致过错误的地方进行阐述，加深一下印象。</p>
<h3 id="一、append-与-的区别"><a href="#一、append-与-的区别" class="headerlink" title="一、append()与+=的区别"></a>一、<code>append()</code>与<code>+=</code>的区别</h3><ul>
<li><p>1.append()是列表的内建函数，而+=是增量赋值语句</p>
</li>
<li><p>2.当传递给函数或赋值语句的是个体对象而不是列表（或者其他可迭代对象）时，两者是等价的</p>
</li>
<li><p>3.当传递给函数或赋值语句的是列表或其他可迭代对象x时，<code>append(x)</code>会把该对象作为一个整体添加到列表末尾，而<code>+= x</code>会把对象的每个迭代值依次添加到列表末尾，如下代码示例</p>
</li>
<li><p>4.<code>+=x</code>和<code>extend(x)</code>是等价的</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b = [<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">a.append(b)</span><br><span class="line"><span class="comment">#此时a = [1,2,3,[4,5]]</span></span><br><span class="line">a += b</span><br><span class="line"><span class="comment">#此时a = [1,2,3,[4,5],4,5]</span></span><br><span class="line">c = []</span><br><span class="line">c.extend(a)</span><br><span class="line"><span class="comment">#此时c = [1,2,3,[4,5],4,5]</span></span><br></pre></td></tr></table></figure>
<h3 id="二、赋值语句a-a-1-与增量赋值语句a-1-的区别"><a href="#二、赋值语句a-a-1-与增量赋值语句a-1-的区别" class="headerlink" title="二、赋值语句a = a + [1]与增量赋值语句a += [1]的区别"></a>二、赋值语句<code>a = a + [1]</code>与增量赋值语句<code>a += [1]</code>的区别</h3><ul>
<li><p>1.首先<code>a = a + [1]</code>会合并两个列表并返回一个新的列表，操作并不是就地进行的</p>
</li>
<li><p>2.<code>a += [1]</code>这种增量赋值语句只要可能的情况下，实际的操作是就地进行的，意思是并非创建一个新对象然后将其赋值给目标，而是修改老的对象</p>
</li>
<li><p>3.所以<code>+=</code>、<code>*=</code>这种语句虽然写着方便又省内存，但是也要想清楚再使用，尤其是当别的集合对象中存在这个列表的引用的时候，不然可能有BUG存在，比如下面这种：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b = [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">b.append(a)</span><br><span class="line"><span class="comment">#这时b应该是[4,5,6,[1,2,3]]</span></span><br><span class="line">a += [<span class="number">4</span>]</span><br><span class="line"><span class="comment">#就地修改，这时b变成了[4,5,6,[1,2,3,4]]</span></span><br><span class="line">a = a + [<span class="number">5</span>]</span><br><span class="line"><span class="comment">#合并列表，这时b仍是[4,5,6,[1,2,3,4]]</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>PEP3119-介绍虚拟基类ABC</title>
    <url>/2016/04/27/python/PEP3119-%E4%BB%8B%E7%BB%8D%E8%99%9A%E6%8B%9F%E5%9F%BA%E7%B1%BBABC/</url>
    <content><![CDATA[<h2 id="一、摘要"><a href="#一、摘要" class="headerlink" title="一、摘要"></a>一、摘要</h2><p>这是一个Python3环境下支持<strong>抽象基类</strong>（<em>Abstract Base Class,ABC</em>）的提议。该提议包括：</p>
<ul>
<li>重载<code>isinstance()</code>和<code>issubclass()</code>方法的方式。</li>
<li>一个新的<code>abc</code>模块，它定义了用于抽象基类的一个元类以及用来定义抽象方法的装饰器。</li>
<li>添加到集合模块<code>collections</code>中，用于容器和迭代器的特定抽象基类。</li>
</ul>
<p>不同于接口或者是泛型函数，大多数关于这个提议的思考并不和ABC的特定原理相关，而是关于一些哲学问题，比如“什么是集合”，“什么是映射”以及“什么是序列”。</p>
<p><a href="https://www.python.org/dev/peps/pep-3141/" target="_blank" rel="noopener">PEP3141</a>是与本提议相关的一个PEP，它定义了数值类型的ABC。</p>
<h2 id="二、原理"><a href="#二、原理" class="headerlink" title="二、原理"></a>二、原理</h2><p>面向对象编程领域，与一个对象的交互模式可以分为两种基本类型：<strong>调用（invocation）</strong>和<strong>自省（inspection）</strong>。</p>
<ul>
<li>调用：就是通过调用其方法与对象交互，它经常与多态相结合，也就是根据对象的类型不同，调用对象的同一个方法执行的代码也不同。</li>
</ul>
<ul>
<li>自省：就是说外部代码（对象方法以外的代码）能够测试对象的类型和属性，基于这些信息来决定如何处理该对象。</li>
</ul>
<p>这两种使用模式都有着相同的目的，就是能够使用通用的方式处理不同的、未知的对象，同时对于每种类型的对象能够特殊对待。</p>
<p>在经典的OOP理论中，偏重于使用调用而不鼓励自省，自省被认为是早期过程式编程风格的产物。但是，在实践中这种观点过于教条了，导致了设计的僵化。而这个问题在Python这种具有动态特性的语言中更加严重。</p>
<p>经常会出现的场景是，在没有对象的类的作者参与的情况下处理与该对象的交互。在对象的每个方法中考虑并满足每个潜在使用者的需求很可能不是最好的解决方案。而且，有很多强大的分派哲学和经典的OOP的要求（行为必须严格的压缩在对象内部）相左，比如说模式匹配驱动逻辑。</p>
<p>另一方面，经典OOP理论学家对于自省的批评在于它缺少一定的标准和被检查的对象的特别特征。在Python这样的语言中，外部代码可以通过反射获得并利用一个对象几乎所有信息，有很多不同方式来测试一个对象是否遵守一个特定的协议。比如，如果想知道一个对象是否是一个<code>可变序列的容器</code>，那么可以寻找是否存在基类<code>list</code>，或者可以寻找是否存在一个<code>__getitem__()</code>方法。但是尽管这些测试看起来很明显可以回答一些问题，但是它们都不正确（不全面）。</p>
<p>一个被广泛认同的方法是标准化这些测试， 把它们组合为正式的排列。最容易想到的就是：通过继承或者其他方式，把每一个类同一个标准的可测试属性集合绑定起来。 每一个测试代表若干个保证：该类的通用行为的保证以及该类中具有哪些方法的保证。</p>
<p>这个PEP提出了一种特定的组织这些测试的策略，也就是<code>抽象基类ABC</code>。ABC也是Python中的类，他们被添加到对象的继承树，从而外部以自省方式操作对象的代码能够检测到该对象的特定特征。这些测试使用<code>isinstance()</code>方法来完成，而特定ABC的出现代表测试通过。</p>
<p>另外，ABC定义了类型的典型行为包含的方法的最小子集。通过ABC类型来分辨出的对象必然包含该子集中的所有方法。这些方法中的每一个都有一个相应的<code>泛化抽象语义定义</code>，该定义在ABC的文档中。这些标准语义定义不强制要求实现，但非常推荐。</p>
<p>就像Python中其他事情一样，这些保证也都是基于君子协定，也就是语言在ABC中进行某些保证，但是否实现取决于具体类的开发者。</p>
<h2 id="三、详述"><a href="#三、详述" class="headerlink" title="三、详述"></a>三、详述</h2><h3 id="3-1-重载isinstance-和issubclass"><a href="#3-1-重载isinstance-和issubclass" class="headerlink" title="3.1 重载isinstance()和issubclass()"></a>3.1 重载<code>isinstance()</code>和<code>issubclass()</code></h3><p>在这个PEP和其伴生<a href="https://www.python.org/dev/peps/pep-3141" target="_blank" rel="noopener">PEP3141</a>的开发过程中，遇到的一个选择就是：开发更多的细粒度标准ABC，还是较少的粗粒度标准ABC。比如说，在开发的某一阶段，<a href="https://www.python.org/dev/peps/pep-3141" target="_blank" rel="noopener">PEP3141</a>为复数对象引入了一些基类<code>MonoidUnderPlus</code>,<code>AdditiveGroup</code>, <code>Ring</code>, <code>Field</code>, <code>Complex</code>，其中每一个都是从前一个所推导出的。也有很多被讨论过但是没有实现的代数分类比如：<code>Algebraic</code>, <code>Transcendental</code>,<code>IntegralDomain</code>, <code>PrincipalIdealDomain</code>。在当前PEP的早期版本，我们考虑过单独类的用例比如：<code>Set</code>, <code>ComposableSet</code>, <code>MutableSet</code>, <code>HashableSet</code>, <code>MutableComposableSet</code>, <code>HashableComposableSet</code>。</p>
<p>我们遇到了一个困境：我们希望ABC的数量更少，但是如果一个用户希望使用一个限定不那么细化的ABC怎么办呢？这问题就像是一个数学家希望定义一种很抽象的数字，但同时希望浮点数和整数被包含在这种抽象类型中。<a href="https://www.python.org/dev/peps/pep-3141" target="_blank" rel="noopener">PEP3141</a>最初提出了对<code>float.__bases__</code>的修改方案来解决这个问题，但是通常我们希望保持内置类型的不变型，因此舍弃了这个方案。</p>
<p>再举一个例子，假如有一个人希望定义一个泛型函数<a href="https://www.python.org/dev/peps/pep-3124" target="_blank" rel="noopener">PEP3124</a>，这个函数可以作用于所有包含<code>append()</code>方法的序列中。现在Sequence的ABC并不承诺包含<code>append()</code>方法，但是MutableSequence的ABC不仅仅承诺了<code>append()</code>方法，还承诺了很多其他方法。</p>
<p>为了解决诸如此类的问题，下一节会提出一个<code>元类</code>（meta-class），将其与ABC一同使用可以创建新的ABC，这些新创建的ABC可以作为任何类（包括ABC类本身）的虚拟基类（<strong>与C++中的虚拟基类概念并不相同</strong>）。这就允许了标准库定义Sequence和MutableSequence这两个ABC，并且把它们注册为相应的内置类型的虚拟基类。下面的语句都会返回<code>True</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">isinstance([], Sequence)</span><br><span class="line">issubclass(list, Sequence)</span><br><span class="line">issubclass(list, MutableSequence)</span><br><span class="line">isinstance((), Sequence)</span><br><span class="line"><span class="keyword">not</span> issubclass(tuple, MutableSequence)</span><br><span class="line">isinstance(<span class="string">""</span>, Sequence)</span><br><span class="line">issubclass(bytearray, MutableSequence)</span><br></pre></td></tr></table></figure>
<p>这里主要提出的原理就是重载内置函数<code>isinstance()</code>和<code>issubclass()</code>。重载的过程：调用<code>isinstance(x,C)</code>后首先检查<code>C.__instancecheck__</code>方法是否存在，若存在就调用<code>C.__instancecheck__(x)</code>而不是通用实现。类似的，<code>issubclass(D,C)</code>也采用类似的检查过程。</p>
<p>这里要注意的是对应的魔术方法名字并非<code>__isinstance__</code>，这是因为在使用中可能会导致歧义。</p>
<h3 id="3-2-abc模块"><a href="#3-2-abc模块" class="headerlink" title="3.2 abc模块"></a>3.2 abc模块</h3><p>abc是新的标准库，使用纯Python实现，用来支持ABC。它定义了一个元类<code>ABCMeta</code>和两个装饰器<code>@abstractmethod</code>和<code>@abstractproperty</code>。</p>
<h4 id="ABCMeta"><a href="#ABCMeta" class="headerlink" title="ABCMeta"></a>ABCMeta</h4><p>ABCMeta类重载了<code>__instancecheck__</code>和<code>__subclasscheck__</code>两个魔术方法。还定义了一个<code>register</code>方法，该方法接受一个类作为参数，在调用<code>B.register(C)</code>之后，<code>issubclass(C, B)</code>即<code>B.__subclasscheck(C)</code>返回True。<code>isinstance(x, B)</code>和<code>issubclass(x.__class__, B)</code>或者<code>issubclass(type(x), B)</code>是等价的。（<code>type(x)</code>和<code>x.__class__</code>可能不等价，比如x是一个代理对象）但是，使用register注册后的虚拟子类无法通过super方式调用虚拟基类的方法，虚拟基类的方法也不会出现在子类的<code>方法解析顺序</code><a href="{filename}mro.md">mro</a>列表中。</p>
<h4 id="abstractmethod"><a href="#abstractmethod" class="headerlink" title="@abstractmethod"></a>@abstractmethod</h4><p>abc模块还定义了<code>@abstractmethod</code>装饰器，用来声明抽象方法。声明了抽象方法的类需要保证子类将抽象方法全都重载否则无法实例化。</p>
<ul>
<li>注意：该装饰器只能在类的主体中使用，并且只能用于元类是ABCMeta及其子类的类。动态添加类的抽象方法或者修改类的方法的抽象状态的操作不被支持。该装饰器只影响正常继承的子类，使用<code>register()</code>方法注册的虚拟子类不受影响。</li>
</ul>
<ul>
<li>实现：该装饰器将函数的<code>__isabstractmethod__</code>属性设为True。<code>ABCMeta.__new__</code>方法计算属性<code>__abstractmethods__</code>，该属性是所有<code>__isabstractmethod__ = True</code>的方法的名字集合。首先将基类的<code>__abstractmethods__</code>取并集，然后根据派生类方法的<code>__isabstractmethod__</code>对该属性进行更新。若类的<code>__abstractmethods__</code>不为空，则该类为抽象类，实例化该类抛出<code>TypeError</code>。（CPython的实现中，使用<code>Py_TPFLAGS_ABSTRACT</code>加速检查过程）</li>
<li>讨论：不同于Java中的抽象方法或者C++里的纯虚函数，Python的抽象方法可能有实现，而且父类实现的抽象方法可以被重载该方法的子类通过super机制调用。这个机制在多继承中很有用。</li>
</ul>
<h4 id="abstractproperty"><a href="#abstractproperty" class="headerlink" title="@abstractproperty"></a>@abstractproperty</h4><p><code>@abstractproperty</code>装饰器用来定义抽象的数据属性，它的实现是一个内置类property的子类，增加了一个<code>__isabstractmethod__</code>属性。该装饰器可以使用两种方式使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span><span class="params">(metaclass=ABCMeta)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个只读的抽象属性:</span></span><br><span class="line"><span class="meta">    @abstractproperty</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">readonly</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__x</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建一个可读写的抽象属性:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getx</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__x</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setx</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self.__x = value</span><br><span class="line">    x = abstractproperty(getx, setx)</span><br></pre></td></tr></table></figure>
<p>同抽象方法类似，子类必须重载抽象属性才能被实例化。Python3.3之后，上述创建抽象属性的方式被废弃，采用下面的方式使用内置的<code>@property</code>装饰器和<code>@abstractmethod</code>配合即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span><span class="params">(metaclass=ABCMeta)</span>:</span></span><br><span class="line">    <span class="comment"># python3.3之后</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">x</span><span class="params">(self)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="meta">    @x.setter</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">x</span><span class="params">(self, val)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span><span class="params">(C)</span>:</span></span><br><span class="line"><span class="meta">    @C.x.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">x</span><span class="params">(self, val)</span>:</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<h3 id="3-3-容器和迭代器的ABC"><a href="#3-3-容器和迭代器的ABC" class="headerlink" title="3.3 容器和迭代器的ABC"></a>3.3 容器和迭代器的ABC</h3><p>collections模块中定义了一些必要的ABC来支持集合、映射、序列、迭代器等的功能，所有这些ABC的元类都是ABCMeta。</p>
<ol>
<li>这些ABC实现了其抽象方法，但是这些实现没什么用。比如<code>__hash__</code>返回0，<code>__iter__</code>返回一个空的迭代器。总之，抽象方法代表了相应类型的空容器的行为。</li>
<li>其中一些ABC也实现了具体方法，比如<code>Iterator</code>类的<code>__iter__</code>方法返回自身实例。这些ABC可以被看做是“混合类”。</li>
<li>所有这些ABC都没有重载<code>__init__</code>,<code>__new__</code>,<code>__str__</code>和<code>__repr__</code>。因为一个标准的构造函数可能会限制自定义的容器类型，而容器的具体字符表示可以由具体的实现来完成。</li>
<li>所有ABC都没有定义比较操作<code>__lt__,__gt__,__le__,__ge__</code>，这是因为若抽象父类order定义了该操作方法，那么可能存在两个实例x和y，x&lt;y。假如list和str都由order继承而来，那么隐含着<code>[1,]&lt;&quot;3&quot;</code>，而Python中对于混合类型的比较是禁止的，见<a href="https://www.python.org/dev/peps/pep-3100" target="_blank" rel="noopener">PEP-3100</a>。</li>
</ol>
<h4 id="一招鲜"><a href="#一招鲜" class="headerlink" title="一招鲜"></a>一招鲜</h4><p>这些抽象类只有一个方法。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">抽象类</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Hashable</td>
<td style="text-align:left">具有一个抽象方法<code>__hash__</code>，返回0。若两个类o1和o2同时继承自该类，那么<code>o1==o2 &lt;-&gt; hash(o1) == hash(o2)</code>。若一个类派生自Hashable，那么该类的hash值应当始终不变。只有不可变对象才可以<code>hash()</code>，比如元组中包含列表对象就无法哈希。</td>
</tr>
<tr>
<td style="text-align:center">Iterable</td>
<td style="text-align:left">定义<code>__iter__</code>方法，返回Iterator的实例。</td>
</tr>
<tr>
<td style="text-align:center">Iterator</td>
<td style="text-align:left">定义<code>__next__</code>的基类，具体方法<code>__iter__</code>返回其实例。</td>
</tr>
<tr>
<td style="text-align:center">Sized</td>
<td style="text-align:left">定义<code>__len__</code>的基类，返回一个&gt;=0的整数。若一个类是Sized和Iterable的子类，那么对于该类的所有实例c有不变式：<code>sum(1 for x in c) == len(c)</code></td>
</tr>
<tr>
<td style="text-align:center">Container</td>
<td style="text-align:left">定义<code>__contains__</code>的基类，返回布尔对象。如果一个类是Container和Iterable的子类，那么对于该类的所有实例c有：<code>(x in c for x in c)</code>中仅生成True元素。</td>
</tr>
</tbody>
</table>
</div>
<h4 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h4><p>集合是一个有长度的可迭代容器，也就是Sized,Iterable和Container的子类，而且保证每个元素只出现一次。</p>
<p>集合的比较是安全的，但是不同的集合之间只有偏序关系，比如对于{1,2}和{1,3}，小于、等于和大于三种操作都返回False。集合无法同映射或者序列进行比较，但是可以比较元素是否完全相同。</p>
<p>集合类还定义了具体操作来计算交集、并集、异或或者差集等操作，返回集合的实例。</p>
<p>集合类定义了具体的<code>__hash__</code>方法，计算每一个元素的hash值。</p>
<h4 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h4><p>这些抽象类代表只读映射或者是可变映射。内置类型dict是从<code>MutableMapping</code>中继承的子类。遍历key,value和item应该返回相同顺序的结果。</p>
<h4 id="序列"><a href="#序列" class="headerlink" title="序列"></a>序列</h4><p>这些抽象类代表只读序列和可变序列。内置类型list和bytes是从<code>MutableSequence</code>中继承的子类。内置类型tuple和str是从<code>Sequence</code>和<code>Hashable</code>中继承的子类。</p>
<h4 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h4><p>Python3至少包含两种内置字符串类型：字节型的byte(MutableSequence)和unicode类型的str(Sequence,Hashable)，<a href="https://www.python.org/dev/peps/pep-0358" target="_blank" rel="noopener">PEP358</a>有更多关于byte类型的内容。</p>
<h3 id="3-4-ABC-vs-其他方法"><a href="#3-4-ABC-vs-其他方法" class="headerlink" title="3.4 ABC vs. 其他方法"></a>3.4 ABC vs. 其他方法</h3><p>比较ABC和其他方式的差异和优缺点。</p>
<h4 id="ABC-vs-鸭子类型"><a href="#ABC-vs-鸭子类型" class="headerlink" title="ABC vs. 鸭子类型"></a>ABC vs. 鸭子类型</h4><p>ABC引入后，鸭子类型就不再需要了吗？并不是的。Python并不要求一个实现了<code>__getitem__</code>方法的类继承BasicMapping或者Sequence基类，同时x[y]这样的语法也并不要求x是上述基类的子类的实例。你还是能够分配任何看上去像是文件的对象到sys.stdout，只要该对象包含write方法。</p>
<p>当然，继承特定的基类还是有些好处的，诸如：一些功能有默认实现或者可以分辨一个类是映射还是序列。但是这不是固定的，如果<code>hasattr(x, &#39;__len__&#39;)</code>够你用了，那么太好啦~ABC只是尝试解决Python2中没有好办法解决的问题，像是分辨映射和序列之类的。</p>
<h4 id="ABC-vs-泛型函数"><a href="#ABC-vs-泛型函数" class="headerlink" title="ABC vs. 泛型函数"></a>ABC vs. 泛型函数</h4><p>ABC和泛型函数GF是兼容的。比如，我（GvR）自己实现的泛型函数使用参数中的类作为分发的key，并允许派生类重载基类。因为从Python的角度看来ABC也是正常的类，所以在GF的默认实现中使用ABC是很适合的。比如，如果我重载了<code>prettyprint</code>函数，那么可能会像下面这样定义其set版本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@prettyprint.register(Set)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pp_set</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"&#123;"</span> + ... + <span class="string">"&#125;"</span></span><br></pre></td></tr></table></figure>
<p>Set特定子类的实现能够很容易的添加。当然，ABC在Phillip Eby的泛函实现中应用也不会有什么问题。</p>
<p>当然，泛函的支持者可能会说有泛函（以及类和实现）就够了。但是他们也必须承认继承的作用，这个PEP中提出的ABC是基类的一种实现选择；而且并非所有用户定义的映射都要从<code>BasicMapping</code>继承。</p>
<h4 id="ABC-vs-接口"><a href="#ABC-vs-接口" class="headerlink" title="ABC vs. 接口"></a>ABC vs. 接口</h4><p>ABC本质上和接口还是兼容的，但是其功能被认为有重合之处。现在起，我希望接口的支持者能够解释为啥接口更好使。希望这些工作能够进展到，比如说定义很多描述映射的接口并且命名方式能够让未来有一个PEP使用可以方便的使用它们代替ABC。</p>
<p>接口在这里指代一些PEP的集合，是一种特殊的类包含一些元数据属性，而这个类并不在常规的类继承结构中，但是允许某些类型的继承测试。</p>
<p>这些元数据应当经过精细的设计，从而能够被应用程序简单的修改，允许开发者覆盖一个对象的分类。</p>
<p>将可变的元数据附加到类中，这种方式的缺点在于类处于一种共享状态，修改它们可能导致冲突。而且，覆盖对象的分类可以用泛型函数更好的完成：比如在基类中定义一个泛函“分类成员”并返回False，然后在感兴趣的子类中重载该函数并返回True。</p>
<h2 id="四、引用"><a href="#四、引用" class="headerlink" title="四、引用"></a>四、引用</h2><ul>
<li><a href="https://www.python.org/dev/peps/pep-3119/#references" target="_blank" rel="noopener">PEP-3119</a> </li>
<li><a href="https://docs.python.org/3/" target="_blank" rel="noopener">Python-Doc</a></li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>PEP</tag>
      </tags>
  </entry>
  <entry>
    <title>Python生成器和协程</title>
    <url>/2016/10/03/python/Python%E7%94%9F%E6%88%90%E5%99%A8%E5%92%8C%E5%8D%8F%E7%A8%8B/</url>
    <content><![CDATA[<h4 id="一、Python生成器和迭代器"><a href="#一、Python生成器和迭代器" class="headerlink" title="一、Python生成器和迭代器"></a>一、Python生成器和迭代器</h4><p>在Python中诸如<code>str,list,tuple,dict,set</code>等很多类型都是可以迭代的，支持如下的语法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ch <span class="keyword">in</span> <span class="string">'python iterable'</span>:</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>在解释器内部，当迭代序列对象时，会通过内置方法<code>iter</code>尝试将该对象转化为一个迭代器，该对象必须：实现了<code>__iter__</code>内置方法返回一个迭代器（Iterator，要求实现<code>__next__</code>方法），或者，实现了<code>__getitem__</code>方法且接受0开始的整形参数。参考<a href="https://www.python.org/dev/peps/pep-0234/" target="_blank" rel="noopener">PEP 234 — Iterators</a>。转化后的迭代器对象通过next方法调用下一个，迭代到序列的最后一个元素后再次调用next方法会抛出StopIteration异常（在for…in…语法中自动处理）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = iter([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">&lt;list_iterator at <span class="number">0x26ee71739b0</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(a)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(a)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(a)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(a)</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">StopIteration:</span><br></pre></td></tr></table></figure>
<p>需要注意的是，使用第二种方式<code>__getitem__</code>实现可迭代的对象，并不符合相应抽象基类的要求，如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> Iterable,Iterator</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(list, Iterable)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(list, Iterator)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(iter([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]), Iterable)</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(iter([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]), Iterator)</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>生成器是另一种可迭代对象，可以使用生成器函数（包含yield语句的函数）或者生成器表达式（列表推导式中括号由小括号代替）产生，创建后每次调用<code>next</code>方法执行，每次执行停留在yield语句处，并返回yield关键字后的表达式的值。执行结束返回stopIteration异常（和迭代器一样）。参考<a href="https://www.python.org/dev/peps/pep-0255/" target="_blank" rel="noopener">PEP 255 — Simple Generators</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countdown</span><span class="params">(n)</span>:</span></span><br><span class="line">  <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">yield</span> n</span><br><span class="line">  n -= <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> x <span class="keyword">in</span> countdown(<span class="number">3</span>):	print(x)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = (<span class="number">2</span>*x <span class="keyword">for</span> x <span class="keyword">in</span> a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">&lt;generator object at <span class="number">0x58760</span>&gt;</span><br></pre></td></tr></table></figure>
<p>生成器函数能够配合contextmanager装饰器实现一个上下文管理器，而无需实现一个拥有<code>__enter__</code>和<code>__exit__</code>方法的类，如下面示例所示，tempdir函数和tempdir类拥有相同的功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tempfile, shutil</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</span><br><span class="line"></span><br><span class="line"><span class="meta">@contextmanager</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tempdir</span><span class="params">()</span>:</span></span><br><span class="line">  dirname = tempfile.mkdtemp()</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">  	<span class="keyword">yield</span> dirname</span><br><span class="line">  <span class="comment"># yield以上的部分是__enter__，yield产出的值是返回值</span></span><br><span class="line">  <span class="keyword">finally</span>:</span><br><span class="line">  	shutil.rmtree(dirname)</span><br><span class="line">  <span class="comment"># yield以下的部分是__exit__</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">tempdir</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">  	self.dirname = tempfile.mkdtemp()</span><br><span class="line">  	<span class="keyword">return</span> self.dirname</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, exc, val, tb)</span>:</span></span><br><span class="line">  	shutil.rmtree(self.dirname)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tempdir() <span class="keyword">as</span> dirname:</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<h4 id="二、Python协程"><a href="#二、Python协程" class="headerlink" title="二、Python协程"></a>二、Python协程</h4><p>通过yield关键字，实现了生成器，允许生成器向调用方发送消息。而在<a href="https://www.python.org/dev/peps/pep-0342/" target="_blank" rel="noopener">PEP-342</a>中，通过将yield变成表达式并增加send等方法，调用方可以向生成器发送消息。通过这个PEP，Python实现了协程。</p>
<p>yield表达式的赋值对象是通过send发送到协程的值，而yield后面的表达式（可以没有）是此次协程返回给调用方的值。</p>
<p>调用下面的printer协程函数会实例化一个生成器对象，但不会开始执行函数体内的语句。该对象每次执行时会在yield处停止，并把yield右边的表达式返回给调用者（这个函数每次执行会返回True），而后在下次调用者调用send(x)方法的时候，将x的值赋给line并继续执行到下一个yield处。函数体执行到最后会抛出StopIteration异常，并把协程的返回值赋值给异常的Value。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printer</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      line = (<span class="keyword">yield</span> <span class="literal">True</span>)<span class="comment"># 每次调用者发送的值赋给line</span></span><br><span class="line">      print(line)</span><br><span class="line">    <span class="keyword">except</span> GeneratorExit:</span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">10</span></span><br></pre></td></tr></table></figure>
<ul>
<li>在生成器或者协程实例化后，需要进行预激（prime or advance），即调用<code>next</code>或者<code>send(None)</code>方法，让其执行到第一个yield语句处。</li>
<li>协程需要响应close方法，即增加对GeneratorExit异常的处理。</li>
<li>协程可以很方便的实现Event Handler（观察者模式）的模型。</li>
</ul>
<p>生成器和协程虽然有相似之处，但本质上是<strong>不同的概念</strong>，生成器目的在于惰性地产出数据以用于迭代，而协程倾向于消费数据。</p>
<p>不同的协程函数之间可以连接起来，就像Unix系统中的pipe一样，从最外层调用者开始，每个协程接收其调用者发送的数据，进行处理，传递（或者不传递）到自己右侧的下一个协程。<strong>David Beazley</strong>在<a href="http://www.dabeaz.com/coroutines/" target="_blank" rel="noopener">A Curious Course on Coroutines and Concurrency</a>里以XML解析处理为例展示了协程在这方面的用法，调用的代码如下所示，其中<code>buses_to_dicts,filter_on_field,bus_locations</code>函数都是协程函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xml.sax.parse(<span class="string">"allroutes.xml"</span>,</span><br><span class="line">              EventHandler(</span><br><span class="line">                   buses_to_dicts(</span><br><span class="line">                   filter_on_field(<span class="string">"route"</span>,<span class="string">"22"</span>,</span><br><span class="line">                   filter_on_field(<span class="string">"direction"</span>,<span class="string">"North Bound"</span>,</span><br><span class="line">                   bus_locations())))</span><br><span class="line">              ))</span><br></pre></td></tr></table></figure>
<p>同时，协程也可以和线程或者进程结合使用，但是多个线程或者进程向同一个协程send数据的时候需要做好同步。</p>
<p>由于协程具有独立的控制流，内部的独立状态，可以通过send和yield返回值相互交互，可以被调度器暂停（通过yield）、启动执行（通过send）以及结束（通过close），因此也可以把协程函数当做是一个Task。</p>
<h4 id="三、子生成器"><a href="#三、子生成器" class="headerlink" title="三、子生成器"></a>三、子生成器</h4><p>这一节没有区分生成器和协程。</p>
<p>生成器对象有下面这些操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gen = generator()</span><br><span class="line">next(gen) <span class="comment"># Advance to next yield</span></span><br><span class="line">gen.send(item) <span class="comment"># Send an item</span></span><br><span class="line">gen.close() <span class="comment"># Terminate</span></span><br><span class="line">gen.throw(exc, val, tb) <span class="comment"># Raise exception</span></span><br><span class="line">result = <span class="keyword">yield</span> <span class="keyword">from</span> gen <span class="comment"># Delegate</span></span><br></pre></td></tr></table></figure>
<p>其中第6个操作，yield from语法，在<a href="https://www.python.org/dev/peps/pep-0380/" target="_blank" rel="noopener">PEP380</a>提出，Python3.3实现，适用于在生成器A内部包含子生成器B的情况下，方便的将消息在调用方和子生成器B之间传递。</p>
<h4 id="四、Async-amp-Await"><a href="#四、Async-amp-Await" class="headerlink" title="四、Async &amp; Await"></a>四、Async &amp; Await</h4><p><a href="https://www.python.org/dev/peps/pep-0492" target="_blank" rel="noopener">PEP492</a>中提出了一种应用在Python3.5以上版本的语法，用于定义异步协程，定义的方式如下，通过这种方式定义的协程不再是Generator类型，而是native coroutine类型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">()</span>:</span></span><br><span class="line">  time.sleep(<span class="number">10</span>)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">()</span>:</span></span><br><span class="line">  data = <span class="keyword">await</span> read_data()</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>原有的建立协程的方法建立的对象被称为’generator-based coroutines’。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li><a href="https://book.douban.com/subject/26278021/" target="_blank" rel="noopener">Fluent-Python</a></li>
<li><a href="http://www.dabeaz.com/coroutines/" target="_blank" rel="noopener">A Curious Course on Coroutines and Concurrency</a></li>
<li><a href="http://www.dabeaz.com/finalgenerator/" target="_blank" rel="noopener">Generators: The Final Frontier</a></li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>PEP</tag>
        <tag>Iterator</tag>
        <tag>Coroutine</tag>
        <tag>Generator</tag>
      </tags>
  </entry>
  <entry>
    <title>探究python的GIL</title>
    <url>/2016/04/06/python/%E6%8E%A2%E7%A9%B6python%E7%9A%84GIL/</url>
    <content><![CDATA[<h3 id="一、全局解释器锁"><a href="#一、全局解释器锁" class="headerlink" title="一、全局解释器锁"></a>一、全局解释器锁</h3><p><code>GIL</code>是<em>CPython</em>解释器中的线程全局锁，今天我们来说一说它。GIL由于历史原因而存在，在之后的CPython中也应该会继续存在下去。</p>
<p>GIL能够保证解释器进程中同时仅有一个线程执行，不允许多线程并行执行，简化了内存管理等底层细节。但它导致了Python多线程程序的执行效率问题，在多核CPU环境下和计算密集型任务中这个问题尤其严重。</p>
<p>Python的线程就是操作系统的线程(POSIX thread || Win thread)，线程调度也直接使用操作系统的线程调度。</p>
<h3 id="二、GIL原理"><a href="#二、GIL原理" class="headerlink" title="二、GIL原理"></a>二、GIL原理</h3><p>运行中的线程持有GIL，当线程遇到I/O操作时，释放GIL。如果是CPU密集型任务，则在一定间隔后进行检查，默认是100ticks，其中一个<code>tick</code>是若干条Python解释器指令。</p>
<p>Python提供了一种用于线程同步的锁，它不是简单的互斥锁，而是由互斥锁和条件变量构成的二进制信号量。GIL是该锁的一个实例。<a href="http://blog.csdn.net/lovecodeless/article/details/24929273" target="_blank" rel="noopener">了解条件变量相关戳我~</a>GIL的释放和持有请求过程如下伪代码所示：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//释放GIL过程</span></span><br><span class="line"><span class="built_in">release</span>()&#123;</span><br><span class="line">    mutex.acquire()<span class="comment">//互斥锁</span></span><br><span class="line">    locked = <span class="number">0</span>    <span class="comment">//状态</span></span><br><span class="line">    mutex.<span class="built_in">release</span>()</span><br><span class="line">    cond.signal()<span class="comment">//通知条件变量队列中第一个线程，进入操作系统Ready队列</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//持有GIL过程</span></span><br><span class="line">acquire()&#123;</span><br><span class="line">    mutex.acquire()</span><br><span class="line">    whiled(locked)&#123;</span><br><span class="line">        cond.wait(mutex)<span class="comment">//进入条件变量等待队列</span></span><br><span class="line">    &#125;</span><br><span class="line">    locked = <span class="number">1</span></span><br><span class="line">    mutex.<span class="built_in">release</span>()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于CPU密集型的多线程应用，在单核和多核环境下利用修改Python源码的方式测试其执行效率<sup><a href="#fn_note1" id="reffn_note1">note1</a></sup>。</p>
<blockquote id="fn_note1">
<sup>note1</sup>. 来自<code>UnderstandingGIL.pdf</code><a href="#reffn_note1" title="Jump back to footnote [note1] in the text."> &#8617;</a>
</blockquote>
<p>单核环境下，它具有不错的性能，线程交替的执行，并且线程切换的频率较低。<br><img src="/images/FkQnydjsDHzHElNKhc4lTxJiQxRZ.jpg" alt=""></p>
<p>但是多核环境下，就会产生灾难性的后果。运行中的线程1在执行一定指令后唤醒条件变量队列中的等待线程2，但是由于两个线程都处于Ready状态，因此操作系统调度后可能仍然运行线程1。这个过程可能反复进行数百次，时间开销就会很大。<br><img src="/images/FjnHSqT-dMRTJ018rm2Co3sGofAi.jpg" alt=""></p>
<p>即使是I/O密集型的应用，由于缓冲区的存在，I/O操作可能不会阻塞，但是却要反复释放GIL造成GIL颠簸，开销也不小。</p>
<h3 id="三、消除GIL的努力"><a href="#三、消除GIL的努力" class="headerlink" title="三、消除GIL的努力"></a>三、消除GIL的努力</h3><p>Python1.5曾有Patch努力消除GIL，但是由于降低了单线程的执行效率、对两个以上的线程效果差、Python大量的库需要重写等原因，该方案最后没有成功。下面的链接是Python的作者Guido对去除GIL的态度。</p>
<p><a href="http://www.artima.com/weblogs/viewpost.jsp?thread=214235" target="_blank" rel="noopener">Guido:It isn’t Easy to Remove the GIL</a></p>
<h3 id="四、改进GIL"><a href="#四、改进GIL" class="headerlink" title="四、改进GIL"></a>四、改进GIL</h3><p>Python3.2之后有了新的GIL机制（2009年by<code>Antoine Pitrou</code>），主要的目的是减轻GIL的颠簸，在这里<a href="https://mail.python.org/pipermail/python-dev/2009-October/093321.html" target="_blank" rel="noopener">Reworking the GIL</a>有对这些改进的阐述。当然下面也有。</p>
<p>不使用tick作为计算密集型线程的释放计量，使用全局变量<code>gil_drop_request</code>。它的初始值为0，运行的线程若不主动释放GIL则一直运行到该变量值变为1。这样线程切换的时间更平稳更加可预计。</p>
<p>等待队列中的线程睡眠一个时间（默认5ms），若该时间间隔内运行线程不主动释放GIL，则将变量设为1并继续睡眠，这样运行线程将进入暂停状态（避免该线程被立刻调度运行），并引发一次操作系统的线程调度（继续运行的并不一定是提出timeout的线程）。如下图所示：</p>
<p><img src="/images/FpNr8FSnah6gqanrBsFEJL7h3pQU.jpg" alt=""></p>
<p>但是，新的GIL也带来了诸如响应时间长等问题（护航效应），比如I/O密集线程释放GIL后，一个计算密集线程获取GIL，那么I/O线程至少需要等待5ms才能再次运行，再次运行5ms后又要释放GIL，这样响应时间就会很长。比如这里给出的这个实验代码<a href="http://dabeaz.blogspot.sg/2010/02/revisiting-thread-priorities-and-new.html" target="_blank" rel="noopener">GIL-with-priorities</a>。但是这里有一个疑问，这里说明这个带有线程优先级的GIL在python3.2开发了，为什么我的环境Python3.4在执行时双线程反而效率降低，这个问题留待探究。</p>
<h3 id="五、避免GIL带来的缺陷"><a href="#五、避免GIL带来的缺陷" class="headerlink" title="五、避免GIL带来的缺陷"></a>五、避免GIL带来的缺陷</h3><p>也存在几种现有方案来避免GIL的缺陷。当然，这些替代方案也有它们各自的问题。</p>
<ul>
<li><p>1.使用multiprocessing替代Thread，多进程通信和切换开销较大</p>
</li>
<li><p>2.用其他的解释器实现，比如Jython或者PyPy，可用的库少</p>
</li>
</ul>
<h3 id="试验"><a href="#试验" class="headerlink" title="试验"></a>试验</h3><p>在4核i5-4210U处理器上进行了简单测试，使用<code>Python3.4</code>，结果说明新的GIL解决了GIL原有的一些效率问题：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""testgil"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeit</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="string">"""time it decorator"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">new_func</span><span class="params">(*args, **args2)</span>:</span></span><br><span class="line">        <span class="string">"""wrapper"""</span></span><br><span class="line">        t_0 = time.time()</span><br><span class="line">        print(<span class="string">"@%s, &#123;%s&#125; start"</span> % (time.strftime(<span class="string">"%X"</span>, time.localtime()), func.__name__))</span><br><span class="line">        back = func(*args, **args2)</span><br><span class="line">        print(<span class="string">"@%s, &#123;%s&#125; end"</span> % (time.strftime(<span class="string">"%X"</span>, time.localtime()), func.__name__))</span><br><span class="line">        print(<span class="string">"@%.3fs taken for &#123;%s&#125;"</span> % (time.time() - t_0, func.__name__))</span><br><span class="line">        <span class="keyword">return</span> back</span><br><span class="line">    <span class="keyword">return</span> new_func</span><br><span class="line"></span><br><span class="line"><span class="meta">@timeit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="string">"""count func"""</span></span><br><span class="line">    <span class="keyword">while</span> num &gt; <span class="number">0</span>:</span><br><span class="line">        num -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_thread</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""multi thread test"""</span></span><br><span class="line">    t_1 = Thread(target=count, args=(<span class="number">100000000</span>,))</span><br><span class="line">    t_1.start()</span><br><span class="line">    t_2 = Thread(target=count, args=(<span class="number">100000000</span>,))</span><br><span class="line">    t_2.start()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single_thread</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""single thread test"""</span></span><br><span class="line">    t_1 = Thread(target=count, args=(<span class="number">200000000</span>,))</span><br><span class="line">    t_1.start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># single_thread()</span></span><br><span class="line">    multi_thread()</span><br></pre></td></tr></table></figure>
<p>测试结果如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>程序</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>单线程</td>
<td>17.927s</td>
</tr>
<tr>
<td>双线程</td>
<td>17.435s</td>
</tr>
</tbody>
</table>
</div>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://www.dabeaz.com/python/UnderstandingGIL.pdf" target="_blank" rel="noopener">UnderstandingGIL</a></p>
<p><a href="http://www.dabeaz.com/python/NewGIL.pdf" target="_blank" rel="noopener">InsideNewGIL</a></p>
<p><a href="https://docs.python.org/3/" target="_blank" rel="noopener">Python3.5.1文档</a></p>
<p><a href="http://cenalulu.github.io/python/gil-in-python/" target="_blank" rel="noopener">Python中的GIL是什么</a></p>
<p><a href="http://www.dabeaz.com/GIL/" target="_blank" rel="noopener">可视化GIL影响</a></p>
<p><a href="#">看完了，返回顶部吧</a></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python面向对象：方法解析顺序MRO</title>
    <url>/2016/04/03/python/python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%EF%BC%9A%E6%96%B9%E6%B3%95%E8%A7%A3%E6%9E%90%E9%A1%BA%E5%BA%8FMRO/</url>
    <content><![CDATA[<h3 id="Python的方法解析顺序"><a href="#Python的方法解析顺序" class="headerlink" title="Python的方法解析顺序"></a>Python的方法解析顺序</h3><p>对于支持继承的编程语言来说，其方法可能定义在当前类，也可能来自于基类，所以在方法调用时就需要对当前类和基类进行搜索以确定方法所在的位置。而搜索的顺序就是所谓的「方法解析顺序」（Method Resolution Order，<code>MRO</code>）。对于Python这样支持多重继承的语言来说，计算MRO的方法比仅支持单继承的语言要复杂。</p>
<p>Python使用的MRO方式在2.2版本之前和之后是不同的，之前的版本使用一种<code>从左向右的深度优先遍历</code>方式解析，而这种方式在菱形继承的情况下会导致错误的解析结果。因此在Python2.3之后采用了一种称为<code>C3</code>的方式来计算MRO。以如下的继承结构来说明C3的解析过程。</p>
<p><img src="/images/FjRUUf7Vund6U9hVs1z4KlVVqCkY.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mro(A) &#x3D; [A,O]</span><br><span class="line">mro(B) &#x3D; [B,A,O]</span><br><span class="line">mro(C) &#x3D; [C,A,O]</span><br><span class="line">mro(D) &#x3D; [D,B,A,O]</span><br><span class="line">mro(E) &#x3D; [E,C,A,O]</span><br><span class="line">mro(F) &#x3D; [F] + merge( mro(D) , mro(E) , [D,E] )</span><br></pre></td></tr></table></figure>
<p>使用下面的规则来化简上面类F的mro：</p>
<blockquote>
<p>1.在merge列表中，如果第一个mro的<code>第一个类</code>出现在其它序列，并且也是<code>第一个</code>，或者<code>不在其它序列中出现</code>，那么这个类就会从这些序列中删除，合并到访问顺序列表中.</p>
<p>2.若第一个mro无法继续找到符合规则1的类，则从<code>下一个mro</code>中继续按照规则1搜索.</p>
<p>3.重复直到<code>列表为空</code>或者<code>无法输出</code>，若列表为空则此时访问顺序列表生成完毕；否则，说明无法构建继承关系.</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mro(F) &#x3D; [F] + merge( [D,B,A,O] , [E,C,A,O] , [D,E] )</span><br><span class="line">       &#x3D; [F,D] + merge( [B,A,O] , [E,C,A,O] , [E] )</span><br><span class="line">       &#x3D; [F,D,B] + merge( [A,O] , [E,C,A,O] , [E] )</span><br><span class="line">       &#x3D; ....</span><br><span class="line">       &#x3D; [F,D,B,E,C,A,O]</span><br></pre></td></tr></table></figure>
<p>类的方法解析顺序可以使用类的mro方法进行验证，如下所示。</p>
<p><img src="/images/FodC95GYgDbxvHdA7TClki2XWTZ9.jpg" alt=""></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>OOP</tag>
      </tags>
  </entry>
  <entry>
    <title>变分推断（一）：介绍变分推断</title>
    <url>/2018/02/28/vi/VI-1/</url>
    <content><![CDATA[<h2 id="变分推断（一）：介绍变分推断"><a href="#变分推断（一）：介绍变分推断" class="headerlink" title="变分推断（一）：介绍变分推断"></a>变分推断（一）：介绍变分推断</h2><h3 id="一、近似推断问题"><a href="#一、近似推断问题" class="headerlink" title="一、近似推断问题"></a>一、近似推断问题</h3><p>马尔科夫蒙特卡洛（MCMC）采样是近似推断（Approximate Inference）的一种重要方法，其改进包括Metropolis-Hastings算法，Gibbs采样。</p>
<p>在MCMC不满足性能要求的时候，我们使用变分推断（Variational Inference，VI）。变分推断通过优化逼近后验，而MCMC通过采样逼近后验。</p>
<p>对观测数据<script type="math/tex">x = (x_1,\ldots,x_n)</script>和隐变量<script type="math/tex">z = (z_1,\ldots,z_m)</script>，推断问题（inference）希望计算出隐变量的后验概率<script type="math/tex">p(z|x)</script>，利用贝叶斯公式重写后验概率如下公式。在确定隐变量z的先验概率<script type="math/tex">p(z)</script>时，公式中的联合概率<script type="math/tex">p(z,x)</script>能够计算，而<script type="math/tex">p(x)</script>需要积分<script type="math/tex">p(x) = \int p(z,x) dz</script>计算，但是通常，这个积分没有解析解，利用采样的方法计算效率也非常低。</p>
<script type="math/tex; mode=display">
p(z|x) =\frac{p(z,x)}{p(x)}</script><h3 id="二、变分分布和ELBO"><a href="#二、变分分布和ELBO" class="headerlink" title="二、变分分布和ELBO"></a>二、变分分布和ELBO</h3><p>变分推断选择一类分布族Q（称为变分分布族），将推断问题转换为优化<script type="math/tex">q^*\in Q</script>使得与后验<script type="math/tex">p(z|x)</script>的差别最小。</p>
<script type="math/tex; mode=display">
q^*(z) = argmin_{q(z)\in Q } KL[q(z)||p(z|x)]</script><p>变分推断使用<script type="math/tex">KL[q||p]</script>衡量变分分布（即分布q）和真实后验p之间的差别，KL散度是非对称的，这种形式的KL着重于惩罚真实后验p很小时变分概率q很大的情况。</p>
<p>我们无法最小化目标函数KL[q||p]，因为KL中包含有<script type="math/tex">\log p(x)</script>，如下推导：</p>
<script type="math/tex; mode=display">
\begin{aligned}
KL[q(z)||p(z|x)] &= E_{q(z)}[\log q(z)] - E_{q(z)}[\log p(z|x)]\\
&=E_{q(z)}[\log q(z)] - E_{q(z)}[\log p(z,x)]+E_{q(z)}[\log p(x)]\\
&=E_{q(z)}[\log q(z)] - E_{q(z)}[\log p(z,x)]+\log p(x)
\end{aligned}</script><p>但是，通过上面公式的变化可以得到一个新的目标函数ELBO：</p>
<script type="math/tex; mode=display">
\begin{aligned}
ELBO(q) &= \log p(x) - KL[q(z)||p(z|x)] \\
&= E_{q(z)}[\log p(z,x)] - E_{q(z)}[\log q(z)]
\end{aligned}</script><p>这个目标函数可以从两个方面来解释：</p>
<ul>
<li>由于<script type="math/tex">\log p(x)</script>相对于<script type="math/tex">q(z)</script>是一个常量，而我们想要最小化KL，ELBO等于负的KL加上一个常量，所以我们最大化ELBO就等价于最小化KL。</li>
<li>最大化<script type="math/tex">\log p(x)</script>就是对观测数据的极大似然估计（即log evidence），由于KL是非负的，所以这个目标函数是极大似然估计的下确界（即evidence lower bound, ELBO）。</li>
</ul>
<p>继续推导：</p>
<script type="math/tex; mode=display">
\begin{aligned}
ELBO(q) &= E_{q(z)}[\log p(z,x)] - E_{q(z)}[\log q(z)]\\
&=E_{q(z)}[\log (p(x|z)p(z))]- E_{q(z)}[\log q(z)]\\
&=E_{q(z)}[\log p(x|z)] + E_{q(z)}[\log p(z)] - E_{q(z)}[\log q(z)]\\
&= E_{q(z)}[\log p(x|z)] - KL[q(z)||p(z)]
\end{aligned}</script><p>从结果来看，ELBO目标函数第一项是似然的期望，第二项是与先验之间的差别。所以最大化ELBO就是在（1）<strong>隐变量对观测数据的解释最佳</strong>和（2）<strong>变分分布q更靠近先验</strong>之间平衡。</p>
<h3 id="三、平均场变分分布族"><a href="#三、平均场变分分布族" class="headerlink" title="三、平均场变分分布族"></a>三、平均场变分分布族</h3><p>对变分推断中选择的变分分布族Q进行介绍。</p>
<p>由于分布族Q越复杂，变分推断的优化就越复杂。一般变分推断中会假定一个性质，就是选择的分布族是平均场变分分布族（mean-field variational family）。这个性质保证了<strong>每一个隐变量<script type="math/tex">z_j</script>都相互独立</strong>而且只受自己的分布<script type="math/tex">q_j</script>的参数影响（即满足<script type="math/tex">q(z) = \prod_j q_j(z_j)</script>）。如果隐变量互相有影响，那么这种变分推断被称为结构化变分推断。</p>
<p>使用平均场变分分布族的缺点是，当隐变量之间互相有关联的时候（性质被破坏）逼近的效果就会下降。比如下图的二维高斯后验，<script type="math/tex">x_1</script>和<script type="math/tex">x_2</script>两个隐变量的后验概率是紫色的椭圆形部分，而我们使用mean-field approximation的时候，会学习到一个均值相同但协方差不同的二维高斯分布。</p>
<p><img src="/images/Fu9ZVDbU07MHvRwdhShbD7NisdZ4.jpg" alt=""></p>
<h3 id="四、优化"><a href="#四、优化" class="headerlink" title="四、优化"></a>四、优化</h3><p>针对平均场变分分布，坐标上升近似推断算法（CAVI）是最常见的优化方法。CAVI交替地更新每个隐变量，更新时固定其他的隐变量的变分分布参数，用来计算当前隐变量<script type="math/tex">z_j</script>的坐标上升公式。CAVI的算法步骤如下图所示。</p>
<p><img src="/images/FkCWyKp2_oL75U4NehECkG3lHfPS.jpg" alt=""></p>
<p>首先，我们计算最佳分布<script type="math/tex">q_j</script>的表达式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q&=E[p(\cdot)]\\
&=exp\{\log E[p(\cdot)]\}\\
&\approx exp\{ \log [E(p(\cdot)] - Var(p(\cdot))/(2*E[p(\cdot)]^2)\}\\
&= exp\{\log (E[p(\cdot)]\} * exp\{ h(p(\cdot)) \}\\
&< exp\{\log (E[p(\cdot)]\}
\end{aligned}</script><p>因为在二阶泰勒展开的条件下<script type="math/tex">q= exp\{\log (E[p(\cdot)]\} * exp\{ h(p(\cdot)) \}\\</script>，且<script type="math/tex">q^*_j(z_j)=E[p(z_j|z_{-j},x)]</script>，故<script type="math/tex">q_j</script>与对数条件概率期望的指数成比例：</p>
<script type="math/tex; mode=display">
q^*_j(z_j)\propto exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}</script><p>由于隐变量之间独立，这个公式右侧的期望项可以转化为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
E_{-j}[\log p(z_j|z_{-j},x)] &= E_{-j}[\log \frac {p(z_j,z_{-j}|x)}{p(z_{-j})}]\\
&=E_{-j}[\log p(z_j,z_{-j}|x)] - E_{-j}[\log p(z_{-j})]\\
&=E_{-j}[\log p(z_j,z_{-j},x)] - E_{-j}[\log p(z_{-j})]- E_{-j}[\log p(x)]\\
&=E_{-j}[\log p(z_j,z_{-j},x)] - const.
\end{aligned}</script><p>因此，我们得到如下结论，也就是知道了联合分布的对数期望（而由于其他的隐变量确定，联合概率可求），就可以求最佳变分分布<script type="math/tex">q^*_j</script>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q^*_j(z_j)&\propto exp\{E_{-j}[\log p(z_j,z_{-j},x)] - const.\} \\
q^*_j(z_j)&\propto exp\{E_{-j}[\log p(z_j,z_{-j},x)]\} \\
\end{aligned}</script><p>在平均场变分分布族的假设下，ELBO可以被分解为对每一个隐变量<script type="math/tex">z_j</script>的函数。根据隐变量分解后的ELBO中，利用q分布的平均场性质，第一项将联合概率的期望迭代求出，第二项分解了变分分布的期望。当我们最大化<script type="math/tex">q_j</script>时，也就最大化了分解后的ELBO。</p>
<script type="math/tex; mode=display">
\begin{aligned}
ELBO(q)&=E_{q(z)}[\log p(z,x)] - E_{q(z)}[\log q(z)]\\
ELBO(q_j)&=E_j[E_{-j}[p(z_j,z_{-j},x)]] - E_j[\log q_j(z_j)] + const.
\end{aligned}</script>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>变分推断</tag>
        <tag>贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title>变分推断（二）：贝叶斯高斯混合模型</title>
    <url>/2018/03/03/vi/VI-2/</url>
    <content><![CDATA[<h2 id="变分推断（二）：贝叶斯高斯混合模型"><a href="#变分推断（二）：贝叶斯高斯混合模型" class="headerlink" title="变分推断（二）：贝叶斯高斯混合模型"></a>变分推断（二）：贝叶斯高斯混合模型</h2><p>以高斯混合模型（Mixture of Gaussians model，GMM）为例，推导变分推断的公式和CAVI算法的过程。</p>
<h3 id="一、联合概率计算"><a href="#一、联合概率计算" class="headerlink" title="一、联合概率计算"></a>一、联合概率计算</h3><p>GMM中观察数据<script type="math/tex">x={x_1,\ldots,x_n}</script>来自于K个独立的高斯分布，每个分布的均值为<script type="math/tex">\mu_k</script>，one-hot向量<script type="math/tex">c_i\in R^K</script>指示每个数据点来自哪个分布，超参数<script type="math/tex">\sigma^2</script>固定。隐变量为<script type="math/tex">\mu,c</script>，下面是其先验值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mu _k &\sim \mathcal N(0,\sigma^2)\\
c_i&\sim  Categorical(\frac{1}{K},\ldots,\frac{1}{K})\\
x_i&\sim \mathcal N(c_i^T\mu,1)
\end{aligned}</script><p>由贝叶斯定理，能够计算出联合概率：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\mu,c,x)&=p(\mu)p(c,x|\mu)\\
&=p(\mu)p(c)p(x|c,\mu)\\
&=p(\mu)\prod_{i=1}^n p(c_i)p(x_i|c_i,\mu)
\end{aligned}</script><p>根据联合概率进行求和、积分可以推导出观测数据x的边缘概率的计算公式，但是这个公式没有解析解，计算的复杂度是<script type="math/tex">O(K^n)</script>。</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(x)&=\int \sum_c p(\mu,c,x)d\mu\\
&=\int p(\mu)\prod_{i=1}^n \sum_{c_i}p(c_i)p(x_i|c_i,\mu) d\mu
\end{aligned}</script><h3 id="二、GMM中的CAVI"><a href="#二、GMM中的CAVI" class="headerlink" title="二、GMM中的CAVI"></a>二、GMM中的CAVI</h3><p>下面计算变分概率<script type="math/tex">q(z)</script>，其中<script type="math/tex">m=(m_1,\ldots,m_K),s^2=(s_1^2,\ldots,s_K^2),\varphi=(\varphi_1,\ldots,\varphi_n)</script>是变分参数，由这些参数决定变分分布q：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(z) &= q(\mu,c)\\
&=\prod_{k=1}^K q(\mu_k;m_k,s_k^2)\cdot \prod_{i=1}^n q(c_i;\varphi_i)
\end{aligned}</script><ul>
<li>0.首先，可以得到ELBO的计算公式，是关于<script type="math/tex">m,s^2,\varphi</script>的函数：</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
ELBO(q) &= ELBO(m,s^2,\varphi)\\
&=E_{q(z)}[\log p(x|z)] + E_{q(z)}[\log p(z)] - E_{q(z)}[\log q(z)]\\
&=\sum_{i=1}^n E[\log p(x_i|c_i,\mu;\varphi_i,m,s^2)] + \{ \sum_{k=1}^KE[\log p(\mu_k);m_k,s_k^2] + \sum_{i=1}^nE[\log p(c_i;\varphi_i)] \} \\
&- \{ \sum_{k=1}^KE[\log q(\mu_k);m_k,s_k^2] + \sum_{i=1}^nE[\log q(c_i;\varphi_i)] \} \\
\end{aligned}</script><ul>
<li>1.由上一节中推导出的CAVI隐变量更新公式，下面带入GMM计算cluster indicator隐变量<script type="math/tex">c</script>的参数更新公式，注意在此时<script type="math/tex">\mu</script>是不变的：</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
q^*_j(c_j;\varphi_i)&\propto exp\{E_{\mu}[\log p(c_i,\mu,x_i)]\} \\
&\propto exp\{E_\mu[\log p(x_i|c_i,\mu)\cdot\log p(c_i,\mu)]\}\\
&\propto exp\{ E_\mu [\log p(x_i|c_i,\mu)] + E_\mu[\log p(c_i,\mu)]\}\\
&\propto exp\{ E_\mu [\log p(x_i|c_i,\mu)] + \log p(c_i)\}
\end{aligned}</script><p>推导出的结果中，第二项是先验的log值，结果为常数<script type="math/tex">-\log K</script>。我们来看第一项，这是第<script type="math/tex">c_i</script>个高斯分布的期望，我们下面对其进行分解。再次说明：<script type="math/tex">c_i=(c_{i1},\ldots,c_{iK})</script>是one-hot向量。公式推导过程中用到了正态分布的概率分布函数。</p>
<script type="math/tex; mode=display">
\tag{1.1}
\begin{aligned}
E_\mu[\log p(x_i|c_i,\mu)] &= \sum_k c_{ik} E_{\mu_k}[\log p(x_i|\mu_k)]  \\
&= \sum_k c_{ik} E_{\mu_k} [-\frac{(x-\mu_k)^2}{2}] + const.\\
&=\sum_k c_{ik} (E_{\mu_k}[\mu_k]x_i + E_{\mu_k}[\mu_k^2]/2) + const.
\end{aligned}</script><p>上面的更新公式中<script type="math/tex">E[\mu_k]</script>和<script type="math/tex">E[\mu_k^2]</script>都可以计算出来，最终对于数据点i，隐变量<script type="math/tex">c</script>的第k个维度的参数<script type="math/tex">\varphi_{ik}</script>的更新公式是：</p>
<script type="math/tex; mode=display">
\varphi_{ik}\propto exp\{ E[\mu_k]x_i + E[\mu_k^2]/2 \}</script><ul>
<li>2.与1中相似地，可以计算出在GMM中隐变量<script type="math/tex">\mu</script>的参数更新公式，其过程是首先计算出最佳变分分布<script type="math/tex">q(\mu_k)</script>，而后根据计算出的形式更新<script type="math/tex">\mu_k</script>的参数<script type="math/tex">m_k,s_k^2</script>：</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
m_k &=\frac{\sum_i\varphi_{ik}\cdot x_i}{1/\sigma^2 + \sum_i\varphi_{ik}}\\
s_k^2&=\frac{1}{1/\sigma^2 +\sum_i\varphi_{ik}}
\end{aligned}</script><p>由上面计算出的结果，我们就得到了CAVI在GMM中的优化过程，如下图所示。</p>
<p><img src="/images/FoxPnhiiVyPpKqy-OQ4t-EBec3yK.jpg" alt=""></p>
<p>这个变分分布求出之后，可以用来估计新数据点最可能的cluster，可以用来估计cluster均值，也可以用来预测新数据点的值。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>变分推断</tag>
        <tag>贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title>变分推断（三）：指数族变分推断</title>
    <url>/2018/03/09/vi/VI-3/</url>
    <content><![CDATA[<h2 id="变分推断（三）：指数族变分推断"><a href="#变分推断（三）：指数族变分推断" class="headerlink" title="变分推断（三）：指数族变分推断"></a>变分推断（三）：指数族变分推断</h2><h3 id="一、坐标更新公式在指数族下的变换"><a href="#一、坐标更新公式在指数族下的变换" class="headerlink" title="一、坐标更新公式在指数族下的变换"></a>一、坐标更新公式在指数族下的变换</h3><p>之前的博文中讨论了GMM中的变分推断和CAVI算法，这篇文章中，把它推广到更一般的情况，也就是分布是<strong>指数分布族</strong>（exponential family）的情况。</p>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Exponential_family" target="_blank" rel="noopener">指数分布族</a>，是对常见分布的性质经过总结和泛化归纳出来的一类分布，具有形如<script type="math/tex">f(x|\theta)=h(x)exp(\eta(\theta)\cdot T(x) - A(\theta))</script>形式的概率密度函数/概率质量函数。其中，T(x)是充分统计量（能够完全表征一个概率分布的所有量）。</p>
</blockquote>
<p>可以用指数族来简化CAVI中坐标更新的公式，我们假定完全条件概率(complete conditional)为如下的指数族形式：</p>
<script type="math/tex; mode=display">
p(z_j|z_{-j},x)=h(z_j)exp\{\eta_j(z_{-j},x)\cdot z_j - \alpha(\eta_j(z_{-j},x))\}</script><p>那么利用平均场假设，可对坐标更新公式做如下变换：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(z_j)&\propto exp\{ E[\log(p(z_j|z_{-j},x))] \}\\
&=exp\{ \log h(z_j) + E[\eta_j(z_{-j},x)]\cdot z_j - E[\alpha(\eta_j(z_{-j},x))] \}\\
&\propto \log h(z_j)\cdot exp\{ E[\eta_j(z_{-j},x)]\cdot z_j\}
\end{aligned}</script><p>所以对当前变分参数<script type="math/tex">\mathcal{v}_j</script>，我们只需将其更新到其隐变量<script type="math/tex">z_j</script>对应的条件概率的期望参数<script type="math/tex">E[\eta_j(z_{-j},x)]</script>即可。</p>
<h3 id="二、条件共轭模型"><a href="#二、条件共轭模型" class="headerlink" title="二、条件共轭模型"></a>二、条件共轭模型</h3><p>带<strong>局部变量</strong>和<strong>全局变量</strong>的条件共轭模型是指数族模型的一种重要的特例。</p>
<p>在这里局部变量指的是只对<strong>某个数据点</strong>起作用的变量，而全局变量指的是对<strong>所有数据</strong>都起作用的变量。比如，在之前的GMM模型中，描述K个高斯分布均值的变量<script type="math/tex">\mu</script>就是全局变量，而描述第i个数据点属于哪个模型的变量<script type="math/tex">z</script>就是局部变量。</p>
<p>现在，假设模型拥有全局变量<script type="math/tex">\beta</script>，局部变量<script type="math/tex">z</script>和数据<script type="math/tex">x</script>，那么联合概率可以写成<script type="math/tex">p(z,\beta,x) = p(\beta)\prod_{i=1}^{n}p(z_i,x_i|\beta)</script>的形式。</p>
<ul>
<li>全局变量</li>
</ul>
<p>我们假定所有的complete conditional都是指数族分布，<script type="math/tex">z_i,x_i</script>的联合概率可以写成下面的指数族形式：</p>
<script type="math/tex; mode=display">
p(z_i,x_i|\beta) = h(z_i,x_i)\cdot exp\{\beta\cdot t(z_i,x_i)-\alpha(\beta)\}</script><p>认为全局变量<script type="math/tex">\beta</script>具有对应的先验共轭分布（为什么？），其中<script type="math/tex">\eta=[\eta_1,\eta_2]</script>是其自然参数，而充分统计量是<script type="math/tex">\beta</script>和局部变量概率密度的对数标准化项：</p>
<script type="math/tex; mode=display">
p(\beta)=h(\beta)exp\{\eta\cdot [\beta,-\alpha(\beta)]- \alpha(\eta)\}</script><p>有了这个共轭先验，全局变量<script type="math/tex">\beta</script>的完全条件概率（就是求坐标更新公式需要的那个<script type="math/tex">p(\beta|z,x)</script>）的参数<script type="math/tex">\hat\eta</script>可以由<script type="math/tex">\eta</script>和<script type="math/tex">z,x</script>求得，如下公式。</p>
<script type="math/tex; mode=display">
\hat\eta = [\eta_1 + \sum_{i=1}^n t(z_i,x_i), \eta_2 +n]</script><ul>
<li>局部变量</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
p(z_i|z_{-i},x,\beta)&=p(z_i|x_i,\beta,x_{-i},z_{-i})\\
&=p(z_i|x_i,\beta)
\end{aligned}</script><ul>
<li>CAVI的坐标更新公式</li>
</ul>
<p>我们用<script type="math/tex">\varphi</script>和<script type="math/tex">\lambda</script>分别代表隐变量<script type="math/tex">z</script>和<script type="math/tex">\beta</script>的变分参数，用<script type="math/tex">q</script>来表示变分分布（也是指数族分布），那么每一步CAVI只需将变分参数的值更新为自然参数的期望。</p>
<p>局部变分参数<script type="math/tex">\varphi</script>，只需要应用CAVI坐标更新公式在指数族模型上的泛化公式即可：</p>
<script type="math/tex; mode=display">
\varphi_i=E_\lambda[\eta_i(\beta,x_i)]</script><p>全局变分参数<script type="math/tex">\lambda</script>，只需要更新到我们上面算出的<script type="math/tex">\beta</script>的完全条件概率的参数<script type="math/tex">\hat\eta</script>的期望：</p>
<script type="math/tex; mode=display">
\lambda = E_\varphi [\hat\eta] = [\eta_1+\sum_{i=1}^n E_{\varphi_i}[t(z_i,x_i)],\eta_2+n]</script><ul>
<li>CAVI的ELBO计算</li>
</ul>
<p>有上面的指数族概率公式，我们可以在每一轮更新隐变量参数之后，计算ELBO直到收敛。</p>
<p>CAVI能够用于GMM、LDA等模型的优化。</p>
<h3 id="三、随机变分推断"><a href="#三、随机变分推断" class="headerlink" title="三、随机变分推断"></a>三、随机变分推断</h3><p>上面的CAVI算法在每次迭代都要遍历所有的数据，因此无法在较大规模的数据上使用。现在我们介绍一种更高效的优化方法——随机变分推断（SVI）。</p>
<p>首先，我们介绍ELBO的自然梯度（natural gradient）用来更新全局变量的参数<script type="math/tex">\lambda</script>，Hoffman在2013年的SVI论文中推导出了ELBO的欧式梯度<script type="math/tex">\bigtriangledown_\lambda ELBO</script>和自然梯度<script type="math/tex">g(\lambda)</script>，如下公式。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\bigtriangledown_\lambda ELBO&=\alpha''(\lambda) (E_\varphi [\hat\eta]-\lambda)\\
g(\lambda)&=E_\varphi [\hat\eta]-\lambda
\end{aligned}</script><p>基于自然梯度的优化在确定步长<script type="math/tex">\epsilon_t</script>之后，更新<script type="math/tex">\lambda_t</script>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\lambda_{t+1}&=\lambda_t + \epsilon_tg(\lambda_t)\\
&=(1-\epsilon_t)\lambda_t+\epsilon_tE_\varphi[\hat\eta]
\end{aligned}</script><p>上面的更新公式计算速度和CAVI的更新公式一样，都需要遍历所有数据，因此还是达不到大数据集的要求。</p>
<p>而随机优化算法证明了，在步长满足一定条件的前提下，使用noisy and unbiased gradient能够得到目标函数的最优值（可能是局部最优），因此首先，我们将自然梯度扩展为Noisy gradient，定义如下。</p>
<script type="math/tex; mode=display">
g(\lambda) = \eta + [\sum_{i=1}^n E_{\varphi_i^*}[t(z_i,x_i)],n] - \lambda</script><p>这个定义中的<script type="math/tex">\varphi_i^*</script>是在固定全局参数<script type="math/tex">\lambda</script>的情况下，在i数据点的最优局部变分参数。</p>
<p>SVI的步骤如下：</p>
<ul>
<li>1.随机从所有数据中选择数据点t；</li>
<li>2.计算noisy gradient：<script type="math/tex">\hat g(\lambda)=\eta + n[E_{\varphi _i^*}[t(z_t,x_t)],1 - \lambda]</script>，也就是把原来的第二项改成当前数据点t计算出来的值乘以数据总量（就是以当前数据点来估计所有数据的梯度）；</li>
<li>3.可以证明，这个noisy natural gradient是无偏的：<script type="math/tex">E[\hat g(\lambda)]=g(\lambda)</script>；</li>
<li>4.这个梯度计算很快，而且可以推广到mini-batch的形式，只需要修改第二项scale的值即可；</li>
<li>5.步长的设置需要满足一定的条件，才能保证收敛。</li>
</ul>
<p>最后，这是自己的第100篇博客，加油！</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>变分推断</tag>
        <tag>贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title>【David Silver强化学习公开课】-1：RL-introduction</title>
    <url>/2017/08/28/deeplearning/DRL/%E3%80%8ADavid%20Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E3%80%8B-1%EF%BC%9ARL-introduction/</url>
    <content><![CDATA[<p>David Silver强化学习公开课第一课的总结笔记。这个公开课的视频本来是在<a href="youtube.com">Youtube</a>上的，不过有筒靴已经搬运到b站了，点这里<a href="http://www.bilibili.com/video/av8912293/?from=search&amp;seid=16169139515703240533" target="_blank" rel="noopener">公开课b站链接</a>。</p>
<h4 id="一、一些概念"><a href="#一、一些概念" class="headerlink" title="一、一些概念"></a>一、一些概念</h4><ul>
<li>agent：一个智能代理，能够依据policy进行一些action，并根据累积的反馈（reward）修改策略，包含以下三个部分；<ul>
<li>Policy：策略，行为函数，从状态到行动的映射；</li>
<li>Value Function：价值函数，通过预测将来的奖励，衡量action或者state的好坏；</li>
<li>Model：agent对于环境的表示，用来表示环境的反应，包含两个部分：<ul>
<li>P预测下一个状态：$P^a_{SS’}=P[S’=s’|S=s,A=a]$；</li>
<li>R预测下一个奖励：$R_S^a=P[R|S=s,A=a]$</li>
</ul>
</li>
</ul>
</li>
<li>environment：代理所处的环境；</li>
<li>goal：学习的目标（通常的假设是其可以用最大化的累积奖励表示），选择一系列的操作使未来的奖励最高；</li>
</ul>
<ul>
<li>reward：$R_t$代表环境的反馈标量，表示第$t$步时agent获得的奖励；</li>
</ul>
<blockquote>
<p>奖励假设：所有的目标都可以通过最大化累计奖励的期望的方式逼近。</p>
</blockquote>
<ul>
<li>observation：$O_t$代表agent在$t$时刻观察到的值，也可以认为是environment在$t$时刻输出的值；</li>
<li>action：$A_t$代表agent在$t$时刻进行的动作；</li>
<li>History：代表O、A、R的序列，例如$H_t={A_1,O_1,R_1,…,A_t,O_t,R_t}$；</li>
<li>state：代表当前时刻的状态，由history决定，$S_t=f(H_t)$；</li>
<li>environment state：$S_t^e$，环境的状态，决定了$O_t$，通常对于agent不可见；</li>
<li>agent state：$S_t^a$，代表agent的当前状态，决定了$A_t$。</li>
</ul>
<p>马尔科夫状态（信息状态）：若当前状态包含了所有历史信息，即下一状态仅仅和当前状态相关，和之前的状态无关，这样的state就是马尔科夫状态。这里历史信息$H_t$和环境状态$S_t^e$是马尔科夫状态。</p>
<script type="math/tex; mode=display">P(S_{t+1}|S_t) = P(S_{t+1}|S_t,S_{t-1},...,S_1)</script><ul>
<li>Full Observability：就是$O_t=S_t^e=s_t^a$，此时是一个马尔科夫决策过程（MDP）</li>
<li>Partial Observability：$S_t^e\neq s_t^a$，此时是一个部分可见马尔科夫决策过程(POMDP)</li>
</ul>
<p><img src="http://jycloud.9uads.com/web/GetObject.aspx?filekey=25a829dd0aa3a1491125e64b2d31bc18" alt="1"></p>
<p>Agent、Environment的交互过程：根据History，agent在t时刻选择action，而environment根据action输出observation和reward。</p>
<p>RL-agent的分类：Value-based，Policy-based，Actor-Critic，Model-Free，Model-based。</p>
<h4 id="二、两个基本的问题"><a href="#二、两个基本的问题" class="headerlink" title="二、两个基本的问题"></a>二、两个基本的问题</h4><ul>
<li>RL过程：模型未知，通过与环境的交互，agent提升其policy；<ul>
<li>通过试错过程学习，Exploration和Exploitation的平衡。</li>
</ul>
</li>
<li>规划过程：agent对于环境的模型已知，根据模型计算（不与环境交互），并提升其policy。</li>
</ul>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>David Silver</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>【David Silver强化学习公开课】-2：MDP</title>
    <url>/2017/08/29/deeplearning/DRL/%E3%80%8ADavid%20Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E3%80%8B-2%EF%BC%9AMDP/</url>
    <content><![CDATA[<h4 id="一、一些概念"><a href="#一、一些概念" class="headerlink" title="一、一些概念"></a>一、一些概念</h4><blockquote>
<p> 马尔科夫性质：当前时刻状态仅仅与前一个时刻相关。</p>
</blockquote>
<p>状态转移矩阵，表明了任意状态a到状态b的条件概率。</p>
<blockquote>
<p>马尔科夫过程（马尔科夫链）：一个具有马尔科夫性质的无记忆的随机过程，包含n个状态。</p>
</blockquote>
<p>马尔科夫激励过程(S,P,R,γ)是一个带有value的马尔科夫链。</p>
<p>用<script type="math/tex">G_t</script>来表示t时刻会得到的总的return。出于数学计算、防止NaN无穷大的return等原因，引入折扣因子<script type="math/tex">\gamma \in [0,1]</script>来对下一时刻的奖励和更远的奖励之间进行取舍。（若所有序列都会在有限步终结，而且策略上合适，γ也可以取1。）</p>
<script type="math/tex; mode=display">
G_t=R_{t+1} + \gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma^k R_{t+k+1}</script><p>价值函数v(s)，在马尔科夫激励过程（MRP）中表征指定状态下，获得的return的期望。是由所有包含该状态的样本Sample序列计算出来的。其中<script type="math/tex">R_s</script>是立即奖励，可以认为是离开状态s时获得的奖励。</p>
<script type="math/tex; mode=display">
v(s)=E[G_t|S_t=s]=E[R_{t+1} + \gamma(v(s_{t+1})|S_t=s)]=R_s+\gamma\sum_{s'\in S} P_{ss'}v(s')</script><p>上面的公式可以向量化的表示为：</p>
<script type="math/tex; mode=display">
V=R+\gamma PV</script><p>而这个公式是有解析解的，MRP每个状态的价值可以直接被解出来。</p>
<h4 id="二、MDP"><a href="#二、MDP" class="headerlink" title="二、MDP"></a>二、MDP</h4><p>马尔科夫决策过程(S,A,P,R,γ)，在MRP基础上增加了有限的action集合。</p>
<blockquote>
<p>策略，给定状态时，关于行为的概率分布，用π来表示。决定了agent的行为。</p>
</blockquote>
<p>MDP和马尔科夫过程、MRP内在的联系。</p>
<blockquote>
<p>状态价值函数<script type="math/tex">v_\pi (s)</script>定义了在状态s下，采用策略π，所能获得的期望return。</p>
<p>行为价值函数<script type="math/tex">q_\pi (s,a)</script>定义了在状态s下，采取行为a，并在之后采用策略π所能获得的期望return。</p>
</blockquote>
<p>这两个价值函数之间密切相关。状态的价值，就等于这个状态下所有行为a产生的行为价值q，乘以做出该行为的概率（策略）π。反之，行为的价值，就等于这个行为所能产生的立即奖励immediate reward加上折扣因子乘以下一个状态（到达这个状态的概率由动态转移矩阵来确定）乘以这个状态的状态价值。</p>
<p>在MDP中，你能够控制你的行为（通过策略），但是你无法控制环境（做出行为之后会发生什么），这个要靠动态转移矩阵来计算。</p>
<blockquote>
<p>最佳价值函数<script type="math/tex">v_*(s)</script>和<script type="math/tex">q_*(s,a)</script>。最佳策略<script type="math/tex">\pi_*</script>，就是在每个状态下选择最大的行为价值函数q*。</p>
</blockquote>
<p>如何计算这个Q<em>呢，Bellman Optimality Equation。也就是对每个状态，其价值等于价值最大的行为的价值，而这个行为的价值又由<strong>直接奖励</strong>和行为<em>*可能会导致的状态价值</em></em>有关。</p>
<script type="math/tex; mode=display">
v_*(s)=max_aq_*(s,a)</script><script type="math/tex; mode=display">
q_*(s,a)=R_s^a+\gamma\sum_{s'\in S}P_{ss'}^av_*(s')</script><p>而这个公式就无法直接解析求解了，求解的方法有：</p>
<ul>
<li>Value iteration</li>
<li>Policy iteration</li>
<li>Q-learning</li>
<li>Sarsa</li>
</ul>
<p>最后，对MDP的扩展模型和其他一些概念进行了简介，如infinite/continuous/POMDP/belief states。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li><a href="https://zhuanlan.zhihu.com/p/21378532" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21378532</a></li>
</ul>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>David Silver</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>【David Silver强化学习公开课】-3：DP</title>
    <url>/2017/08/30/deeplearning/DRL/%E3%80%8ADavid%20Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E3%80%8B-3%EF%BC%9ADP/</url>
    <content><![CDATA[<h4 id="一、一些概念"><a href="#一、一些概念" class="headerlink" title="一、一些概念"></a>一、一些概念</h4><p>MDP的两个规划问题：</p>
<ul>
<li>预测，给定MDP和策略π，求出价值函数<script type="math/tex">v_\pi</script></li>
<li>控制，给定MDP，求出最佳价值函数<script type="math/tex">v_*</script>和最佳策略<script type="math/tex">\pi_*</script></li>
</ul>
<p>Policy Evaluation策略评估：</p>
<p>给定一个策略，从<script type="math/tex">v_0</script>,<script type="math/tex">v_1</script>一直求到<script type="math/tex">v_\pi</script>,第k步求出的状态价值函数，通过Bellman期望方程可以求出k+1步的状态价值函数。这样一直迭代下去，最终状态价值函数会收敛，完成对策略π的评估。</p>
<p>Policy Iteration策略迭代：</p>
<ul>
<li>1.评估策略，使用策略评估的方式更新价值函数；</li>
<li>2.改进策略，根据上一步的价值函数，用贪心原则更新策略；</li>
<li>3.迭代上两步，直到找到最优策略π<em>，也就找到了最优价值函数v</em>.</li>
</ul>
<p>价值迭代：</p>
<ul>
<li>按照bellman最优方程，每个循环计算（更新）价值函数；</li>
<li>没有显式的策略，贪心的计算方式在最优方程中，更为直接。</li>
</ul>
<script type="math/tex; mode=display">
v_{k+1}(s)=max_{a\in A} [R_s^a + \gamma\sum_{s'\in S}P_{ss'}^av_k(s')]</script><p>值迭代的一些扩展，比如in-place（不存储旧的状态价值函数，状态的价值函数更新后立刻可以被用于其他状态价值函数的更新），使用Bellman误差（新价值与旧价值的差值）决定状态更新的优先级。</p>
<p>DP的时间开销比较大，而且无论价值迭代还是策略迭代都需要知道R和转移矩阵P，那么就需要对模型有比较深的了解，下一讲将会介绍一种开销更低而且是model-free的方法：采样。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li><a href="https://zhuanlan.zhihu.com/p/21378532" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21378532</a></li>
</ul>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DP</tag>
        <tag>David Silver</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>【David Silver强化学习公开课】-4：Model-Free Prediction</title>
    <url>/2017/08/31/deeplearning/DRL/%E3%80%8ADavid%20Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E3%80%8B-4%EF%BC%9AModel-Free%20Prediction/</url>
    <content><![CDATA[<h4 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h4><p>无论是价值迭代还是策略迭代，都是在已经知道MDP模型（也就是动态转移矩阵P和奖励R）的前提下用DP的方式进行控制。那么如果对模型的这些属性并不了解，要如何进行预测和控制呢？</p>
<p>本节主要讲几种方法来进行无模型前提下的策略评估（model-free policy evaluation）。</p>
<h4 id="二、Monte-Carlo-RL方法"><a href="#二、Monte-Carlo-RL方法" class="headerlink" title="二、Monte-Carlo RL方法"></a>二、Monte-Carlo RL方法</h4><blockquote>
<p>Episodic MDP: 所有的行为序列都在有限步终止。</p>
</blockquote>
<p>MC方法从已有的<strong>完整经验片段（complete episode of experience）</strong>或者说History中采样，这些片段都是采用策略π的，那么MC方法使用经验平均return来代替期望return，用这种方式来估计策略π下的状态价值函数<script type="math/tex">v_\pi</script>。</p>
<ul>
<li>First-Visit MC Policy Evaluation，仅仅计算每个片段第一次到达某个状态时return的平均值。</li>
<li>Every-Visit MC Policy Evaluation，计算每个片段每次到达某个状态时return的均值。</li>
</ul>
<blockquote>
<p>一个递增式的均值计算公式，<script type="math/tex">\mu_k=\mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1})</script>。</p>
</blockquote>
<p>所以，MC-Policy-Evaluation的更新公式可以表示为（这个公式中的α是<script type="math/tex">\frac{1}{k}</script>的一种变形，可以看做是一种衰减系数），这种公式下的方法叫做Incremental-MC：</p>
<script type="math/tex; mode=display">
V(S_t) \leftarrow V(S_t) + \alpha (G_t-V(S_t))</script><h4 id="三、Temporal-Difference-Learning"><a href="#三、Temporal-Difference-Learning" class="headerlink" title="三、Temporal-Difference Learning"></a>三、Temporal-Difference Learning</h4><p>TD也是一种Model-Free的方法，但是它可以从<strong>不完整的片段</strong>中学习。</p>
<p>下面介绍一种最简单的TD方法，TD(0)。</p>
<p>它同Incremental-MC相比，把公式中的实际回报<script type="math/tex">G_t</script>变成了估计回报<script type="math/tex">R_{t+1}+\gamma V(S_{t+1})</script>，而这个估计回报在TD中被叫做TD target，估计回报和现在价值的差被称为<script type="math/tex">\delta_t</script>:TD error。</p>
<script type="math/tex; mode=display">
V(S_t) \leftarrow V(S_t) + \alpha (R_{t+1}+\gamma V(S_{t+1})-V(S_t))</script><p>TD和MC的区别，可以通过一个例子来说明。假设，你现在按照某种方式（策略π）在做一件事情，会尝试很多次（有很多Episode），这件事情中一共要经历几个阶段（状态），每次行动之后你会获得一个效果评价（R)。</p>
<p>现在你用MC的方式对做这件事情的方式（策略π）进行评估，首先你找到了其中一次你做这件事情的整个流程(初始状态，行动1，状态1，效果1….，最终状态)，而后你在评估每一个阶段（状态）的时候，都看向一遍整个过程，看看这个状态之后在整个过程里你能获得怎样的平均收益（因为是用<script type="math/tex">G_t</script>来评估的）。</p>
<p>如果你用TD的方式进行评估，那么你只需要找到（状态t，行动t，效果t）就能够更新你对某个阶段（状态）的评估了，因为你评估的时候，只会向后看一步，以我对这些阶段（状态）的了解，用这个阶段（状态）后面可能到达的阶段（状态）来评估这个状态的好坏。也就是在做事情的时候，你每走一步，都可以根据外界对你这一步的效果评价，更新你对每个阶段的评价（也就是更新你对你做事情的方式的评价）。</p>
<p>MC的估计方式，估计的过程中不会出现偏差，但是比较慢。</p>
<p>TD的估计方式，在估计的过程中会出现一些偏差（毕竟事情还没有完全做完就评估了），但是拥有更低的方差【这里没看懂。。】。</p>
<p>TD利用了马尔科夫性质而MC没有，因此在不具备马尔科夫性质的问题中，MC更适合。</p>
<blockquote>
<p>bootstrap：在更新值的过程中是否使用了估计值。如果你向前走了n步，没有走到最后就更新参数，那么就是使用了bootstrap。</p>
</blockquote>
<p>MC没有bootstrap过程而DP和TD都有。</p>
<h4 id="四、TD-λ"><a href="#四、TD-λ" class="headerlink" title="四、TD(λ)"></a>四、TD(λ)</h4><p>另一种扩展TD方法的方式是，每次使用向前走n步（而不是上一节中的1步），这时的TD target变成了下面的样子：</p>
<script type="math/tex; mode=display">
G_T^{(n)}=R_{t+1}++\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^n V(S_{t+n})</script><p>更进一步，把向前n步的估计return结合起来加权求和，得到一个新的估计return，然后用这个估计作为TD target，这就是TD(λ)的思想。</p>
<script type="math/tex; mode=display">
G_t^\lambda = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G_t^{(n)}</script><p>当然，TD(λ)需要在一个complete episode中进行。</p>
<p>从backward view的角度来看，TD(λ)相当于使用了MC方法的TD error：<script type="math/tex">\delta_t</script>，并在此基础上给予每个状态一个能量等级，这个状态最近出现的越频繁，这个能量等级就越高。</p>
<script type="math/tex; mode=display">
E_t(s)=\gamma\lambda E_{t-1}(s)+1</script><script type="math/tex; mode=display">
V(s) \leftarrow V(s) + \alpha \delta_tE_t(s)</script><p>λ=1时，且在Offline-updates的前提下，TD(λ)等价于Every-Visit-MC。</p>
<p>疑问：</p>
<ul>
<li>为什么说TD(λ)可以在TD(0)相似的时间开销下完成，感觉要慢很多啊。</li>
<li>为什么λ=0时，TD(λ)等价于TD(0)。</li>
</ul>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li>​</li>
</ul>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>David Silver</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>【David Silver强化学习公开课】-6：Value Function Approximation</title>
    <url>/2017/09/04/deeplearning/DRL/%E3%80%8ADavid%20Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E3%80%8B-6%EF%BC%9AValue%20Function%20Appro/</url>
    <content><![CDATA[<h4 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h4><p>找到一种适应真实情况（很大的状态空间）的RL方法，之前的价值函数表示是通过一个S×A的表（Table）来表示Q(s,a)。状态空间很大时，这种表示内存占用过大，而且单独学习每个state的价值函数太慢了。而且在遇到没有见过的状态时，表现会很差（缺少泛化能力）。</p>
<h4 id="二、价值函数逼近-Incremental-Online"><a href="#二、价值函数逼近-Incremental-Online" class="headerlink" title="二、价值函数逼近-Incremental Online"></a>二、价值函数逼近-Incremental Online</h4><p>使用参数化的价值函数V’(s,w)来逼近V(s)，或者Q’(s,a,w)逼近Q(s,a)。常用的方法有：特征线性组合，神经网络等。那么我们就需要不断的优化这个逼近函数。</p>
<blockquote>
<p>训练逼近函数的过程中，还要注意数据non-stationary和non-iid的性质。</p>
</blockquote>
<ul>
<li>梯度下降，以<strong>真实价值函数</strong><script type="math/tex">v_\pi</script>和估计的价值函数的MSE作为objective，用GD进行训练。（这里用V函数举例，对Q函数同理）</li>
</ul>
<script type="math/tex; mode=display">
J(W) = E_\pi[(v_\pi(s)-v'(s,w))^2]</script><p>但是，在RL中，真实价值函数是不知道的，因此在实际使用中，我们在不同的方法中使用不同的target：</p>
<p>MC中，target是<script type="math/tex">G_t</script>；而TD(0)中，target是<script type="math/tex">R_t+\gamma V'(S_{t+1},w)</script>；TD(λ)中，target是<script type="math/tex">G_t^\lambda</script>。</p>
<p>所以，我们采用逼近式的策略评估过程时，相当于采用MC或者TD的target，并用GD的训练方式得到一个与真实价值函数相近的函数。</p>
<h4 id="三、价值函数逼近-batch-method"><a href="#三、价值函数逼近-batch-method" class="headerlink" title="三、价值函数逼近-batch method"></a>三、价值函数逼近-batch method</h4><p>从乱序的数据集D中采样状态价值对，然后用来优化逼近函数。这里的优化目标是所有样本的MSE经验期望（均值）。</p>
<p>乱序+采样，减弱了样本之间本来的相关性。</p>
<p>这个部分介绍的内容也就是DQN中使用的experience replay。</p>
<p>第二个DQN中使用的部分就是fixed Q-target，使用两个相同的神经网络，但是计算target的网络参数较老，定期从learning网络更新参数。因为target老是更新的话，不会稳定。这个trick其实理论依据不强，主要是实践中效果比较好。</p>
<p>从最后给出的表格可以看出，DQN的训练非常依赖这两个trick，没有这两种变化的话，效果不怎么好。</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>David Silver</tag>
        <tag>公开课</tag>
        <tag>Q-learning</tag>
      </tags>
  </entry>
  <entry>
    <title>【David Silver强化学习公开课】-5：Model-Free Control</title>
    <url>/2017/09/01/deeplearning/DRL/%E3%80%8ADavid%20Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E3%80%8B-5%EF%BC%9AModel-Free%20Control/</url>
    <content><![CDATA[<h4 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h4><p>这一讲的内容是大部分情况下真实使用的算法，也就是在对环境一无所知的情况下，去学习出一个好的策略。首先介绍一些概念：</p>
<blockquote>
<p>Model-Free Control，在环境未知的前提下，如何学习策略（价值）以获得最大的奖励。</p>
<p>On-Policy和Off-Policy，两种控制的类型，前一种是策略已知，后一种是策略未知。后者允许你使用其他人的experience sequence来学习，而前者只能在知道自己策略的前提下学习。如果计算target的时候使用的是get data的时候的policy，那么就是on-policy的方法，否则是off-policy。</p>
<p><script type="math/tex">\epsilon</script>-贪婪，每一次进行动作的时候，以<script type="math/tex">\epsilon</script>的概率随机执行一个动作，以<script type="math/tex">1-\epsilon</script>的概率执行q值最高的动作。随着训练进行<script type="math/tex">\epsilon</script>逐渐减小。这样能够尽可能多的探索各种策略，同时达到训练的目的。同时可以很容易证明<script type="math/tex">q_{\pi'}(s)\geq q_\pi(s)</script>。</p>
</blockquote>
<h4 id="二、Monte-Carlo-Control"><a href="#二、Monte-Carlo-Control" class="headerlink" title="二、Monte-Carlo Control"></a>二、Monte-Carlo Control</h4><p>对于每个Episode，控制过程分成两个步骤：</p>
<ul>
<li>Monte-Carlo的策略评估过程，这个过程结束后<script type="math/tex">Q\approx q(\pi)</script>.</li>
<li><script type="math/tex">\epsilon</script>-贪婪的策略优化过程。</li>
</ul>
<h4 id="三、Sarsa"><a href="#三、Sarsa" class="headerlink" title="三、Sarsa"></a>三、Sarsa</h4><blockquote>
<p>TD -Control，用TD代替MC进行策略评估。那么每一步都会更新一次价值函数的评估，而不是每个Episode。</p>
</blockquote>
<p>Sarsa，在每个time-step，用Sarsa的方式进行策略评估，并用<script type="math/tex">\epsilon</script>-贪婪的方式进行策略优化。</p>
<p>我们来回顾一下TD(0)的更新公式：</p>
<script type="math/tex; mode=display">
V(S_t) \leftarrow V(S_t) + \alpha (R_{t+1}+\gamma V(S_{t+1})-V(S_t))</script><p>下面是Sarsa的更新公式，很像是把TD应用在Q函数上的形式：</p>
<script type="math/tex; mode=display">
Q(S,A) \leftarrow Q(S,A) + \alpha (R+\gamma Q(S',A')-Q(S,A))</script><p>不过Sarsa的算法是属于on-policy的，因为你在更新Q函数的过程中，需要对一个状态S获得一个行为A，并观察下一个状态S’，且使用现有的策略π获得行为A’。因为确定行为A’需要知道你的策略，所以是on-policy的。当然，Sarsa也可以推广到n-step的形式和Sarsa(λ)的形式。</p>
<p>疑问：</p>
<ul>
<li>在Sarsa算法中，你采用了一个action，然后观察reward和s’，那么这个r和s’是如何产生的呢？这里有某种形式的采样过程吗？</li>
<li>也就是公式中的R(t+1),R(t+2),….等等是如何获得的？</li>
</ul>
<h4 id="四、Off-Policy-Learning"><a href="#四、Off-Policy-Learning" class="headerlink" title="四、Off-Policy Learning"></a>四、Off-Policy Learning</h4><p>上面说的Sarsa的方法，必须在已知自己的策略的时候，才能进行策略学习，即On-Policy。但是与之对应的Off-Policy有很多好处。比如可以利用别人的经验来学习，可以在学习最优策略的同时多多尝试，等。</p>
<p>Q-learning是一种off-policy的Q值学习方法。在off-policy的方法中，使用两种policy。</p>
<ul>
<li>behaviour policy，<script type="math/tex">\mu</script>，是固定的，用来进行exploration。在Q-learning中被用来采样得到行为<script type="math/tex">a_t</script>，常用<script type="math/tex">\epsilon-greedy</script>。</li>
<li>target policy, <script type="math/tex">\pi</script>，用来进行evaluation和improve的policy。Q-learning中用来获得行为<script type="math/tex">a'</script>，也就是<script type="math/tex">s_{t+1}</script>时刻的行为，常见的选择是greedy。</li>
</ul>
<p>将最常见的选择带入上面两个policy之后，Q-learning的更新公式如下：</p>
<script type="math/tex; mode=display">
Q(S,A) \leftarrow Q(S,A) + \alpha (R+\gamma max_{a'}Q(S',A')-Q(S,A))</script><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li><a href="https://stats.stackexchange.com/questions/184657/difference-between-off-policy-and-on-policy-learning" target="_blank" rel="noopener">difference between on-policy and off-policy</a></li>
</ul>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>David Silver</tag>
        <tag>公开课</tag>
        <tag>Q-learning</tag>
      </tags>
  </entry>
  <entry>
    <title>【David Silver强化学习公开课】-7：Policy Gradient</title>
    <url>/2017/09/06/deeplearning/DRL/%E3%80%8ADavid%20Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E3%80%8B-7%EF%BC%9APolicy%20Gradient/</url>
    <content><![CDATA[<h4 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h4><p>之前的控制方法都是Value-based，而在确定价值函数之后，其实我们是在价值函数的基础上确定了某种策略（贪婪，<script type="math/tex">\epsilon</script>-贪婪）找到action。那么我们为什么不直接通过策略函数控制action呢？</p>
<p>这样做的好处：</p>
<ul>
<li>连续的动作空间（或者高维空间）中更加高效；</li>
<li>可以实现随机化的策略；</li>
<li>某种情况下，价值函数可能比较难以计算，而策略函数较容易。</li>
</ul>
<h4 id="二、Finite-Difference-Policy-Gradient"><a href="#二、Finite-Difference-Policy-Gradient" class="headerlink" title="二、Finite Difference Policy Gradient"></a>二、Finite Difference Policy Gradient</h4><p>首先，对某种参数化策略<script type="math/tex">\pi_\theta</script>，我们需要确定一个目标函数<script type="math/tex">J(\theta)</script>，这里给出了三种：</p>
<ul>
<li>start value</li>
<li>average value</li>
<li>average reward per time-step</li>
</ul>
<p>由于要最大化目标函数，因此使用梯度上升的方法优化参数<script type="math/tex">\theta</script>。</p>
<p>那么要怎么计算策略梯度呢，使用了一种叫做finite difference的方法，也就是在每个维度k上增加一个很小的值，然后求出一个接近偏导数的值：</p>
<script type="math/tex; mode=display">
\frac{\partial J(\theta)}{\partial\theta}\approx \frac{J(\theta +\epsilon u_k) - J(\theta)}{\epsilon}</script><blockquote>
<p>likelihood ratio，如下的公式：<script type="math/tex">\partial_\theta\pi(s,a)=\pi(s,a)\times \partial_\theta log\pi(s,a)</script></p>
</blockquote>
<p>Softmax Policy：利用特征的线性组合进行softmax，决定动作的概率的策略。</p>
<p>Gaussian Policy：利用特征的线性组合作为分布的均值<script type="math/tex">\mu</script>，<script type="math/tex">\pi \sim N(\mu,\sigma^2)</script>。</p>
<p>对于任意可微的策略函数<script type="math/tex">\pi</script>，其在MDP中的梯度计算如下：</p>
<script type="math/tex; mode=display">
\partial_\theta J(\theta)=E_{\pi_\theta}[\partial_\theta log\pi_\theta(s,a)Q_{\pi_\theta}(s,a)]</script><p>所以最后，以上面这个梯度计算的公式为基础，给出了Monte-Carlo-Policy-Gradient的流程，其中以样本中的return作为Q的无偏估计：</p>
<p><img src="/images/Fg5xVLhZU3V_8p0onKmo9u5YjpdJ.jpg" alt=""></p>
<h4 id="三、Actor-Critic"><a href="#三、Actor-Critic" class="headerlink" title="三、Actor-Critic"></a>三、Actor-Critic</h4><p>上一讲中的MC-PG方法造成的方差太大，引入Actor-Critic方法解决。</p>
<blockquote>
<p>actor，参数θ，在环境中学习策略并且执行，进行一个Policy-Gradient的过程更新θ；</p>
<p>critic，参数w，用来估计价值函数Q，进行一个策略评估的过程更新w。</p>
</blockquote>
<p> 减少方差的trick：</p>
<ul>
<li>减去一个Baseline函数，也就是在PG的过程中，不再使用Q函数，而是使用Advantage函数，即Q-V，这里Baseline函数也就是状态价值函数V。（Dueling）</li>
</ul>
<p>估计Advantage函数，可以使用TD error，因为这两者是等价的。</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>David Silver</tag>
        <tag>公开课</tag>
        <tag>Actor-Critic</tag>
      </tags>
  </entry>
  <entry>
    <title>【David Silver强化学习公开课】-8：Integrating Learning and Planning</title>
    <url>/2017/09/11/deeplearning/DRL/%E3%80%8ADavid%20Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E3%80%8B-8%EF%BC%9AIntegrating%20Learning%20and%20Planning/</url>
    <content><![CDATA[<h4 id="一、Model-based-RL"><a href="#一、Model-based-RL" class="headerlink" title="一、Model-based RL"></a>一、Model-based RL</h4><blockquote>
<p>Model-Free RL，从经验中<strong>学习</strong>价值函数（以及/或者策略）。</p>
</blockquote>
<p>Model-based RL，从经验中直接学习环境的MDP模型。（状态转移概率P以及奖励矩阵R）从模型出发，<strong>规划</strong>价值函数（和/或策略）。能够更加有效的学习，减少模型的不确定性，但是缺点是会带来两个（学习模型，进行规划）过程的误差。</p>
<p>这里有一个重要的假设，就是R和P是互相独立的，也就是某一时刻的状态和行为<script type="math/tex">(s,a)</script>获得的下一时刻收益<script type="math/tex">r\sim R</script>和下一时刻状态<script type="math/tex">s\sim P</script>无关。</p>
<ul>
<li>那么第一步，经验中学习模型就是两个监督学习问题：</li>
</ul>
<p>回归问题： <script type="math/tex">s,a \rightarrow r</script></p>
<p>分类问题： <script type="math/tex">s, a \rightarrow s'</script></p>
<p>至于用什么来表示模型的P和R，高斯过程模型、线性高斯模型、神经网络模型都是可以的。</p>
<ul>
<li>第二步，就是利用学习到的模型进行规划。</li>
</ul>
<p>我们有价值迭代，策略迭代，树搜索等方法。此外，还可以直接对已知模型进行抽样，对抽样出的experience利用前几节的model-free方法如Q-learning、Sarsa、Monte-Carlo-Control等进行规划。</p>
<h4 id="二、Integrated-Arch"><a href="#二、Integrated-Arch" class="headerlink" title="二、Integrated Arch"></a>二、Integrated Arch</h4><p>Dyna：从真实experience和simulated experience中<strong>学习和规划</strong>价值函数（策略）。其中后者是我们学习到的MDP（不精确的模型）产生的sample。</p>
<p><img src="/images/FqpVBrA3MnS2s-ebdto8kX2SA9wM.jpg" alt=""></p>
<p><img src="/images/FrCX1p3AdYB8aW9iN5tBmWZUd_4F.jpg" alt=""></p>
<p>从算法过程上来看，就是在每一步，用真实环境的Sample数据学习一次Q，并学习一次Model，然后用Model产生的sample学习n次Q。</p>
<h4 id="三、Simulation-based-Search"><a href="#三、Simulation-based-Search" class="headerlink" title="三、Simulation-based Search"></a>三、Simulation-based Search</h4><p>关注当前状态，利用Forward Search算法，建立一个以当前状态<script type="math/tex">s_t</script>为root的搜索树。</p>
<p>基于模拟的搜索：从当前状态开始，利用我们的模型计算出k个episode；然后利用model-free的方法进行学习和规划。</p>
<p>模拟过程中采用的策略：如果当前需要的状态和动作已经包含在构造的树中，那么最大化Q；否则随机选择Action（exploration）。</p>
<p>Dyna-2，利用real experience学习long-term memory，利用simulated experience 学习 short-term memory。</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>David Silver</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>双向预训练语言模型BERT</title>
    <url>/2018/10/18/deeplearning/LM/BERT/</url>
    <content><![CDATA[<h3 id="BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><a href="#BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding" class="headerlink" title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"></a>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h3><p>Google AI Language在2018.10发布的论文，提出一种预训练语言模型，在十多个NLP任务中取得了SOTA的效果。</p>
<h4 id="1-现有的LM"><a href="#1-现有的LM" class="headerlink" title="1.现有的LM"></a>1.现有的LM</h4><p>两种使用语言模型的方法：</p>
<p>Feature-based，Word2Vec、Glove、ELMo，把预训练好的语言模型当做一个feature加到word embedding中。</p>
<p>fine-tuning，OpenAI-GPT，尽量不改动预训练的语言模型，加输出层并在下游的任务上fine-tune。</p>
<p>他们的共同点：</p>
<p>1.在预训练的时候用的是同一个目标函数；</p>
<p>2.使用单向语言模型学习语言表示。其中，GPT在预训练的时候，预测下一个词只使用其左侧的上下文，相当于使用了Transformer的Decoder的结构。而ELMo等语言模型虽然得到了双向表示，但是是两个独立的目标函数下得到的双向表示直接拼接起来。</p>
<h4 id="2-亮点"><a href="#2-亮点" class="headerlink" title="2.亮点"></a>2.亮点</h4><p>结构：</p>
<p>每层都是双向Context-Dependency。</p>
<p>目标函数：MLM（Masked Language Model） + Next Sentence Prediction。</p>
<p>结果：</p>
<p>不需要调整模型结构，只需要在预训练的LM上加上一层输出层并fine-tune就能应用在多种NLP任务。</p>
<h4 id="3-细节"><a href="#3-细节" class="headerlink" title="3.细节"></a>3.细节</h4><p>（1）输入：</p>
<p>由词向量+位置向量+Seg向量三个部分组成，</p>
<p>词向量，第一个词是CLS，如果是分类任务这个token对应的embedding可以作为学习到的句子表示。</p>
<p>可学习的位置向量，用来改善self-attention的效果。</p>
<p>对于Sentence-Pair输入类型，采用和GPT类似的方式concat两个句子，以Seg向量区分token所在的句子。</p>
<p><img src="/images/BERT/input.jpg" alt="1"></p>
<p>（2）Train Objective：</p>
<p>MLM：</p>
<p>为了改善单向LM问题，如果直接使用多层双向那么模型相当于直接看到了答案，所以使用类似Cloze的Masked Language Model。</p>
<p>训练的时候如果只是把输入中的一些单词mask为一个特殊字符然后预测它们，那么和使用的时候差别太大。为了减轻这个影响，使用了一些策略（mask 15%的单词）：80%的概率mask为mask字符，10%的概率替代为特殊字符，10%的概率使用原有单词。</p>
<p>Next Sentence Prediction:</p>
<p>另一个pre-train的目标，预测句子之间是否是相邻关系。这样使LM在需要理解句子之间关系的下游任务中表现更好。</p>
<p>（3）Pre-train过程：</p>
<p>BookCorpus+Wiki数据的文档级别的语料，抽取出两个sentence（50%抽取相邻句子，50%抽取段落中随机句子）。</p>
<p>（4）Fine-Tune过程：</p>
<p>对于句子级别的分类过程，取第一个token的最后一层作为表示，再加上一个全连接层。</p>
<p>对于span级别的分类过程（比如SQuAD），增加了一个Start Vector：<script type="math/tex">S \in R^H</script>和End Vector：<script type="math/tex">E \in R^H</script>，用这两个向量与所有token点乘的softmax作为每个token作为span的概率。</p>
<h4 id="4-缺陷"><a href="#4-缺陷" class="headerlink" title="4.缺陷"></a>4.缺陷</h4><p>MLM每个batch只需要预测15%的Mask Token，所以比之前的LM收敛要慢。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>SQuAD</tag>
        <tag>Language Model</tag>
      </tags>
  </entry>
  <entry>
    <title>融合所有层次的表示就能够找到正确答案-FusionNet</title>
    <url>/2018/04/15/deeplearning/MRC/FusionNet/</url>
    <content><![CDATA[<h3 id="一、《Fusionnet-fusing-via-fully-aware-attention-with-application-to-machine-comprehension》"><a href="#一、《Fusionnet-fusing-via-fully-aware-attention-with-application-to-machine-comprehension》" class="headerlink" title="一、《Fusionnet: fusing via fully-aware attention with application to machine comprehension》"></a>一、《Fusionnet: fusing via fully-aware attention with application to machine comprehension》</h3><h4 id="1、主要内容"><a href="#1、主要内容" class="headerlink" title="1、主要内容"></a>1、主要内容</h4><p>2017年11月的论文，国立台湾大学学生在MSR实习时产出。</p>
<blockquote>
<p>最近的MRC模型的核心创新点就是如何让问题q和上下文c的内容进行深度的<strong>交互</strong>，但是这些模型受制于学习能力、计算能力的限制通常只利用了<strong>部分信息</strong>。</p>
<p>图像识别领域的研究认为<strong>不同层次</strong>的表示代表着不同的信息，而这一认识在MRC中也是适用的。</p>
<p>提出了一种能够<strong>利用和融合</strong>问题和答案中不同层次信息的模型。</p>
</blockquote>
<p>核心概念：融合，history-of-word</p>
<p>核心问题：</p>
<h5 id="a-说以往的模型只利用了部分信息，那么它们分别利用了哪些信息如何利用的，可否进行一下分析？"><a href="#a-说以往的模型只利用了部分信息，那么它们分别利用了哪些信息如何利用的，可否进行一下分析？" class="headerlink" title="a.说以往的模型只利用了部分信息，那么它们分别利用了哪些信息如何利用的，可否进行一下分析？"></a>a.说以往的模型只利用了部分信息，那么它们分别利用了哪些信息如何利用的，可否进行一下分析？</h5><p>给定两个包含信息的向量集A和B（比如Q和C的词向量集合或者更高级向量表示的集合），利用A的信息更新B或者利用B的信息更新A的过程叫做融合（fusion），以往的模型大多都是在设计这个融合的过程。</p>
<p>下图是论文关于问题1的一个陈述，包含了主要的几种信息融合的方式以及以往模型的做法，其中矩形一般是各种RNN网络，箭头指向的集合代表被融合的集合。融合的过程一般用注意力机制实现。</p>
<p><img src="http://7xqpl8.com1.z0.glb.clouddn.com/aed80f38-a3ed-c4ff-7cb4-ac692639a4a4%2F201841611359.jpg" alt=""></p>
<p>（1）代表单词级别的融合，把问题的单词蕴含的信息给到上下文中。可以通过把二进制形式的feature添加到词向量之后的形式做Hu2017&amp;Chen2017，该特征表征上下文里的单词是否出现在问题中。</p>
<p>（2）利用问题的高层次表示更新上下文的高层次表示，帮助寻找答案。但是高层次表示本身是不精确的，可能丢失一些细节信息。</p>
<p>（2‘）把问题的高层次表示融合到上下文的单词级别的表示。</p>
<p>（3）self-boosted-fusion。利用上下文的高层次表示本身，更新高层次表示，一般用self-attention实现。</p>
<p>（3‘）在融合单词表示之前进行self-boosted-fusion，比如Xiong2017提出的coattention。</p>
<h5 id="b-怎么进行深度融合，才算是利用了所有信息？"><a href="#b-怎么进行深度融合，才算是利用了所有信息？" class="headerlink" title="b.怎么进行深度融合，才算是利用了所有信息？"></a>b.怎么进行深度融合，才算是利用了所有信息？</h5><p>History-of-word：一个单词从低层次到高层次所有表示向量一起称做这个词的history。</p>
<p>利用fully-aware-attention的机制，进行B-&gt;A的融合的时候，首先利用A、B集合中所有词的<strong>history-of-word表示</strong>计算注意力权重，然后把计算得到的权重通过与B中每个单词加权求和，之后把融合得到的信息<script type="math/tex">\hat h^A_i</script>连接到原本的信息<script type="math/tex">h^A_i</script>之后。</p>
<script type="math/tex; mode=display">
s_{ij} = S(His(W^A_i), His(w^B_j))\\
\alpha_{ij} = exp(s_{ij}) / \sum_k exp(s_{ik})\\
\hat h^A_i = \sum_j \alpha_{ij} h^B_j</script><p>具体的做法如下：</p>
<p><img src="http://7xqpl8.com1.z0.glb.clouddn.com/aed80f38-a3ed-c4ff-7cb4-ac692639a4a4%2F201841894137.jpg" alt=""></p>
<p>（1）3层Fully-Aware-Attention，把3个不同级别问题表示concat后向上下文C进行融合。</p>
<p>（2）把多层上下文C的表示和（1）中融合q的C表示一起传入BiLSTM。</p>
<p>（3）C自己的Fully-Aware-Attention，就是不同级别的C一起进行self-attention。</p>
<p>（4）把self-attention后上下文C的信息和C本身的信息concat起来传入BiLSTM中。</p>
<ul>
<li>输入：300维Glove向量+600维Context向量，在C中增加额外的POS、NER和词频表示。输入是<script type="math/tex">w^C\in R^{920},w^Q\in R^{900}</script>。(这里和后面指代维度都是指矩阵中每一个向量的维度)</li>
<li>Fully-Aware Word-Level Fusion：对Q和C的Glove向量进行Fusion，并把得到的fusion表示、C的输入向量和em向量（C中单词是否在Q中出现）concat起来作为输出。本层利用Q更新了C的单词级别表示，得到了<script type="math/tex">\tilde w^C\in R^{1200}</script>。</li>
<li>Reading：多层BiLSTM利用Q和C上一层的输出，得到不同层次250维的向量输出。本层得到了Q和C不同层次的表示<script type="math/tex">h^{Ql},h^{Qh},h^{Cl},h^{Ch}</script>，维度是<script type="math/tex">R^{250}</script>。</li>
<li>Question Understanding：单层BiLSTM利用上一层的输出，得到问题Q的最终表示<script type="math/tex">U_Q\in R^{250}</script>。</li>
<li>Fully-Aware Higher-Level Fusion：利用问题的History-of-word，更新上下文的History-of-word向量【1400维】（就是刚刚产生的所有向量concat在一起），得到上下文的三个question-aware-encoding：<script type="math/tex">\hat h^{Cl},\hat h^{Ch},\hat u^{C}</script>。这三个encoding再通过BiLSTM得到Quesion-Full-Aware-Encoding：<script type="math/tex">v^C\in R^{250}</script>。</li>
<li>Fully-Aware Self-boosted Fusion：如上文（3）（4）中所述。得到了上下文C的最终表示<script type="math/tex">U_C\in R^{250}</script>。</li>
<li>最后利用得到的上下文和问题的表示进行解码，推理答案。</li>
</ul>
<p>此外，对于Full-Attention使用的注意力函数，以及不同层次融合对结果的影响进行了Ablation Analysis。对SQuAD的部分结果进行了与BIDAF的Case Study对比。</p>
<h4 id="2、实验"><a href="#2、实验" class="headerlink" title="2、实验"></a>2、实验</h4><p>在SQuAD上进行了实验，单模型得到了78.8%的准确率。在SQuAD的几个对抗性数据集上也表现不错，说明模型的鲁棒性很好。</p>
<h3 id="文献"><a href="#文献" class="headerlink" title="文献"></a>文献</h3><p><a href="https://arxiv.org/abs/1711.07341" target="_blank" rel="noopener">Fusionnet: fusing via fully-aware attention with application to machine comprehension</a></p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>机器阅读理解</tag>
        <tag>attention</tag>
        <tag>SQuAD</tag>
      </tags>
  </entry>
</search>
